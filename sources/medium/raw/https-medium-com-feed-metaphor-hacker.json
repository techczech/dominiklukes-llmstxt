{
  "bozo": false,
  "entries": [
    {
      "title": "No back row, no corridor: Metaphors for online teaching and learning",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://medium.com/feed/metaphor-hacker",
        "value": "No back row, no corridor: Metaphors for online teaching and learning"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://medium.com/metaphor-hacker/no-back-row-no-corridor-metaphors-for-online-teaching-and-learning-9628f164fc37?source=rss----67ad910371b8---4"
        },
        {
          "rel": "license",
          "href": "https://creativecommons.org/licenses/by-sa/4.0/"
        }
      ],
      "link": "https://medium.com/metaphor-hacker/no-back-row-no-corridor-metaphors-for-online-teaching-and-learning-9628f164fc37?source=rss----67ad910371b8---4",
      "id": "https://medium.com/p/9628f164fc37",
      "guidislink": false,
      "tags": [
        {
          "term": "metaphor",
          "scheme": null,
          "label": null
        },
        {
          "term": "online-teacher",
          "scheme": null,
          "label": null
        },
        {
          "term": "education",
          "scheme": null,
          "label": null
        },
        {
          "term": "online-teaching",
          "scheme": null,
          "label": null
        },
        {
          "term": "elearning",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Dominik Lukes"
        }
      ],
      "author": "Dominik Lukes",
      "author_detail": {
        "name": "Dominik Lukes"
      },
      "published": "Sun, 28 Jun 2020 08:12:15 GMT",
      "published_parsed": [
        2020,
        6,
        28,
        8,
        12,
        15,
        6,
        180,
        0
      ],
      "updated": "2020-06-28T08:27:05.552Z",
      "updated_parsed": [
        2020,
        6,
        28,
        8,
        27,
        5,
        6,
        180,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://medium.com/feed/metaphor-hacker",
          "value": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*K1Rl8aoSxR5mt4l5\" /><figcaption>Photo by <a href=\"https://unsplash.com/@johnschno?utm_source=medium&amp;utm_medium=referral\">John Schnobrich</a> on <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h3>Publication note</h3><p>An earlier version of this was published in the <a href=\"https://staff.admin.ox.ac.uk/oxford-magazine\">Oxford Magazine</a> No 422. This post expands certain sections based on questions and feedback I received following the first publication of the piece. It is also available on my <a href=\"http://metaphorhacker.net/2020/06/no-back-row-no-corridor-metaphors-for-online-teaching-and-learning/\">blog Metaphorhacker.net</a>.</p><h3>The state of digital dislocation</h3><p>The current state of digital dislocation is forcing us to reevaluate what is the essence of teaching and learning. The “grammar of schooling” [1] has been taken away from us and we are forced to learn a new dialect by immersion with just a few phrasebooks, hastily pulled off the shelf, to guide us. Digital teaching is still teaching but it is teaching with an accent, one where we’re still trying to acquire enough fluency and idiomaticity to feel completely at home. When we add to it the culture shock of being in a new situation without any of the familiar cues, sights, sounds and smells of our native environment, it is not surprising that many people are feeling stressed and long for a swift return to “normal”. But it is also no surprise that many others are examining the current situation and finding the new land to be one of endless opportunity and thinking of establishing a permanent residence or at least buying a holiday home.</p><p>At one extreme, we are hearing voices calling online learning “clearly inferior,” lacking the essential personal contact that defines the University experience and asking whether the cost, expressed in fees, is too high. At the other pole, we hear “online teaching is clearly better,” doing away with all the distractions and deadweight of spaces, commutes and providing the focus so essential to learning. The same person can find themselves taking either position depending on the stage of culture shock they are living through at the moment. Both of these perspectives were reflected in an eloquent summary by Ray Williamson from the Oxford Student Union in a recent issue of the Oxford Magazine.[2] Here, I’d like to elaborate on what is at stake and look at ways of conceptualising the different perspectives.</p><h3>Making sense of digital with affordance metaphors</h3><p>I suggest that the two divergent views can best be reconciled when we contrast the affordances of the physical and virtual environments in which teaching and learning take place. By affordances I mean those features of the environment that present themselves to us for direct action and interaction and thus make the world around us meaningful and define what it means to live in the space we’re in. Affordance is a concept fundamental to design thinking and interaction and ignoring them is the most frequent cause of failure both in digital and physical products. [3]</p><p>The best way I found to bring the contrast between the physical and the virtual into focus are two metaphors that can be summarised as “No back row” and “No corridor”.</p><h3>“No back row”</h3><p>“No back row” expresses mostly the potential of the online experience to be positive for learning: the digital space is the great equaliser, no student is left hiding in the dark corner of the room, everybody’s contribution is coming from the front. This leads to higher engagement with the study material, and better learning. It is so powerful that the American online course provider 2U trademarked the slogan as part of their corporate philosophy [4]. Of course, just because it has the potential to be beneficial for learning, it doesn’t mean that we can just put the same course online and get its benefits. We have to design the online courses to take advantage of this. Nor should we be mislead by the visual metaphor of the Zoom call that 2U use on their corporate page. This applies to an entirely forum-based course, as well. The very fact that they have to engage with the content may put additional demands and stresses on students that will require support. This is in addition to the issues that are captured by the ‘no corridor’ metaphor.</p><h3>“No corridor”</h3><p>“No corridor” reflects the largely negative aspects of the virtual when contrasted with face to face. It reflects the lack of physical and social space connecting the learning situations. There are no natural landmarks to guide us, no flow of the crowd to follow. Everything has to be scheduled, bookmarked or emailed. There is little serendipity and no feeling of just “being there”. This makes it easy for a student to disappear and find themselves in “no row” at all. The physical space is doing a lot of work that is beyond the conscious notice of educators and programme administrators and allows them to be less specific in their instructions and leave things to ‘work themselves’ out without realising it. Their planning may be meticulous and painstaking but it is always framed by what the space affords them when it is filled with students, signs, and other signals that may feel almost subliminal. This can be easily seen when we compare the instructions students receive before arrival (what to bring, where to come, what to expect) and when they arrive which may be as little as a time table followed by ad-hoc announcements. And this comparison may gives clues to some aspects of mitigating the downsides of ‘no corridor’.</p><h3>Affordances of the physical vs virtual</h3><figure><img alt=\"“”\" src=\"https://cdn-images-1.medium.com/max/1024/0*-edmob2wLvw5euZi\" /><figcaption>Photo by <a href=\"https://unsplash.com/@ciabattespugnose?utm_source=medium&amp;utm_medium=referral\">Lucrezia Carnelos</a> on <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Luckily, we can mitigate the downsides of the virtual and amplify its benefits, if we pay careful attention to the affordances of the physical. There are successful ways of making up for the lack of the corridor’s hidden contribution to the learning process but we must avoid taking the normal environment in which learning takes place for granted. We rightly focus on personal relationships as essential to learning but as we saw above it is easy to underestimate the power of the spaces in which they are situated.</p><p>In the physical space, it is much easier to just follow the flow of the environment and learn, without realising it, by reflecting others’ reactions to it. There are spaces laid out so obviously that our use of them passes completely beneath any level of conscious notice. We do not need to deliberate on how to open doors, sit facing the speaker, not to sit in a seat already occupied. And where there are issues (locked doors, missing markers, drilling outside the window), we have established scripts for coping and frames for interpreting them.</p><p>None of these features are present in the virtual environment. Every action (at least initially), requires the effort of directed attention. We need to learn the “interfaces” of Zoom, establish routines of where to ‘find the link to join’, keep track of bookmarks for the learning materials, and manage actual time for virtual events and assignment deadlines. All of this virtual effort is taking place in an actual physical environment where we are the only person engaging in the activity. What’s worse, when we study or teach virtually, we do not appear to the world around us any different from when we idly browse the web or are binge-watching a TV show. We then have to negotiate with that environment and people in it in ways that travelling to ‘school’ or the ‘library’ does for us without any words having to be exchanged other than ‘I’m going to class’.</p><p>It’s no wonder many are finding themselves more stressed, tired and downright disoriented. But equally, to no one’s surprise, there are many who are thriving without the extra burden of the physical space which they may have found too overwhelming, full of distractions and uncertainties. We know that not all students cherish the demands of the physical spaces into which attending a university thrusts them; those who only feel comfortable huddled in the back row or for whom passage through the corridor is an exercise fraught with anxiety. Universities have ample built-in support structures and processes (albeit imperfect) for the latter but none for the former.</p><h3>For a successful online learning experience</h3><p>Yet, we know that it is possible to build a sense of “being there” in fully virtual environments and it is also possible to establish durable personal support relationships. This was possible even before the rise of Zoom or Teams as the success of Open University can attest but now it is even more within reach. Perhaps the most powerful indications of this are coming from the successes of telemedicine and even online psychotherapy. Many patients are finding that their one-on-one experience with a therapist is enhanced without the stressful overhead of travel, sitting in waiting rooms, walking through crowds, etc. [6]</p><p>Telemedicine also shows the way when we think about the heterogeneity of needs and inclinations. It is clearly not always appropriate to conduct therapeutic interventions over Skype but it is sufficient or even superior in more instances than may have been thought before the current situation made them a necessity. Do we think that education is radically different, here?</p><p>What does a University have to do to make the most of the benefits of ‘no back row’ and minimise the downsides of ‘no corridor’? What does the individual educator? The solutions are surprisingly simple and non-technical. Above all, we need to realise how much we can leave unsaid because the physical environment says it for us and then make it explicit in the virtual setting. We need to communicate more clearly and more frequently. We need to design the virtual learning spaces to minimize unnecessary cognitive load, structure information better, pay attention to navigation and consistency. We need to constantly fine-tune the balance between information overload and not enough information. We need to build structures that support the students who are struggling with the technological as well as personal aspects of learning.</p><h3>New roles for the relationship business</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*YYULQpRgFbqdh-lo\" /><figcaption>Photo by <a href=\"https://unsplash.com/@brookecagle?utm_source=medium&amp;utm_medium=referral\">Brooke Cagle</a> on <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>But ultimately and most importantly, we need to realise that educational institutions are not in the information business, they are in the relationship business (to borrow a metaphor from the media critic Jeff Jarvis [5]). It is easy to deploy an army of learning technologists and media production specialists, and think we’ve done virtual teaching justice. But online teaching requires other support roles and activities than just those leading to the deployment of “tech”.</p><p>There need to be roles whose main job it is to make sure students are opening the right virtual doors and sitting facing the right way in the virtual learning spaces. There need to be roles that pay attention to the real physical spaces and social situations on the other side of the Zoom call. When students are on campus, so much of this is done for us by the affordances of the space built up over centuries and so ingrained into our conceptual and perceptual systems that interacting with them feels to be a matter of instinct.</p><p>When all we have is emails, forum posts, webcams and the screen, we need to put in additional work to make up for this. Over time, it will come to seem as natural as what we have now but not without the initial effort. For instance, it is not anyone’s job to explicitly make sure students socialise with others in the physical environment. We don’t ask students if they “went out for a drink” with others when they’re on campus, but perhaps, it needs to be somebody’s job in the virtual situation. [7]</p><h3>Sources of learning</h3><p>Luckily, we have ample models of successful practice to draw on. The Open University is one such, Oxford’s own Continuing Education department is another. Private online education providers such as GetSmarter / 2U, who provided the first part of the metaphor, are others.</p><p>As far back as 2009 before Zoom or video conferencing, I taught a module on language and education in a physical setting followed a year later by a similar module in a fully online course for teachers. I was struck, when reading the final essays, how much more the online students seem to have engaged with the subject.</p><p>In the physical space, I had a feeling of engagement during my seminars with the students. But the ‘feedback’ I was getting from them hid the relative shallowness and unevenness of their engagement. I never saw the online students in person, so I had to design the course to get this feedback in other ways and I could easily see where all individual students were and guide them back in the right direction if they seemed to be floundering. It was more work for me and them but the learning gains were there to see.</p><p>The lessons of this anecdote are supported by research evidence and by experiences of educators the world over [8]. We do not need to provide inferior experiences to students just because they are not in the same room as us.</p><p>Eventually the world of university teaching and learning will return to “normal” but we should be mindful that culture shock happens on returning home, as well.[9] We can take advantage of what we learned during this forced sojourn in digital lands to develop a more robust bi-cultural approach to teaching by blending the best of both worlds.</p><h3>Footnotes</h3><p>[1] Tyack, D.B. and Cuban, L., 1995. Tinkering toward utopia: a century of public school reform. Harvard University Press, Cambridge, Mass ; London.</p><p>[2] Williams, R. 2020. “Students and remote learning” Oxford Magazine, 421, Trinity.</p><p>[3] Norman, D.A., 2013. The design of everyday things. Basic books, New York, N.Y.</p><p>[4] No Back Row | 2U [WWW Document], n.d. URL <a href=\"https://cdn2.2u.com/about/no-back-row/\">https://cdn2.2u.com/about/no-back-row/</a> (accessed 6.8.20).</p><p>[5] Jarvis, J., 2012. What the media can learn from Facebook. The Guardian, 15 February 2012, sec. Media Network. <a href=\"https://www.theguardian.com/media-network/media-network-blog/2012/feb/15/what-media-learn-facebook\">https://www.theguardian.com/media-network/media-network-blog/2012/feb/15/what-media-learn-facebook</a>.</p><p>[6] These two recent pieces summarise the pros and cons of mental and physical health interventions and point to relevant research.</p><p>Joyce, N., 2020. Online therapy having its moment, bringing insights on how to expand mental health services going forward [WWW Document]. The Conversation. URL <a href=\"http://theconversation.com/online-therapy-having-its-moment-bringing-insights-on-how-to-expand-mental-health-services-going-forward-136374\">http://theconversation.com/online-therapy-having-its-moment-bringing-insights-on-how-to-expand-mental-health-services-going-forward-136374</a> (accessed 6.8.20).</p><p>Novella, S. 2020. It’s Time for Telehealth. NeuroLogica Blog. URL <a href=\"https://theness.com/neurologicablog/index.php/its-time-for-telehealth/\">https://theness.com/neurologicablog/index.php/its-time-for-telehealth/</a> (accessed 6.8.20).</p><p>[7] Redmond, P., Heffernan, A., Abawi, L., Brown, A., Henderson, R., 2018. An Online Engagement Framework for Higher Education. Online Learning 22. <a href=\"https://doi.org/10.24059/olj.v22i1.1175\">https://doi.org/10.24059/olj.v22i1.1175</a></p><p>[8] The following systematic reviews show that online higher education is at least as effective as offline education when it comes to learning outcomes.</p><p>Means, B., Toyama, Y., Murphy, R., Bakia, M., Jones, K., 2009. Evaluation of Evidence-Based Practices in Online Learning: A Meta-Analysis and Review of Online Learning Studies, US Department of Education. US Department of Education.</p><p>Nguyen, T., 2015. The Effectiveness of Online Learning: Beyond No Significant Difference and Future Horizons 11, 11.</p><p>Pei, L., Wu, H., 2019. Does online learning work better than offline learning in undergraduate medical education? A systematic review and meta-analysis. Med Educ Online 24. <a href=\"https://doi.org/10.1080/10872981.2019.1666538\">https://doi.org/10.1080/10872981.2019.1666538</a></p><p>[9] Gaw, K.F., 2000. Reverse culture shock in students returning from overseas. International Journal of Intercultural Relations 24, 83–104. <a href=\"https://doi.org/10.1016/S0147-1767(99)00024-3\">https://doi.org/10.1016/S0147-1767(99)00024-3</a></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9628f164fc37\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/no-back-row-no-corridor-metaphors-for-online-teaching-and-learning-9628f164fc37\">No back row, no corridor: Metaphors for online teaching and learning</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        }
      ],
      "summary": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*K1Rl8aoSxR5mt4l5\" /><figcaption>Photo by <a href=\"https://unsplash.com/@johnschno?utm_source=medium&amp;utm_medium=referral\">John Schnobrich</a> on <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h3>Publication note</h3><p>An earlier version of this was published in the <a href=\"https://staff.admin.ox.ac.uk/oxford-magazine\">Oxford Magazine</a> No 422. This post expands certain sections based on questions and feedback I received following the first publication of the piece. It is also available on my <a href=\"http://metaphorhacker.net/2020/06/no-back-row-no-corridor-metaphors-for-online-teaching-and-learning/\">blog Metaphorhacker.net</a>.</p><h3>The state of digital dislocation</h3><p>The current state of digital dislocation is forcing us to reevaluate what is the essence of teaching and learning. The “grammar of schooling” [1] has been taken away from us and we are forced to learn a new dialect by immersion with just a few phrasebooks, hastily pulled off the shelf, to guide us. Digital teaching is still teaching but it is teaching with an accent, one where we’re still trying to acquire enough fluency and idiomaticity to feel completely at home. When we add to it the culture shock of being in a new situation without any of the familiar cues, sights, sounds and smells of our native environment, it is not surprising that many people are feeling stressed and long for a swift return to “normal”. But it is also no surprise that many others are examining the current situation and finding the new land to be one of endless opportunity and thinking of establishing a permanent residence or at least buying a holiday home.</p><p>At one extreme, we are hearing voices calling online learning “clearly inferior,” lacking the essential personal contact that defines the University experience and asking whether the cost, expressed in fees, is too high. At the other pole, we hear “online teaching is clearly better,” doing away with all the distractions and deadweight of spaces, commutes and providing the focus so essential to learning. The same person can find themselves taking either position depending on the stage of culture shock they are living through at the moment. Both of these perspectives were reflected in an eloquent summary by Ray Williamson from the Oxford Student Union in a recent issue of the Oxford Magazine.[2] Here, I’d like to elaborate on what is at stake and look at ways of conceptualising the different perspectives.</p><h3>Making sense of digital with affordance metaphors</h3><p>I suggest that the two divergent views can best be reconciled when we contrast the affordances of the physical and virtual environments in which teaching and learning take place. By affordances I mean those features of the environment that present themselves to us for direct action and interaction and thus make the world around us meaningful and define what it means to live in the space we’re in. Affordance is a concept fundamental to design thinking and interaction and ignoring them is the most frequent cause of failure both in digital and physical products. [3]</p><p>The best way I found to bring the contrast between the physical and the virtual into focus are two metaphors that can be summarised as “No back row” and “No corridor”.</p><h3>“No back row”</h3><p>“No back row” expresses mostly the potential of the online experience to be positive for learning: the digital space is the great equaliser, no student is left hiding in the dark corner of the room, everybody’s contribution is coming from the front. This leads to higher engagement with the study material, and better learning. It is so powerful that the American online course provider 2U trademarked the slogan as part of their corporate philosophy [4]. Of course, just because it has the potential to be beneficial for learning, it doesn’t mean that we can just put the same course online and get its benefits. We have to design the online courses to take advantage of this. Nor should we be mislead by the visual metaphor of the Zoom call that 2U use on their corporate page. This applies to an entirely forum-based course, as well. The very fact that they have to engage with the content may put additional demands and stresses on students that will require support. This is in addition to the issues that are captured by the ‘no corridor’ metaphor.</p><h3>“No corridor”</h3><p>“No corridor” reflects the largely negative aspects of the virtual when contrasted with face to face. It reflects the lack of physical and social space connecting the learning situations. There are no natural landmarks to guide us, no flow of the crowd to follow. Everything has to be scheduled, bookmarked or emailed. There is little serendipity and no feeling of just “being there”. This makes it easy for a student to disappear and find themselves in “no row” at all. The physical space is doing a lot of work that is beyond the conscious notice of educators and programme administrators and allows them to be less specific in their instructions and leave things to ‘work themselves’ out without realising it. Their planning may be meticulous and painstaking but it is always framed by what the space affords them when it is filled with students, signs, and other signals that may feel almost subliminal. This can be easily seen when we compare the instructions students receive before arrival (what to bring, where to come, what to expect) and when they arrive which may be as little as a time table followed by ad-hoc announcements. And this comparison may gives clues to some aspects of mitigating the downsides of ‘no corridor’.</p><h3>Affordances of the physical vs virtual</h3><figure><img alt=\"“”\" src=\"https://cdn-images-1.medium.com/max/1024/0*-edmob2wLvw5euZi\" /><figcaption>Photo by <a href=\"https://unsplash.com/@ciabattespugnose?utm_source=medium&amp;utm_medium=referral\">Lucrezia Carnelos</a> on <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Luckily, we can mitigate the downsides of the virtual and amplify its benefits, if we pay careful attention to the affordances of the physical. There are successful ways of making up for the lack of the corridor’s hidden contribution to the learning process but we must avoid taking the normal environment in which learning takes place for granted. We rightly focus on personal relationships as essential to learning but as we saw above it is easy to underestimate the power of the spaces in which they are situated.</p><p>In the physical space, it is much easier to just follow the flow of the environment and learn, without realising it, by reflecting others’ reactions to it. There are spaces laid out so obviously that our use of them passes completely beneath any level of conscious notice. We do not need to deliberate on how to open doors, sit facing the speaker, not to sit in a seat already occupied. And where there are issues (locked doors, missing markers, drilling outside the window), we have established scripts for coping and frames for interpreting them.</p><p>None of these features are present in the virtual environment. Every action (at least initially), requires the effort of directed attention. We need to learn the “interfaces” of Zoom, establish routines of where to ‘find the link to join’, keep track of bookmarks for the learning materials, and manage actual time for virtual events and assignment deadlines. All of this virtual effort is taking place in an actual physical environment where we are the only person engaging in the activity. What’s worse, when we study or teach virtually, we do not appear to the world around us any different from when we idly browse the web or are binge-watching a TV show. We then have to negotiate with that environment and people in it in ways that travelling to ‘school’ or the ‘library’ does for us without any words having to be exchanged other than ‘I’m going to class’.</p><p>It’s no wonder many are finding themselves more stressed, tired and downright disoriented. But equally, to no one’s surprise, there are many who are thriving without the extra burden of the physical space which they may have found too overwhelming, full of distractions and uncertainties. We know that not all students cherish the demands of the physical spaces into which attending a university thrusts them; those who only feel comfortable huddled in the back row or for whom passage through the corridor is an exercise fraught with anxiety. Universities have ample built-in support structures and processes (albeit imperfect) for the latter but none for the former.</p><h3>For a successful online learning experience</h3><p>Yet, we know that it is possible to build a sense of “being there” in fully virtual environments and it is also possible to establish durable personal support relationships. This was possible even before the rise of Zoom or Teams as the success of Open University can attest but now it is even more within reach. Perhaps the most powerful indications of this are coming from the successes of telemedicine and even online psychotherapy. Many patients are finding that their one-on-one experience with a therapist is enhanced without the stressful overhead of travel, sitting in waiting rooms, walking through crowds, etc. [6]</p><p>Telemedicine also shows the way when we think about the heterogeneity of needs and inclinations. It is clearly not always appropriate to conduct therapeutic interventions over Skype but it is sufficient or even superior in more instances than may have been thought before the current situation made them a necessity. Do we think that education is radically different, here?</p><p>What does a University have to do to make the most of the benefits of ‘no back row’ and minimise the downsides of ‘no corridor’? What does the individual educator? The solutions are surprisingly simple and non-technical. Above all, we need to realise how much we can leave unsaid because the physical environment says it for us and then make it explicit in the virtual setting. We need to communicate more clearly and more frequently. We need to design the virtual learning spaces to minimize unnecessary cognitive load, structure information better, pay attention to navigation and consistency. We need to constantly fine-tune the balance between information overload and not enough information. We need to build structures that support the students who are struggling with the technological as well as personal aspects of learning.</p><h3>New roles for the relationship business</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*YYULQpRgFbqdh-lo\" /><figcaption>Photo by <a href=\"https://unsplash.com/@brookecagle?utm_source=medium&amp;utm_medium=referral\">Brooke Cagle</a> on <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>But ultimately and most importantly, we need to realise that educational institutions are not in the information business, they are in the relationship business (to borrow a metaphor from the media critic Jeff Jarvis [5]). It is easy to deploy an army of learning technologists and media production specialists, and think we’ve done virtual teaching justice. But online teaching requires other support roles and activities than just those leading to the deployment of “tech”.</p><p>There need to be roles whose main job it is to make sure students are opening the right virtual doors and sitting facing the right way in the virtual learning spaces. There need to be roles that pay attention to the real physical spaces and social situations on the other side of the Zoom call. When students are on campus, so much of this is done for us by the affordances of the space built up over centuries and so ingrained into our conceptual and perceptual systems that interacting with them feels to be a matter of instinct.</p><p>When all we have is emails, forum posts, webcams and the screen, we need to put in additional work to make up for this. Over time, it will come to seem as natural as what we have now but not without the initial effort. For instance, it is not anyone’s job to explicitly make sure students socialise with others in the physical environment. We don’t ask students if they “went out for a drink” with others when they’re on campus, but perhaps, it needs to be somebody’s job in the virtual situation. [7]</p><h3>Sources of learning</h3><p>Luckily, we have ample models of successful practice to draw on. The Open University is one such, Oxford’s own Continuing Education department is another. Private online education providers such as GetSmarter / 2U, who provided the first part of the metaphor, are others.</p><p>As far back as 2009 before Zoom or video conferencing, I taught a module on language and education in a physical setting followed a year later by a similar module in a fully online course for teachers. I was struck, when reading the final essays, how much more the online students seem to have engaged with the subject.</p><p>In the physical space, I had a feeling of engagement during my seminars with the students. But the ‘feedback’ I was getting from them hid the relative shallowness and unevenness of their engagement. I never saw the online students in person, so I had to design the course to get this feedback in other ways and I could easily see where all individual students were and guide them back in the right direction if they seemed to be floundering. It was more work for me and them but the learning gains were there to see.</p><p>The lessons of this anecdote are supported by research evidence and by experiences of educators the world over [8]. We do not need to provide inferior experiences to students just because they are not in the same room as us.</p><p>Eventually the world of university teaching and learning will return to “normal” but we should be mindful that culture shock happens on returning home, as well.[9] We can take advantage of what we learned during this forced sojourn in digital lands to develop a more robust bi-cultural approach to teaching by blending the best of both worlds.</p><h3>Footnotes</h3><p>[1] Tyack, D.B. and Cuban, L., 1995. Tinkering toward utopia: a century of public school reform. Harvard University Press, Cambridge, Mass ; London.</p><p>[2] Williams, R. 2020. “Students and remote learning” Oxford Magazine, 421, Trinity.</p><p>[3] Norman, D.A., 2013. The design of everyday things. Basic books, New York, N.Y.</p><p>[4] No Back Row | 2U [WWW Document], n.d. URL <a href=\"https://cdn2.2u.com/about/no-back-row/\">https://cdn2.2u.com/about/no-back-row/</a> (accessed 6.8.20).</p><p>[5] Jarvis, J., 2012. What the media can learn from Facebook. The Guardian, 15 February 2012, sec. Media Network. <a href=\"https://www.theguardian.com/media-network/media-network-blog/2012/feb/15/what-media-learn-facebook\">https://www.theguardian.com/media-network/media-network-blog/2012/feb/15/what-media-learn-facebook</a>.</p><p>[6] These two recent pieces summarise the pros and cons of mental and physical health interventions and point to relevant research.</p><p>Joyce, N., 2020. Online therapy having its moment, bringing insights on how to expand mental health services going forward [WWW Document]. The Conversation. URL <a href=\"http://theconversation.com/online-therapy-having-its-moment-bringing-insights-on-how-to-expand-mental-health-services-going-forward-136374\">http://theconversation.com/online-therapy-having-its-moment-bringing-insights-on-how-to-expand-mental-health-services-going-forward-136374</a> (accessed 6.8.20).</p><p>Novella, S. 2020. It’s Time for Telehealth. NeuroLogica Blog. URL <a href=\"https://theness.com/neurologicablog/index.php/its-time-for-telehealth/\">https://theness.com/neurologicablog/index.php/its-time-for-telehealth/</a> (accessed 6.8.20).</p><p>[7] Redmond, P., Heffernan, A., Abawi, L., Brown, A., Henderson, R., 2018. An Online Engagement Framework for Higher Education. Online Learning 22. <a href=\"https://doi.org/10.24059/olj.v22i1.1175\">https://doi.org/10.24059/olj.v22i1.1175</a></p><p>[8] The following systematic reviews show that online higher education is at least as effective as offline education when it comes to learning outcomes.</p><p>Means, B., Toyama, Y., Murphy, R., Bakia, M., Jones, K., 2009. Evaluation of Evidence-Based Practices in Online Learning: A Meta-Analysis and Review of Online Learning Studies, US Department of Education. US Department of Education.</p><p>Nguyen, T., 2015. The Effectiveness of Online Learning: Beyond No Significant Difference and Future Horizons 11, 11.</p><p>Pei, L., Wu, H., 2019. Does online learning work better than offline learning in undergraduate medical education? A systematic review and meta-analysis. Med Educ Online 24. <a href=\"https://doi.org/10.1080/10872981.2019.1666538\">https://doi.org/10.1080/10872981.2019.1666538</a></p><p>[9] Gaw, K.F., 2000. Reverse culture shock in students returning from overseas. International Journal of Intercultural Relations 24, 83–104. <a href=\"https://doi.org/10.1016/S0147-1767(99)00024-3\">https://doi.org/10.1016/S0147-1767(99)00024-3</a></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=9628f164fc37\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/no-back-row-no-corridor-metaphors-for-online-teaching-and-learning-9628f164fc37\">No back row, no corridor: Metaphors for online teaching and learning</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Innovation is bad for business: 3 more ‘I’ words to compare innovation to",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://medium.com/feed/metaphor-hacker",
        "value": "Innovation is bad for business: 3 more ‘I’ words to compare innovation to"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://medium.com/metaphor-hacker/innovation-is-bad-for-business-3-more-i-words-to-compare-innovation-to-cb0ff0615256?source=rss----67ad910371b8---4"
        }
      ],
      "link": "https://medium.com/metaphor-hacker/innovation-is-bad-for-business-3-more-i-words-to-compare-innovation-to-cb0ff0615256?source=rss----67ad910371b8---4",
      "id": "https://medium.com/p/cb0ff0615256",
      "guidislink": false,
      "tags": [
        {
          "term": "innovation",
          "scheme": null,
          "label": null
        },
        {
          "term": "metaphor",
          "scheme": null,
          "label": null
        },
        {
          "term": "business",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Dominik Lukes"
        }
      ],
      "author": "Dominik Lukes",
      "author_detail": {
        "name": "Dominik Lukes"
      },
      "published": "Wed, 17 Oct 2018 05:31:09 GMT",
      "published_parsed": [
        2018,
        10,
        17,
        5,
        31,
        9,
        2,
        290,
        0
      ],
      "updated": "2018-10-17T05:31:08.694Z",
      "updated_parsed": [
        2018,
        10,
        17,
        5,
        31,
        8,
        2,
        290,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://medium.com/feed/metaphor-hacker",
          "value": "<p>This first appeared on <a href=\"http://metaphorhacker.net/2018/10/innovation-is-bad-for-business-3-more-i-words-to-compare-innovation-to/\">MetaphorHacker.net</a>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*X2fDro4hA7ej0w2PqphOxA.jpeg\" /></figure><p>Innovation is the ‘in’ thing. Innovate or die is the buzz up and down the hive mind. Everybody is feeling like they must innovate all of the things all of the time. But is the incessant innovation the right mode of approaching this?</p><p>We constantly spin up stories of the intrepid innovator and the change they bring about in the world. But is that what is really happening on the ground? I think we can bring up some metaphors to bear on this that may open up some different possibilities of conceptualizing innovation.</p><p><strong>Note:</strong> I’m writing this as an early adopter and some time professional innovator. But despite a personal drive to constantly try new things, I find that approaching innovation uncritically and without regard to the full diversity of its instantiations is counterproductive. Therefore, this is meant to be a corrective as much to myself as others.</p><h3>Infection as metaphor of innovation</h3><p>When an innovation spreads through an institution, it does so just like an infection. It starts attacking existing systems who then have to spend time and resources on averting the damage done by the attack. We only tell the stories where the attack led to the strengthening of the system — like with childhood maladies or inoculation. But chronic disease and death of the system are also not uncommon — we just don’t tell them in association with innovation.</p><p>This weakening happens through many processes that every innovator and innovatee (and this covers most people in one way or another) will have direct experience of. Innovators have experienced resistance, doubt, slow response times. Those are all defense mechanisms the disease of innovation has to overcome.</p><p>The people who experience the innovation as the infected cells, can attest to lower productivity because of the need to learn new things, endless meetings on how to implement the new thing keeping them from doing the job, miscommunication and misunderstanding leading to higher error rate (be it in production or management).</p><p>Where this metaphor breaks down is that if the innovation is successful, the system is transformed. Almost, as if instead of a tumor killing us it would grow us a new useful organ while others may fall without much harm to the organism as a whole.</p><p>On the other hand, the success of the system in extinguishing a malignant innovation can make it more resilient to innovation in the future. And this may lower its chances of survival in the face of environmental changes that make it easier to thrive for those where innovation took the system over. This latter aspect is what proponents of innovation as an unalloyed good point to. But that is a backward perspective. From the ground, the system as an organism will always have to start by defending against innovation as infection no matter how well-intentioned everybody involved may be.</p><h3>Ignorance as metaphor of innovation</h3><p>Innovation is often equated with knowledge. People research new ways of doing things, they bring together existing strands of knowledge and weave from them beautiful tapestries of brighter futures. But in practice, innovation almost always benefits from ignorance.</p><p>Ignorance by inventors is a well-known companion to some of the biggest inventions and discoveries. When Morse set out to create the telegraph system, it was received wisdom that what he was trying to do was physically impossible but he did not know that. Columbus is often given as the example of the innovator who was right and pursued his correct knowledge in spite of ridicule. But the truth was he was a zealot crank and completely wrong about everything. Everybody had known the Earth was round for over a thousand years by the time of Columbus and they also knew how large it was. He did not doubt it was round but he subscribed to a crank theory that it was much smaller than it was. He just got lucky there was a continent in the way.</p><p>This ignorance-of-the-impossible narrative can be applied to many of the other famous inventors. But this story is often told with the naysayers as hidebound blocks to progress and inventors as courageous pursuers of the truth. But this way the narrative misses the overwhelming majority of ignorant would-be inventors being simple cranks. For every clueless Morse and cranky Columbus, there were thousands of unknown failures who did not know or believe something truly impossible was impossible (just like most random mutations do not win the natural selection lottery). The correct response to somebody claiming able to do something known to be impossible is to doubt it. The trick is being able to update one’s priors in a way that helps us better judge the signs of success.</p><p>But we should also not overlook the ignorance among the adopters of innovations. The lack of information on the side of adopters of innovation is another necessary ingredient to success. Every innovation is too uncertain and often unreliable in its earliest stages to be considered by the well-informed other than as a bet. This ignorance is partly a result of pure uncertainty as to the viability of something new. But much more commonly, it is just ignorance of how the new thing works, what are its limitations — and how it trully differs from the old. This results in Potemkin innovations like the original Mechanical Turk. The innovations or their effects are often too complex to be fully understood (even by their inventors). This then leads to the creation of a zeitgeist (a sort of general framing) which provides the innovation with enough vectors for infection and the possibility for improvement.</p><p>But it also makes it very easy for impostors to sneak in. At present, there are many examples of companies simply labeling products with ‘machine learning’ or ‘blockchain’ and selling it to credulous investors and customers even if the underlying technology is not actually using anything that could be meaningfully described that way.</p><h3>Imitation as metaphor of innovation</h3><p>Innovation is associated with inventiveness and creativity. Strokes of brilliance and flashes of genius. But almost all of the great innovations were imitating a previous less successful attempt. It is well known that great inventions and discoveries often appear multiple times simultaneously as different people synthesize available information into similar outcomes. Perhaps the most famous examples are Newton and Leibniz for calculus and Darwin and Wallace for natural selection. But this holds true for almost all the great inventions. Either somebody figured it out as well, or was getting very close.</p><p>But more importantly, by the time we get to talk about almost any innovation, it will have reached us through a long chain of imitations. Novel ways of thinking or doing things really only become innovations when somebody copies them.</p><p>This process of imitation is similar to that of natural selection, so it usually leads to refinement and strengthening of the original idea. But let’s not forget that natural selection is based around the idea of imperfect copies (random mutations) finding uses that increase their chances of spreading (reproduction). So as part of this metaphor, innovation without copying would just be lots of random ideas that go nowhere.</p><p>Not only is innovation the result of imitation, without imitation, there would be no point to it in the first place.</p><h3>Conclusion</h3><p>There is not meant to be conclusion here. Investigating metaphors just opens up new prisms that slightly change the way we look at things. Sometimes, it’s the process of thinking through mappings in the metaphor that forces you to investigate one of the domains more closely. And that’s what this is all about.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=cb0ff0615256\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/innovation-is-bad-for-business-3-more-i-words-to-compare-innovation-to-cb0ff0615256\">Innovation is bad for business: 3 more ‘I’ words to compare innovation to</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        }
      ],
      "summary": "<p>This first appeared on <a href=\"http://metaphorhacker.net/2018/10/innovation-is-bad-for-business-3-more-i-words-to-compare-innovation-to/\">MetaphorHacker.net</a>.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*X2fDro4hA7ej0w2PqphOxA.jpeg\" /></figure><p>Innovation is the ‘in’ thing. Innovate or die is the buzz up and down the hive mind. Everybody is feeling like they must innovate all of the things all of the time. But is the incessant innovation the right mode of approaching this?</p><p>We constantly spin up stories of the intrepid innovator and the change they bring about in the world. But is that what is really happening on the ground? I think we can bring up some metaphors to bear on this that may open up some different possibilities of conceptualizing innovation.</p><p><strong>Note:</strong> I’m writing this as an early adopter and some time professional innovator. But despite a personal drive to constantly try new things, I find that approaching innovation uncritically and without regard to the full diversity of its instantiations is counterproductive. Therefore, this is meant to be a corrective as much to myself as others.</p><h3>Infection as metaphor of innovation</h3><p>When an innovation spreads through an institution, it does so just like an infection. It starts attacking existing systems who then have to spend time and resources on averting the damage done by the attack. We only tell the stories where the attack led to the strengthening of the system — like with childhood maladies or inoculation. But chronic disease and death of the system are also not uncommon — we just don’t tell them in association with innovation.</p><p>This weakening happens through many processes that every innovator and innovatee (and this covers most people in one way or another) will have direct experience of. Innovators have experienced resistance, doubt, slow response times. Those are all defense mechanisms the disease of innovation has to overcome.</p><p>The people who experience the innovation as the infected cells, can attest to lower productivity because of the need to learn new things, endless meetings on how to implement the new thing keeping them from doing the job, miscommunication and misunderstanding leading to higher error rate (be it in production or management).</p><p>Where this metaphor breaks down is that if the innovation is successful, the system is transformed. Almost, as if instead of a tumor killing us it would grow us a new useful organ while others may fall without much harm to the organism as a whole.</p><p>On the other hand, the success of the system in extinguishing a malignant innovation can make it more resilient to innovation in the future. And this may lower its chances of survival in the face of environmental changes that make it easier to thrive for those where innovation took the system over. This latter aspect is what proponents of innovation as an unalloyed good point to. But that is a backward perspective. From the ground, the system as an organism will always have to start by defending against innovation as infection no matter how well-intentioned everybody involved may be.</p><h3>Ignorance as metaphor of innovation</h3><p>Innovation is often equated with knowledge. People research new ways of doing things, they bring together existing strands of knowledge and weave from them beautiful tapestries of brighter futures. But in practice, innovation almost always benefits from ignorance.</p><p>Ignorance by inventors is a well-known companion to some of the biggest inventions and discoveries. When Morse set out to create the telegraph system, it was received wisdom that what he was trying to do was physically impossible but he did not know that. Columbus is often given as the example of the innovator who was right and pursued his correct knowledge in spite of ridicule. But the truth was he was a zealot crank and completely wrong about everything. Everybody had known the Earth was round for over a thousand years by the time of Columbus and they also knew how large it was. He did not doubt it was round but he subscribed to a crank theory that it was much smaller than it was. He just got lucky there was a continent in the way.</p><p>This ignorance-of-the-impossible narrative can be applied to many of the other famous inventors. But this story is often told with the naysayers as hidebound blocks to progress and inventors as courageous pursuers of the truth. But this way the narrative misses the overwhelming majority of ignorant would-be inventors being simple cranks. For every clueless Morse and cranky Columbus, there were thousands of unknown failures who did not know or believe something truly impossible was impossible (just like most random mutations do not win the natural selection lottery). The correct response to somebody claiming able to do something known to be impossible is to doubt it. The trick is being able to update one’s priors in a way that helps us better judge the signs of success.</p><p>But we should also not overlook the ignorance among the adopters of innovations. The lack of information on the side of adopters of innovation is another necessary ingredient to success. Every innovation is too uncertain and often unreliable in its earliest stages to be considered by the well-informed other than as a bet. This ignorance is partly a result of pure uncertainty as to the viability of something new. But much more commonly, it is just ignorance of how the new thing works, what are its limitations — and how it trully differs from the old. This results in Potemkin innovations like the original Mechanical Turk. The innovations or their effects are often too complex to be fully understood (even by their inventors). This then leads to the creation of a zeitgeist (a sort of general framing) which provides the innovation with enough vectors for infection and the possibility for improvement.</p><p>But it also makes it very easy for impostors to sneak in. At present, there are many examples of companies simply labeling products with ‘machine learning’ or ‘blockchain’ and selling it to credulous investors and customers even if the underlying technology is not actually using anything that could be meaningfully described that way.</p><h3>Imitation as metaphor of innovation</h3><p>Innovation is associated with inventiveness and creativity. Strokes of brilliance and flashes of genius. But almost all of the great innovations were imitating a previous less successful attempt. It is well known that great inventions and discoveries often appear multiple times simultaneously as different people synthesize available information into similar outcomes. Perhaps the most famous examples are Newton and Leibniz for calculus and Darwin and Wallace for natural selection. But this holds true for almost all the great inventions. Either somebody figured it out as well, or was getting very close.</p><p>But more importantly, by the time we get to talk about almost any innovation, it will have reached us through a long chain of imitations. Novel ways of thinking or doing things really only become innovations when somebody copies them.</p><p>This process of imitation is similar to that of natural selection, so it usually leads to refinement and strengthening of the original idea. But let’s not forget that natural selection is based around the idea of imperfect copies (random mutations) finding uses that increase their chances of spreading (reproduction). So as part of this metaphor, innovation without copying would just be lots of random ideas that go nowhere.</p><p>Not only is innovation the result of imitation, without imitation, there would be no point to it in the first place.</p><h3>Conclusion</h3><p>There is not meant to be conclusion here. Investigating metaphors just opens up new prisms that slightly change the way we look at things. Sometimes, it’s the process of thinking through mappings in the metaphor that forces you to investigate one of the domains more closely. And that’s what this is all about.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=cb0ff0615256\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/innovation-is-bad-for-business-3-more-i-words-to-compare-innovation-to-cb0ff0615256\">Innovation is bad for business: 3 more ‘I’ words to compare innovation to</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "10 ways in which music is like language and 8 (more important) ways in which it is not",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://medium.com/feed/metaphor-hacker",
        "value": "10 ways in which music is like language and 8 (more important) ways in which it is not"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://medium.com/metaphor-hacker/10-ways-in-which-music-is-like-language-and-8-more-important-ways-in-which-it-is-not-7a1e82aad88c?source=rss----67ad910371b8---4"
        }
      ],
      "link": "https://medium.com/metaphor-hacker/10-ways-in-which-music-is-like-language-and-8-more-important-ways-in-which-it-is-not-7a1e82aad88c?source=rss----67ad910371b8---4",
      "id": "https://medium.com/p/7a1e82aad88c",
      "guidislink": false,
      "tags": [
        {
          "term": "music",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Dominik Lukes"
        }
      ],
      "author": "Dominik Lukes",
      "author_detail": {
        "name": "Dominik Lukes"
      },
      "published": "Mon, 19 Mar 2018 22:46:22 GMT",
      "published_parsed": [
        2018,
        3,
        19,
        22,
        46,
        22,
        0,
        78,
        0
      ],
      "updated": "2018-03-19T22:46:21.805Z",
      "updated_parsed": [
        2018,
        3,
        19,
        22,
        46,
        21,
        0,
        78,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://medium.com/feed/metaphor-hacker",
          "value": "<p>People often talk about music as if it were language. Leonard Bernstein even recorded a series of lectures applying <a href=\"https://www.youtube.com/watch?v=MB7ZOdp__gQ\">Chomsky’s theory of generative grammar to music</a>. <a href=\"https://www.youtube.com/watch?v=Kojvij-PNq8\">Chomsky himself</a> answered a question on this in a not very satisfying manner. Some people can get very exercised over this.</p><p>But it seems to me that a playing around with strengths and weakness of the music = language metaphor can help us come to grips with the question a bit better. We can find a number of mappings between music and language but an equal number of mis-mappings. We do not need to go very far into it to see where they are. That does not mean that a deeper investigation into musical properties of language and linguistic properties of music cannot be fruitful. And obviously they are both universal human faculties — but looking for a musical essence in language or a linguistic essence in music is what a metaphor aware approach to this question is hoping to warn against.</p><p>Here are some of the obvious similarities and dissimilarities. (Note: After, I finished my list, I came across a Chomskean <a href=\"http://ase.tufts.edu/cogstud/jackendoff/papers/Parallels%26nonparallelsprintversion.pdf\">comparison by Jackendoff </a>which has a slightly different focus but comes to the same general conclusion.)</p><h3><strong>Music is like (a) language in that:</strong></h3><ol><li>It can be described through <strong>a system of rules that operate on a limited vocabulary</strong>. There are 12 notes (on the Western chromatic scale) that can produce an infinite variety of melodies just purely in their combination further enhanced by their combination with rhythms, tempos and harmonies. (Although I have argued elsewhere that <a href=\"http://metaphorhacker.net/2014/11/what-language-looks-like-dictionary-and-grammar-are-to-language-what-standing-on-one-foot-is-to-running/\">language is not actually much like this</a>, at all.)</li><li>It combines <strong>small building blocks into larger components </strong>that are like words, phrases, sentences and text. In fact, we talk about phrasing in music. But we have things like bars, stanzas, movements, etc.</li><li>It is <strong>recursively expressive</strong>. I can embed little segments of music in others indefinitely. Bach’s variations are an example of this as is jazz improvisation.</li><li>It has <strong>dual articulation </strong>in that smaller segments like scales are organized independently of large segments. This is well-known about language (in certain circles). We articulate sounds into words and words into statements at the same time but also seemingly independently of each other — we know this because we can be good at one and bad at the other (thus dual articulation). In music, producing individual notes (e.g. fingering on piano or guitar, or breathing and embouchure on a trumpet or trombone) is a skill independent of expressing the musical ideas contained in the notes.</li><li>It has <strong>phraseology and idioms</strong>: We speak of musical phrasing but that is more a question of production. But music also has set ways of expressing certain things. There are things like chord progressions or minor or major modes that combine together to express musical meaning. They are more than the sum of their parts and form their own building blocks.</li><li>It can <strong>cross-reference </strong>between compositions (texts): We can hear echoes of folk songs in classical music or we see direct quotations of melodies in jazz. (But it should be noted that this is much less common than in language where co-reference is one of the core components of language.)</li><li>It can communicate emotion both <strong>segmentally </strong>(sequences of notes) and <strong>suprasegmentally </strong>(expression, emphasis, etc.) In the same way, any phrase (such as <em>Please, sit down.) </em>can be pronounced in many different ways (e.g. in a welcoming, quizzical or threatening manner; deliberately, offhand, formally or casually, etc.).</li><li>It has <strong>styles, genres and dialects or even accents</strong>. We can instantly recognise music recorded in different time periods or in different styles. Even individual artists have particular ways of expressing themselves musically that can be imitated or parodied. YouTube is full of videos of people playing X in the style of Y.</li><li>It can be <strong>acquired and learned</strong>. To reach a ‘fluency’ in music requires an effort that is not dissimilar to the acquisition or learning of language. Part of it happens naturally, simply through exposure to music and part of it is formal — such as learning words to name parts of music, such as notes, chords, harmony, etc. or learning to ‘read sheet music’.</li><li>It is <strong>culturally conditioned</strong>. Different cultures have developed very different takes on what music sounds like. Chinese music does not sound very musical to Western ears due to the very different approach to tonality.</li></ol><p>This list makes it seem like music and language are very similar. But the next list of dissimilarities shows that they are also different in fundamental ways.</p><h3><strong>Music is NOT like (a) language in that:</strong></h3><ol><li>It cannot be used to directly <strong>communicate propositional meaning</strong>. I can say, ‘my house is right at the end of the street’ or ‘that will be 50 cents’. But there’s no way to express this kind of content in music. Sometimes music tries to imitate language (Janáček is often cited doing that) but without the words, nobody would know what an opera is about.</li><li>It has <strong>radically smaller set of building blocks and rules </strong>for their combination than language. There are only 12 intervals/tones in Western Music. But this in itself would not be a problem. There are languages with similarly low numbers of phonemes (distinct sounds). However, there is no intermediary unit of expression equivalent to the word. The rules of melodic composition operate directly on these 12 (or more if we include microtones) tones.</li><li>It does not have <strong>internal instruments of disambiguation</strong>. Being able to repair a conversation that is broken with phrases like ‘what did you say’ or ‘can you say this again’ is a fundamental part of the communicative process. Without them language would not be nearly as useful. There’s nothing like that to be found within the ‘communicative’ inventory of music that does not rely on verbal or written language in one way or another.</li><li>It <strong>can only be universally acquired in the most rudimentary sense </strong>(i.e. everybody can hum a tune but very few people can play an instrument). Everybody can and does learn their first language. And everybody acquires some musicality as part of their socialisation into their culture. But most of what we would consider musical fluency is learned through some means of instruction. Music (in this strict sense) is more like written than spoken language.</li><li><strong>There is much a greater difference between receptive and productive competence. </strong>Everybody knows more of their language receptively (passively) than productively. Depending on context, this gap is very small or very large. A person speaking a language of a small community without a lot of specialisation will have a smaller gap between what they can say and what they can understand(although there are many specialised languages even in these contexts). But this difference will be greater when it comes to technical language between a first year university student and their professor. But in music, everybody can listen passively and receive most of the intended effect while their ability to produce the same music will be severely limited. Many people can hum back a simple tune but they cannot reproduce a full musical performance. Even professional musicians will vary in their ability here.</li><li>There is <strong>much greater variability in individuals’ ability to produce music </strong>beyond the most trivial. It requires effort and study to produce music in a way that we think of as music. We can say that everyone is musical to some degree but most people cannot actually produce anything beyond the simplest of tunes. However, in language, everybody can communicate (even people with impairments) to a significant degree. The differences in competence only appear at the higher levels.</li><li>Much more of the <strong>production process requires cooperation </strong>among individuals. While not a requirement, most music we consume is produced by groups of people. Or if not another person, it requires an instrument made by another person. Language production (at least in spoken form) is primarily by individuals. (Although, there are instances of group production — theatre, speeches, etc.)</li><li>It is much more <strong>limited in its dialogic potential </strong>(i.e. it is most often used for a one way communication between few producers and more recipients or joint co-production of producer/recipients). Language is fundamentally dialogic. Anything that is said can be responded to. Questions can be answered, propositions can be countered or elaborated. Music, on the other hand, is primarily declarative. Of course, metaphorically we can talk about dialogic elements in music. In jazz or blues, we have call and response, in classical music we have things like counterpoint. We can also talk about members of an orchestra communicating and responding to each others’ musical ideas. But there is no such thing in music as saying: ‘No, I disagree with you about X, instead, I believe Y and this is why.’</li></ol><p>Ultimately, this is not all that important. We know what language is and we know what music is. Saying one is or is not like the other in some way won’t change any of that. However, it can help us think more clearly about them and avoid ignoring important aspects and unique properties of both.</p><h3><strong>Note: What is music</strong></h3><p>You often hear popularising musicians saying things like ‘Everybody is musical’ or ‘everything is music’. But that is not what most people’s intuitions about music tell them. For most people, music is the sort of thing they hear on the radio. It is the result of composition and production. There are instruments involved, and skills and ability in playing those instruments. That is the sense in which I’m comparing language to music. When I sing in the shower, I may be engaging in a musical activity but it is not the prototypical meaning of the word. And if we try to build our case for music around that, we’d be leaving other important aspects out.</p><h3><strong>Language of X</strong></h3><p>Lists similar to this one could be constructed for other cases where people talk about the ‘language of X’. Programming, gestures, art, or architecture. These would probably end up with lists that overlap with this one in many ways.</p><p>There are different cliches in use about many of these domain. So people tend to overestimate the extent to which facial expressions or architecture are like language or art but underestimate the degree to which programming languages are like natural language.</p><p>So in some contexts, I’d want to stress the dissimilarities. When people say things like ‘He can express all he needs through dance.’ or ‘90% of all language is nonverbal’, we would want to point to the propositional and dialogic aspects of language that are lacking in these domains.</p><p>But in other contexts, we’d want to point to the parallels. For instance, we may want to remind ourselves that programming languages are more like natural language in many (but not all) of the ways, music is like language. They have dialects, phrases and idioms, multiple levels of articulation, recursivenes (duh!), etc.</p><p><strong>Note:</strong></p><p>This post started life as <a href=\"https://linguistics.stackexchange.com/posts/10013/revisions\">an answer on Stack Exchange</a> which I then <a href=\"http://metaphornik.tumblr.com/post/107870296587/music-is-like-a-natural-language-in-some-respects\">cleaned up for Tumblr</a>. This is an expanded version.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7a1e82aad88c\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/10-ways-in-which-music-is-like-language-and-8-more-important-ways-in-which-it-is-not-7a1e82aad88c\">10 ways in which music is like language and 8 (more important) ways in which it is not</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        }
      ],
      "summary": "<p>People often talk about music as if it were language. Leonard Bernstein even recorded a series of lectures applying <a href=\"https://www.youtube.com/watch?v=MB7ZOdp__gQ\">Chomsky’s theory of generative grammar to music</a>. <a href=\"https://www.youtube.com/watch?v=Kojvij-PNq8\">Chomsky himself</a> answered a question on this in a not very satisfying manner. Some people can get very exercised over this.</p><p>But it seems to me that a playing around with strengths and weakness of the music = language metaphor can help us come to grips with the question a bit better. We can find a number of mappings between music and language but an equal number of mis-mappings. We do not need to go very far into it to see where they are. That does not mean that a deeper investigation into musical properties of language and linguistic properties of music cannot be fruitful. And obviously they are both universal human faculties — but looking for a musical essence in language or a linguistic essence in music is what a metaphor aware approach to this question is hoping to warn against.</p><p>Here are some of the obvious similarities and dissimilarities. (Note: After, I finished my list, I came across a Chomskean <a href=\"http://ase.tufts.edu/cogstud/jackendoff/papers/Parallels%26nonparallelsprintversion.pdf\">comparison by Jackendoff </a>which has a slightly different focus but comes to the same general conclusion.)</p><h3><strong>Music is like (a) language in that:</strong></h3><ol><li>It can be described through <strong>a system of rules that operate on a limited vocabulary</strong>. There are 12 notes (on the Western chromatic scale) that can produce an infinite variety of melodies just purely in their combination further enhanced by their combination with rhythms, tempos and harmonies. (Although I have argued elsewhere that <a href=\"http://metaphorhacker.net/2014/11/what-language-looks-like-dictionary-and-grammar-are-to-language-what-standing-on-one-foot-is-to-running/\">language is not actually much like this</a>, at all.)</li><li>It combines <strong>small building blocks into larger components </strong>that are like words, phrases, sentences and text. In fact, we talk about phrasing in music. But we have things like bars, stanzas, movements, etc.</li><li>It is <strong>recursively expressive</strong>. I can embed little segments of music in others indefinitely. Bach’s variations are an example of this as is jazz improvisation.</li><li>It has <strong>dual articulation </strong>in that smaller segments like scales are organized independently of large segments. This is well-known about language (in certain circles). We articulate sounds into words and words into statements at the same time but also seemingly independently of each other — we know this because we can be good at one and bad at the other (thus dual articulation). In music, producing individual notes (e.g. fingering on piano or guitar, or breathing and embouchure on a trumpet or trombone) is a skill independent of expressing the musical ideas contained in the notes.</li><li>It has <strong>phraseology and idioms</strong>: We speak of musical phrasing but that is more a question of production. But music also has set ways of expressing certain things. There are things like chord progressions or minor or major modes that combine together to express musical meaning. They are more than the sum of their parts and form their own building blocks.</li><li>It can <strong>cross-reference </strong>between compositions (texts): We can hear echoes of folk songs in classical music or we see direct quotations of melodies in jazz. (But it should be noted that this is much less common than in language where co-reference is one of the core components of language.)</li><li>It can communicate emotion both <strong>segmentally </strong>(sequences of notes) and <strong>suprasegmentally </strong>(expression, emphasis, etc.) In the same way, any phrase (such as <em>Please, sit down.) </em>can be pronounced in many different ways (e.g. in a welcoming, quizzical or threatening manner; deliberately, offhand, formally or casually, etc.).</li><li>It has <strong>styles, genres and dialects or even accents</strong>. We can instantly recognise music recorded in different time periods or in different styles. Even individual artists have particular ways of expressing themselves musically that can be imitated or parodied. YouTube is full of videos of people playing X in the style of Y.</li><li>It can be <strong>acquired and learned</strong>. To reach a ‘fluency’ in music requires an effort that is not dissimilar to the acquisition or learning of language. Part of it happens naturally, simply through exposure to music and part of it is formal — such as learning words to name parts of music, such as notes, chords, harmony, etc. or learning to ‘read sheet music’.</li><li>It is <strong>culturally conditioned</strong>. Different cultures have developed very different takes on what music sounds like. Chinese music does not sound very musical to Western ears due to the very different approach to tonality.</li></ol><p>This list makes it seem like music and language are very similar. But the next list of dissimilarities shows that they are also different in fundamental ways.</p><h3><strong>Music is NOT like (a) language in that:</strong></h3><ol><li>It cannot be used to directly <strong>communicate propositional meaning</strong>. I can say, ‘my house is right at the end of the street’ or ‘that will be 50 cents’. But there’s no way to express this kind of content in music. Sometimes music tries to imitate language (Janáček is often cited doing that) but without the words, nobody would know what an opera is about.</li><li>It has <strong>radically smaller set of building blocks and rules </strong>for their combination than language. There are only 12 intervals/tones in Western Music. But this in itself would not be a problem. There are languages with similarly low numbers of phonemes (distinct sounds). However, there is no intermediary unit of expression equivalent to the word. The rules of melodic composition operate directly on these 12 (or more if we include microtones) tones.</li><li>It does not have <strong>internal instruments of disambiguation</strong>. Being able to repair a conversation that is broken with phrases like ‘what did you say’ or ‘can you say this again’ is a fundamental part of the communicative process. Without them language would not be nearly as useful. There’s nothing like that to be found within the ‘communicative’ inventory of music that does not rely on verbal or written language in one way or another.</li><li>It <strong>can only be universally acquired in the most rudimentary sense </strong>(i.e. everybody can hum a tune but very few people can play an instrument). Everybody can and does learn their first language. And everybody acquires some musicality as part of their socialisation into their culture. But most of what we would consider musical fluency is learned through some means of instruction. Music (in this strict sense) is more like written than spoken language.</li><li><strong>There is much a greater difference between receptive and productive competence. </strong>Everybody knows more of their language receptively (passively) than productively. Depending on context, this gap is very small or very large. A person speaking a language of a small community without a lot of specialisation will have a smaller gap between what they can say and what they can understand(although there are many specialised languages even in these contexts). But this difference will be greater when it comes to technical language between a first year university student and their professor. But in music, everybody can listen passively and receive most of the intended effect while their ability to produce the same music will be severely limited. Many people can hum back a simple tune but they cannot reproduce a full musical performance. Even professional musicians will vary in their ability here.</li><li>There is <strong>much greater variability in individuals’ ability to produce music </strong>beyond the most trivial. It requires effort and study to produce music in a way that we think of as music. We can say that everyone is musical to some degree but most people cannot actually produce anything beyond the simplest of tunes. However, in language, everybody can communicate (even people with impairments) to a significant degree. The differences in competence only appear at the higher levels.</li><li>Much more of the <strong>production process requires cooperation </strong>among individuals. While not a requirement, most music we consume is produced by groups of people. Or if not another person, it requires an instrument made by another person. Language production (at least in spoken form) is primarily by individuals. (Although, there are instances of group production — theatre, speeches, etc.)</li><li>It is much more <strong>limited in its dialogic potential </strong>(i.e. it is most often used for a one way communication between few producers and more recipients or joint co-production of producer/recipients). Language is fundamentally dialogic. Anything that is said can be responded to. Questions can be answered, propositions can be countered or elaborated. Music, on the other hand, is primarily declarative. Of course, metaphorically we can talk about dialogic elements in music. In jazz or blues, we have call and response, in classical music we have things like counterpoint. We can also talk about members of an orchestra communicating and responding to each others’ musical ideas. But there is no such thing in music as saying: ‘No, I disagree with you about X, instead, I believe Y and this is why.’</li></ol><p>Ultimately, this is not all that important. We know what language is and we know what music is. Saying one is or is not like the other in some way won’t change any of that. However, it can help us think more clearly about them and avoid ignoring important aspects and unique properties of both.</p><h3><strong>Note: What is music</strong></h3><p>You often hear popularising musicians saying things like ‘Everybody is musical’ or ‘everything is music’. But that is not what most people’s intuitions about music tell them. For most people, music is the sort of thing they hear on the radio. It is the result of composition and production. There are instruments involved, and skills and ability in playing those instruments. That is the sense in which I’m comparing language to music. When I sing in the shower, I may be engaging in a musical activity but it is not the prototypical meaning of the word. And if we try to build our case for music around that, we’d be leaving other important aspects out.</p><h3><strong>Language of X</strong></h3><p>Lists similar to this one could be constructed for other cases where people talk about the ‘language of X’. Programming, gestures, art, or architecture. These would probably end up with lists that overlap with this one in many ways.</p><p>There are different cliches in use about many of these domain. So people tend to overestimate the extent to which facial expressions or architecture are like language or art but underestimate the degree to which programming languages are like natural language.</p><p>So in some contexts, I’d want to stress the dissimilarities. When people say things like ‘He can express all he needs through dance.’ or ‘90% of all language is nonverbal’, we would want to point to the propositional and dialogic aspects of language that are lacking in these domains.</p><p>But in other contexts, we’d want to point to the parallels. For instance, we may want to remind ourselves that programming languages are more like natural language in many (but not all) of the ways, music is like language. They have dialects, phrases and idioms, multiple levels of articulation, recursivenes (duh!), etc.</p><p><strong>Note:</strong></p><p>This post started life as <a href=\"https://linguistics.stackexchange.com/posts/10013/revisions\">an answer on Stack Exchange</a> which I then <a href=\"http://metaphornik.tumblr.com/post/107870296587/music-is-like-a-natural-language-in-some-respects\">cleaned up for Tumblr</a>. This is an expanded version.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7a1e82aad88c\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/10-ways-in-which-music-is-like-language-and-8-more-important-ways-in-which-it-is-not-7a1e82aad88c\">10 ways in which music is like language and 8 (more important) ways in which it is not</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "17 Things Everyone Should Know About Language But Doesn’t Even Know to Ask",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://medium.com/feed/metaphor-hacker",
        "value": "17 Things Everyone Should Know About Language But Doesn’t Even Know to Ask"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://medium.com/metaphor-hacker/17-things-everyone-should-know-about-language-but-doesnt-even-know-to-ask-e078e3fe39c2?source=rss----67ad910371b8---4"
        }
      ],
      "link": "https://medium.com/metaphor-hacker/17-things-everyone-should-know-about-language-but-doesnt-even-know-to-ask-e078e3fe39c2?source=rss----67ad910371b8---4",
      "id": "https://medium.com/p/e078e3fe39c2",
      "guidislink": false,
      "tags": [
        {
          "term": "education",
          "scheme": null,
          "label": null
        },
        {
          "term": "linguistics",
          "scheme": null,
          "label": null
        },
        {
          "term": "language",
          "scheme": null,
          "label": null
        },
        {
          "term": "english-language-teaching",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Dominik Lukes"
        }
      ],
      "author": "Dominik Lukes",
      "author_detail": {
        "name": "Dominik Lukes"
      },
      "published": "Sun, 11 Mar 2018 09:12:16 GMT",
      "published_parsed": [
        2018,
        3,
        11,
        9,
        12,
        16,
        6,
        70,
        0
      ],
      "updated": "2018-03-11T09:12:16.410Z",
      "updated_parsed": [
        2018,
        3,
        11,
        9,
        12,
        16,
        6,
        70,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://medium.com/feed/metaphor-hacker",
          "value": "<p>Languages are not a simple matter of grammar. Any language policy must consider what is known about language from the fields of pragmatics, sociolinguistics, and cognitive linguistics. Here I collected, 17 key findings about language from across many fields of linguistic inquiry. These are descriptive things we know with some certainty about how language works.</p><p>Who should know these things? Everyone would be a nice thing but in the absence of that, I’d settle for some of these: Teachers and especially language teachers. Journalists. Language policy experts. And (and here we’re just praying for a miracle) grammar mavens.</p><p>Not knowing these things does cause harm. People discriminate against one another based on the way they speak, assuming that there is only one right way to speak. People feel bad about themselves and waste money on stupid language self-help books. People launch into ill-conceived campaigns for language improvement and in extreme cases destroy cultures and communities all in the name of misconceptions, about language.</p><p>Grammar and linguistics are unaccountably hard for most people at the theoretical level. But all of the things below are perfectly within grasp of anyone willing to let go of the admonitions of their third-grade English teacher.</p><p><strong>The list:</strong></p><p><strong>1. </strong>Every sentence communicates much more than just its basic content (propositional meaning). We also communicate our desires and beliefs (e.g. “It’s cold here” may communicate, “Close the window” and “John denied that he cheats on his taxes” communicates that somebody accused John of cheating on his taxes. Similarly choosing a particular form of speech, like slang or jargon, communicates belonging to a particular group or community.</p><p><strong>2. </strong>The understanding of any utterance is always dependent on a complex network of knowledge about language, about the world, as well as about the context of the utterance. “China denied involvement.” requires the understanding of the context in which countries operate, as well as metonymy (whole stands for parts), as well as the grammar and vocabulary. Consider the knowledge we need to possess to interpret “In 1939, the world exploded.” vs. “In Star Wars, a world exploded.” Limiting ourselves to what we find in dictionaries, we can never have a full sense of a what any word ‘means’. Dictionaries are not language, just special texts written in the language.</p><p><strong>3. </strong>Purely literal language is very rare. Most language use is to some degree figurative. “Between 3 and 4pm.”, “Out of sight”, “In deep trouble”, “An argument flared up”, “Deliver a service”, “You are my rock”, “Access for all” are all figurative to different degrees. Metaphors are ubiquitous and they are not just some appendage to the ‘literal’ or ‘good’ way of speaking.</p><p><strong>4. </strong>We all speak more than one variety of our language: formal/informal, school/friends/family, written/spoken, etc. Each of these varieties has its own code. For instance, “she wanted to learn and so took a course” vs. “her desire to learn resulted in her taking a course” demonstrates a common difference between spoken and <a href=\"http://en.wikipedia.org/wiki/Standard_written_English\">written English</a> where written English often builds complex nouns phrases instead of simple clauses. In this, written and spoken English grammars are almost as different as those of two different languages.</p><p><strong>5. </strong>We constantly switch between different codes (sometimes even within a single utterance). Think about what is going on in a sentence like “Then Joe spoke unto Karen.” It would take a page to describe all the layers of humor and culture embedded in that simple switch between codes.</p><p><strong>6. </strong>The “standard” or “correct” English is just one of the many dialects, not English itself. It is not something other dialects diverge from, it is their linguistic equal. Its privileged position is only due to long-standing political processes. It is not better or worse, just associated with ‘higher prestige’ contexts.</p><p><strong>7. </strong>The difference between a language and a dialect is just as much political as linguistic. An old joke in linguistics goes: “<a href=\"http://en.wikipedia.org/wiki/A_language_is_a_dialect_with_an_army_and_navy\">A language is a dialect with an army and a navy</a>.” There is no standard measure or universal definition of how to determine where one language or dialect stops and another begins. For example, Serbian and Croatian are less different than US and UK English but for political reasons, they are treated differently. On the other hand, the many “dialects” of Chinese are completely different languages with no way of their speakers to understand each other. And there are many examples in between.</p><p><strong>8. </strong>Language prescription and requirements of <a href=\"http://en.wikipedia.org/wiki/Linguistic_purism\">language purity</a> (incl. simple language) are as much political statements as linguistic or cognitive ones. All language use is related to power relationships. Language purists often just parrot half-remembered rules from school and personal peeves. Speaking a language represents a commitment to a shared set of patterns but this is constantly changing.</p><p><strong>9. </strong>All languages are full of redundancy, polysemy and homonymy. It is the context and our knowledge of what is to be expected that makes it easy to figure out the right meaning. Speakers always use context, expectation and all kinds of inference to figure out the intended meaning.</p><p><strong>10. </strong>Language speakers have many tools to figure out what a statement is about other than just listening or reading carefully. In a dialogue, people use something called ‘conversation repair’, they raise their eye-brows, ask ‘Sorry’, etc. With written texts, they use reference materials, highlight, bookmark, look things up in an index, etc. All of these strategies are a part of their language competence.</p><p><strong>12. </strong>There is no straightforward relationship between grammatical features and language obfuscation or lack of clarity (e.g. It is just as easy to hide things using active as passive voice or any Subject-Verb-Object sentence as Object-Subject-Verb).</p><p><strong>12. </strong>It is difficult to call any one feature of a language universally simple (for instance, <a href=\"http://en.wikipedia.org/wiki/Subject%E2%80%93verb%E2%80%93object\">SVO word order</a> or no morphology) because many other languages use what we call complex as the default without any increase in difficulty for the native speakers (e.g. use of verb prefixes/particles in English and German). There are some languages that have simpler morphologies than others but it has no impact on their expressive ability.</p><p><strong>13. </strong>Language is not really organized into sentences but into texts. Texts have internal organization to hang together formally (John likes coffee. <em>He</em> likes <em>it</em> a lot.) and semantically (As I said about John. He likes coffee.) Texts also relate to external contexts (cross reference) and their situations. This relationship is both implicit and explicit in the text. The shorter the text, the more context it needs for interpretation. For instance, if all we see is “He likes it.” written on a piece of paper, we do not have enough context to interpret the meaning.</p><p><strong>14. </strong>Language is not used uniformly. Some parts of language are used more frequently than others. But this is not enough to understand frequency. Some parts of language are used more frequently together than others. The frequent coocurrence of some words with other words is called “collocation”. This means that when we say “bread and …”, we can predict that the next word will be “butter”. You can check this with a linguistic tool like a corpus, or even by using Google’s predictions in the search. Some words are so strongly collocated with other words that their meaning is “tinged” by those other words (this is called semantic prosody). For example, “set in” has a slightly negative connotation because of its collocation with “rot”. So if you describe someone’s relationship as in ‘then love set in’, you’re likely not all that positive about that development.</p><p><strong>15. </strong>All language is idiomatic to some degree. You cannot determine the meaning of all sentences just by understanding the meanings of all their component parts and the rules for putting them together. And vice versa, you cannot just take all the words and rules in a language, apply them and get meaningful sentences. Consider “I will not put the picture up with John.” and “I will not put up the picture with John.” and “I will not put up John.” and “I will not put up with John.”</p><p><strong>16. </strong>Bilingualism is the norm in language knowledge, not the exception. About half the world’s population regularly speaks more than one language but everybody is “bi-lingual” in the sense that they deal with multiple codes in their language. They may seem like very close but for a child without much familial academic background, entering school and learning to read may feel very much like a foreign language and they often need the same sort of support learners of second languages need.</p><p><strong>17. </strong><a href=\"http://en.wikipedia.org/wiki/Controlled_natural_language\">Simplified languages</a> develop their own complexities if used by a real community through a process known as creolization. (This process is well described for pidgins but not as well for artificial languages.) This has implications for movements such as simple language campaigns. They ignore the fact that simple is relative to the needs of a speech community and not a universal measure.</p><p>More details and examples <a href=\"http://metaphorhacker.net/2012/09/the-complexities-of-simple-what-simple-language-proponents-should-know-about-linguistics/\">THE COMPLEXITIES OF SIMPLE: WHAT SIMPLE LANGUAGE PROPONENTS SHOULD KNOW ABOUT LINGUISTICS on MetaphorHacker.net</a></p><p>(Previously also appeared on Tumblr. Slightly edited here.)</p><p><strong>Post script:</strong> Will linguists agree with this list? That depends on what kind of linguistics they do. Many mathematical and computational linguists are not all that concerned with the subtleties of language or its use in social contexts. General linguists would probably agree in principle with most of these things but would add different qualifications.</p><p>But I’d say that in general, this list is less controversial than something seemingly more innocuous than what the definition of “word” is.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e078e3fe39c2\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/17-things-everyone-should-know-about-language-but-doesnt-even-know-to-ask-e078e3fe39c2\">17 Things Everyone Should Know About Language But Doesn’t Even Know to Ask</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        }
      ],
      "summary": "<p>Languages are not a simple matter of grammar. Any language policy must consider what is known about language from the fields of pragmatics, sociolinguistics, and cognitive linguistics. Here I collected, 17 key findings about language from across many fields of linguistic inquiry. These are descriptive things we know with some certainty about how language works.</p><p>Who should know these things? Everyone would be a nice thing but in the absence of that, I’d settle for some of these: Teachers and especially language teachers. Journalists. Language policy experts. And (and here we’re just praying for a miracle) grammar mavens.</p><p>Not knowing these things does cause harm. People discriminate against one another based on the way they speak, assuming that there is only one right way to speak. People feel bad about themselves and waste money on stupid language self-help books. People launch into ill-conceived campaigns for language improvement and in extreme cases destroy cultures and communities all in the name of misconceptions, about language.</p><p>Grammar and linguistics are unaccountably hard for most people at the theoretical level. But all of the things below are perfectly within grasp of anyone willing to let go of the admonitions of their third-grade English teacher.</p><p><strong>The list:</strong></p><p><strong>1. </strong>Every sentence communicates much more than just its basic content (propositional meaning). We also communicate our desires and beliefs (e.g. “It’s cold here” may communicate, “Close the window” and “John denied that he cheats on his taxes” communicates that somebody accused John of cheating on his taxes. Similarly choosing a particular form of speech, like slang or jargon, communicates belonging to a particular group or community.</p><p><strong>2. </strong>The understanding of any utterance is always dependent on a complex network of knowledge about language, about the world, as well as about the context of the utterance. “China denied involvement.” requires the understanding of the context in which countries operate, as well as metonymy (whole stands for parts), as well as the grammar and vocabulary. Consider the knowledge we need to possess to interpret “In 1939, the world exploded.” vs. “In Star Wars, a world exploded.” Limiting ourselves to what we find in dictionaries, we can never have a full sense of a what any word ‘means’. Dictionaries are not language, just special texts written in the language.</p><p><strong>3. </strong>Purely literal language is very rare. Most language use is to some degree figurative. “Between 3 and 4pm.”, “Out of sight”, “In deep trouble”, “An argument flared up”, “Deliver a service”, “You are my rock”, “Access for all” are all figurative to different degrees. Metaphors are ubiquitous and they are not just some appendage to the ‘literal’ or ‘good’ way of speaking.</p><p><strong>4. </strong>We all speak more than one variety of our language: formal/informal, school/friends/family, written/spoken, etc. Each of these varieties has its own code. For instance, “she wanted to learn and so took a course” vs. “her desire to learn resulted in her taking a course” demonstrates a common difference between spoken and <a href=\"http://en.wikipedia.org/wiki/Standard_written_English\">written English</a> where written English often builds complex nouns phrases instead of simple clauses. In this, written and spoken English grammars are almost as different as those of two different languages.</p><p><strong>5. </strong>We constantly switch between different codes (sometimes even within a single utterance). Think about what is going on in a sentence like “Then Joe spoke unto Karen.” It would take a page to describe all the layers of humor and culture embedded in that simple switch between codes.</p><p><strong>6. </strong>The “standard” or “correct” English is just one of the many dialects, not English itself. It is not something other dialects diverge from, it is their linguistic equal. Its privileged position is only due to long-standing political processes. It is not better or worse, just associated with ‘higher prestige’ contexts.</p><p><strong>7. </strong>The difference between a language and a dialect is just as much political as linguistic. An old joke in linguistics goes: “<a href=\"http://en.wikipedia.org/wiki/A_language_is_a_dialect_with_an_army_and_navy\">A language is a dialect with an army and a navy</a>.” There is no standard measure or universal definition of how to determine where one language or dialect stops and another begins. For example, Serbian and Croatian are less different than US and UK English but for political reasons, they are treated differently. On the other hand, the many “dialects” of Chinese are completely different languages with no way of their speakers to understand each other. And there are many examples in between.</p><p><strong>8. </strong>Language prescription and requirements of <a href=\"http://en.wikipedia.org/wiki/Linguistic_purism\">language purity</a> (incl. simple language) are as much political statements as linguistic or cognitive ones. All language use is related to power relationships. Language purists often just parrot half-remembered rules from school and personal peeves. Speaking a language represents a commitment to a shared set of patterns but this is constantly changing.</p><p><strong>9. </strong>All languages are full of redundancy, polysemy and homonymy. It is the context and our knowledge of what is to be expected that makes it easy to figure out the right meaning. Speakers always use context, expectation and all kinds of inference to figure out the intended meaning.</p><p><strong>10. </strong>Language speakers have many tools to figure out what a statement is about other than just listening or reading carefully. In a dialogue, people use something called ‘conversation repair’, they raise their eye-brows, ask ‘Sorry’, etc. With written texts, they use reference materials, highlight, bookmark, look things up in an index, etc. All of these strategies are a part of their language competence.</p><p><strong>12. </strong>There is no straightforward relationship between grammatical features and language obfuscation or lack of clarity (e.g. It is just as easy to hide things using active as passive voice or any Subject-Verb-Object sentence as Object-Subject-Verb).</p><p><strong>12. </strong>It is difficult to call any one feature of a language universally simple (for instance, <a href=\"http://en.wikipedia.org/wiki/Subject%E2%80%93verb%E2%80%93object\">SVO word order</a> or no morphology) because many other languages use what we call complex as the default without any increase in difficulty for the native speakers (e.g. use of verb prefixes/particles in English and German). There are some languages that have simpler morphologies than others but it has no impact on their expressive ability.</p><p><strong>13. </strong>Language is not really organized into sentences but into texts. Texts have internal organization to hang together formally (John likes coffee. <em>He</em> likes <em>it</em> a lot.) and semantically (As I said about John. He likes coffee.) Texts also relate to external contexts (cross reference) and their situations. This relationship is both implicit and explicit in the text. The shorter the text, the more context it needs for interpretation. For instance, if all we see is “He likes it.” written on a piece of paper, we do not have enough context to interpret the meaning.</p><p><strong>14. </strong>Language is not used uniformly. Some parts of language are used more frequently than others. But this is not enough to understand frequency. Some parts of language are used more frequently together than others. The frequent coocurrence of some words with other words is called “collocation”. This means that when we say “bread and …”, we can predict that the next word will be “butter”. You can check this with a linguistic tool like a corpus, or even by using Google’s predictions in the search. Some words are so strongly collocated with other words that their meaning is “tinged” by those other words (this is called semantic prosody). For example, “set in” has a slightly negative connotation because of its collocation with “rot”. So if you describe someone’s relationship as in ‘then love set in’, you’re likely not all that positive about that development.</p><p><strong>15. </strong>All language is idiomatic to some degree. You cannot determine the meaning of all sentences just by understanding the meanings of all their component parts and the rules for putting them together. And vice versa, you cannot just take all the words and rules in a language, apply them and get meaningful sentences. Consider “I will not put the picture up with John.” and “I will not put up the picture with John.” and “I will not put up John.” and “I will not put up with John.”</p><p><strong>16. </strong>Bilingualism is the norm in language knowledge, not the exception. About half the world’s population regularly speaks more than one language but everybody is “bi-lingual” in the sense that they deal with multiple codes in their language. They may seem like very close but for a child without much familial academic background, entering school and learning to read may feel very much like a foreign language and they often need the same sort of support learners of second languages need.</p><p><strong>17. </strong><a href=\"http://en.wikipedia.org/wiki/Controlled_natural_language\">Simplified languages</a> develop their own complexities if used by a real community through a process known as creolization. (This process is well described for pidgins but not as well for artificial languages.) This has implications for movements such as simple language campaigns. They ignore the fact that simple is relative to the needs of a speech community and not a universal measure.</p><p>More details and examples <a href=\"http://metaphorhacker.net/2012/09/the-complexities-of-simple-what-simple-language-proponents-should-know-about-linguistics/\">THE COMPLEXITIES OF SIMPLE: WHAT SIMPLE LANGUAGE PROPONENTS SHOULD KNOW ABOUT LINGUISTICS on MetaphorHacker.net</a></p><p>(Previously also appeared on Tumblr. Slightly edited here.)</p><p><strong>Post script:</strong> Will linguists agree with this list? That depends on what kind of linguistics they do. Many mathematical and computational linguists are not all that concerned with the subtleties of language or its use in social contexts. General linguists would probably agree in principle with most of these things but would add different qualifications.</p><p>But I’d say that in general, this list is less controversial than something seemingly more innocuous than what the definition of “word” is.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e078e3fe39c2\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/17-things-everyone-should-know-about-language-but-doesnt-even-know-to-ask-e078e3fe39c2\">17 Things Everyone Should Know About Language But Doesn’t Even Know to Ask</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Fruit loops and metaphors: Metaphors are not about explaining the abstract through concrete but…",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://medium.com/feed/metaphor-hacker",
        "value": "Fruit loops and metaphors: Metaphors are not about explaining the abstract through concrete but…"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://medium.com/metaphor-hacker/fruit-loops-and-metaphors-metaphors-are-not-about-explaining-the-abstract-through-concrete-but-4e0209dd70b6?source=rss----67ad910371b8---4"
        },
        {
          "rel": "license",
          "href": "http://creativecommons.org/licenses/by/4.0/"
        }
      ],
      "link": "https://medium.com/metaphor-hacker/fruit-loops-and-metaphors-metaphors-are-not-about-explaining-the-abstract-through-concrete-but-4e0209dd70b6?source=rss----67ad910371b8---4",
      "id": "https://medium.com/p/4e0209dd70b6",
      "guidislink": false,
      "tags": [
        {
          "term": "linguistics",
          "scheme": null,
          "label": null
        },
        {
          "term": "discourse",
          "scheme": null,
          "label": null
        },
        {
          "term": "philosophy",
          "scheme": null,
          "label": null
        },
        {
          "term": "metaphor",
          "scheme": null,
          "label": null
        },
        {
          "term": "framing",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Dominik Lukes"
        }
      ],
      "author": "Dominik Lukes",
      "author_detail": {
        "name": "Dominik Lukes"
      },
      "published": "Mon, 05 Mar 2018 23:15:45 GMT",
      "published_parsed": [
        2018,
        3,
        5,
        23,
        15,
        45,
        0,
        64,
        0
      ],
      "updated": "2018-03-06T06:30:55.138Z",
      "updated_parsed": [
        2018,
        3,
        6,
        6,
        30,
        55,
        1,
        65,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://medium.com/feed/metaphor-hacker",
          "value": "<h3>Fruit loops and metaphors: Metaphors are not about explaining the abstract through concrete but about the dynamic process of negotiated sensemaking</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/1*_T0MTlgi3p0CjzWNmCYDIg.jpeg\" /></figure><p>One of the less fortunate consequences of the popularity of the conceptual metaphor paradigm (which is also the one I by and large work with) is the highlighting of the embodied metaphor at the expenses of others. This gives the impression that metaphors are there to explain more abstract concepts in terms of more concrete ones.</p><blockquote>Wikipedia: “Conceptual metaphors typically employ a more abstract concept as target and a more concrete or physical concept as their source. For instance, metaphors such as ‘the days [the more abstract or target concept] ahead’ or ‘giving my time’ rely on more concrete concepts, thus expressing time as a path into physical space, or as a substance that can be handled and offered as a gift.“</blockquote><p>And it is true that many of the more interesting conceptual metaphors that help us frame the fundamentals of language are projections from a concrete domain to one that we think of as more abstract. We talk about time in terms of space, emotions in terms of heat, thoughts in terms of objects, conversations as physical interactions, etc. We can even deploy this aspect of metaphor in a generative way, for instance when we think of electrons as a crowd of little particles.</p><p>But I have come to view this as a very unhelpful perspective on what metaphor is and how it works. Instead, going back to Lakoff’s formulation in <em>Women, Fire, and Dangerous Things</em>, I’d like to propose we think of a metaphor as a principle that helps us give structure to our mental models (or frames). But unlike Lakoff, I like to think of these as incredibly dynamic and negotiated (conceptual blending gives us a good description of the process).</p><p>Metaphor does two things: 1. It helps us (re)structure one conceptual domain by projecting another conceptual domain onto it and 2. In the process of 1, it creates new (blended) conceptual domains.</p><p>We do not really understand one domain in terms of another through metaphor. We ‘understand’ both domains in different ways. And this helps us create new perspectives which are themselves conceptual domains that can be projected or projected into. (As described by Fauconnier and Turner in <em>The Way We Think</em>).</p><p>This makes sense when we look at some of the conventional examples used to illustrate metaphors. “The man is a lion” does not help us understand lesser known or more abstract ‘man’ by using the better known or more concrete ‘lion’. No, we actually know a lot more about men and the specific man we’re thus describing than we do about lions. We are just projecting domain of ‘lions’ including the conventionalised schemas of bravery and fierceness onto a particular man.</p><p>This perspective depends on our conventionalised way of projecting these 2 domains. Comparison between languages illustrates this further. The Czech framing of lions is essentially the same as English but the projection into people also maps lion’s vigour into work to mean ‘hard working’. So you can say “she works as a lion”, meaning she works hard. But in the age of documentaries about lions, a joke subverting the conventionalised mapping also appeared and people say “I work like a lion. I roar and go take a nap.” This is something that could only emerge as more became conventionally known about lions.</p><p>But even more embodied metaphors do not always go in a predictable direction. We often structure affective states in terms of the physical world or bodily states. We talk about ‘being in love’ or ‘love hitting a rocky patch’ or ‘breaking hearts’ (where metonymy also plays a role). But does that really mean that we somehow know less about love than we know about travelling on roads? Love is conventionally seen as less concrete than roads or hearts but here we allow ourselves to be mislead by traditional terminology. The domain of ‘love’ is richly structured and does not ‘feel’ all that abstract to the participants. (I’d prefer to think of ‘love’ as a non-prototypical noun; more prototypical than ‘rationalisation’ but less prototypical than ‘cat’).</p><p>Which is why ‘love’ can also be used as the source domain. We can say things like “The camera loves him.” and it is clear what we mean by it. We can talk about physical things “being in harmony” with each other and thus helping us understand them in different ways despite harmony being supposedly more abstract than the things being in harmony.</p><p>The conceptual domains that enter into metaphoric relationships are incredibly rich and multifaceted (nothing like the dictionaries or encyclopedias we often model linguistic meaning after). And the most important point of unlikeness is their dynamic nature. They are constantly adapting to the context of the listeners and speakers, never exactly the same from use to use. We have a rich inventory of them at our disposal but by reaching into it, we are also constantly remaking it.</p><p>We assume that the words we use have some meanings but it is us who has the meanings. The words and other structures just carry the triggers we use to create meanings in the process of the negotiation with the world and our interlocutors that language and cognition are.</p><p>But this sounds much more mysterious and ineffable than it actually is. These things are completely mundane and they are happening every time we open our mouths or our minds. Here’s a very simple but nevertheless illuminating illustration of the process.</p><p>Not too long ago, there were two TV shows that had some premise similarities (Psych and The Mentalist). One of them came out a year earlier and its creators were feeling like their premise was copied by the other one. And they used the following analogy:</p><blockquote>“When you go to the cereal aisle in a grocery store, and you see Fruit Loops there. If you look down on the bottom, there’s something that looks just like Fruit Loops, and it’s in a different bag, and it’s called Fruity Loop-Os.” <a href=\"http://th3tvobsessed.blogspot.co.uk/2009/08/psych-vs-mentalist.html\">http://th3tvobsessed.blogspot.co.uk/2009/08/psych-vs-mentalist.html</a></blockquote><p>I was watching both shows at the time but I did not notice that much similarity. But as soon as I read that comparison it was immediately clear to me what the speaker was trying to say. I could immediately see the projection between the two domains. But even though it seemed the cereal domain was more specific, it actually brought a lot more with it than the specificity of cereal boxes and their placement on store shelves. What it brought over was the abstract relationship between them in quality and value but also many cultural scripts and knowledge associated with cereal brands and their copycats.</p><p>But there was even more to it than that. The metaphor does not stop at its first outing (it’s kind of like mushrooms and their <a href=\"https://en.wikipedia.org/wiki/Rhizome\">rhizomes</a> in this way). Whenever, I see a powerful analogy or generative metaphor on the internet, I always look for the comments where people try to reframe it and create new meanings. Something I have been calling ‘frame negotiation’. Take almost any salient metaphoric domain projection and you will find that it is only a part in a process of negotiated sense making. This goes beyond people simply disagreeing with each other’s metaphors. It includes the marshalling of complex structuring conceptual phenomena from schemas, rich images, scenarios, scripts, to propositions, definitions, taxonomies and conventionalised collocations.</p><p>This blog post and its comments contain almost all of them: <a href=\"http://th3tvobsessed.blogspot.co.uk/2009/08/psych-vs-mentalist.html\">http://th3tvobsessed.blogspot.co.uk/2009/08/psych-vs-mentalist.html</a>. First, the post author spends three paragraphs (from third on), comparing the two shows and finding similarities and differences. This may not seem like anything interesting but it reveals that the conceptual blends compressed in the cereal analogy are completely available and can be discussed as if it was a literal statement of fact.</p><p>Next, the commenters, who have much less space, return to debating the proposition by recompressing it into more metaphors. These are the first four comments in full:</p><ol><li><em>Anonymous said… They’re not totally different. It’s more like comparing Fruit Loops to Fruit Squares which happen to taste like beef.</em></li><li><a href=\"http://www.nikki0417.deviantart.com/\"><em>Nikki0417</em></a><em> said… I think a better comparison would Corn Flakes and Frosted Flakes. Both are made with the same cereal, but one’s sweeter (Psych).</em></li><li><a href=\"https://www.blogger.com/profile/09117251732141325156\"><em>TV Obsessed</em></a><em> said… Sweeter as in more comedy oriented? They are vastly different shows that are different on so many levels.</em></li><li><em>Anonymous said… nikki could not be more right with the corn flakes and frosties analogy</em></li></ol><p>Here we see the process of sense making in action. The metaphoric projection is used as one of several structuring devices around which frames are made. Comment 1. Opens the the process by bringing in the idea of reframing through other analogs in the cereal domain. 2. Continues that process by offering an alternative. 3. Challenge the very idea of using these two domains and 4. Agrees with 2 as if this were a literal statement but also referring to the metalinguistic tool being used.</p><p>The subsequent comments return to comparing the two shows . Some by offering propositions and scenarios, others by marshalling a new analogy.</p><blockquote><a href=\"https://www.blogger.com/profile/12554378526046963007\">W. D. Stephenson</a> said… The reason the Mentalist feels like House is because house is a modern day medical version of Homes as in Holmes Sherlock. Also both Psych and The Mentalist are both Holmsian in creation. That being said I love the wit and humor of psych</blockquote><p>Again, there is no evidence of the concrete/abstract duality or even one between less and better known domains. It is all about making sense of the domains in both cognitive and affective ways. Some domains have very shallow projections (partial mappings) such as cornflakes and frosty flakes, others have very deep mappings such as Sherlock Holmes. They are not providing new information or insight in the way we traditionally think of them. Nor are they providing an explanation to the uninitiated. They are giving new structure to the existing knowledge and thus recreating what is known.</p><p>The reason I picked such a seemingly mundane example is because all of this is mundane and it’s all part of the same process. One of my disagreements with much of metaphor application is the overlooking of the ‘boring’ bits surrounding the first time a metaphor is used. But metaphors are always a part of a complex textual and discursive patterns and while they are not parasitic on the literal as was the traditional slight against them, they are also not the only thing that goes one when people make sense.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4e0209dd70b6\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/fruit-loops-and-metaphors-metaphors-are-not-about-explaining-the-abstract-through-concrete-but-4e0209dd70b6\">Fruit loops and metaphors: Metaphors are not about explaining the abstract through concrete but…</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        }
      ],
      "summary": "<h3>Fruit loops and metaphors: Metaphors are not about explaining the abstract through concrete but about the dynamic process of negotiated sensemaking</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/1*_T0MTlgi3p0CjzWNmCYDIg.jpeg\" /></figure><p>One of the less fortunate consequences of the popularity of the conceptual metaphor paradigm (which is also the one I by and large work with) is the highlighting of the embodied metaphor at the expenses of others. This gives the impression that metaphors are there to explain more abstract concepts in terms of more concrete ones.</p><blockquote>Wikipedia: “Conceptual metaphors typically employ a more abstract concept as target and a more concrete or physical concept as their source. For instance, metaphors such as ‘the days [the more abstract or target concept] ahead’ or ‘giving my time’ rely on more concrete concepts, thus expressing time as a path into physical space, or as a substance that can be handled and offered as a gift.“</blockquote><p>And it is true that many of the more interesting conceptual metaphors that help us frame the fundamentals of language are projections from a concrete domain to one that we think of as more abstract. We talk about time in terms of space, emotions in terms of heat, thoughts in terms of objects, conversations as physical interactions, etc. We can even deploy this aspect of metaphor in a generative way, for instance when we think of electrons as a crowd of little particles.</p><p>But I have come to view this as a very unhelpful perspective on what metaphor is and how it works. Instead, going back to Lakoff’s formulation in <em>Women, Fire, and Dangerous Things</em>, I’d like to propose we think of a metaphor as a principle that helps us give structure to our mental models (or frames). But unlike Lakoff, I like to think of these as incredibly dynamic and negotiated (conceptual blending gives us a good description of the process).</p><p>Metaphor does two things: 1. It helps us (re)structure one conceptual domain by projecting another conceptual domain onto it and 2. In the process of 1, it creates new (blended) conceptual domains.</p><p>We do not really understand one domain in terms of another through metaphor. We ‘understand’ both domains in different ways. And this helps us create new perspectives which are themselves conceptual domains that can be projected or projected into. (As described by Fauconnier and Turner in <em>The Way We Think</em>).</p><p>This makes sense when we look at some of the conventional examples used to illustrate metaphors. “The man is a lion” does not help us understand lesser known or more abstract ‘man’ by using the better known or more concrete ‘lion’. No, we actually know a lot more about men and the specific man we’re thus describing than we do about lions. We are just projecting domain of ‘lions’ including the conventionalised schemas of bravery and fierceness onto a particular man.</p><p>This perspective depends on our conventionalised way of projecting these 2 domains. Comparison between languages illustrates this further. The Czech framing of lions is essentially the same as English but the projection into people also maps lion’s vigour into work to mean ‘hard working’. So you can say “she works as a lion”, meaning she works hard. But in the age of documentaries about lions, a joke subverting the conventionalised mapping also appeared and people say “I work like a lion. I roar and go take a nap.” This is something that could only emerge as more became conventionally known about lions.</p><p>But even more embodied metaphors do not always go in a predictable direction. We often structure affective states in terms of the physical world or bodily states. We talk about ‘being in love’ or ‘love hitting a rocky patch’ or ‘breaking hearts’ (where metonymy also plays a role). But does that really mean that we somehow know less about love than we know about travelling on roads? Love is conventionally seen as less concrete than roads or hearts but here we allow ourselves to be mislead by traditional terminology. The domain of ‘love’ is richly structured and does not ‘feel’ all that abstract to the participants. (I’d prefer to think of ‘love’ as a non-prototypical noun; more prototypical than ‘rationalisation’ but less prototypical than ‘cat’).</p><p>Which is why ‘love’ can also be used as the source domain. We can say things like “The camera loves him.” and it is clear what we mean by it. We can talk about physical things “being in harmony” with each other and thus helping us understand them in different ways despite harmony being supposedly more abstract than the things being in harmony.</p><p>The conceptual domains that enter into metaphoric relationships are incredibly rich and multifaceted (nothing like the dictionaries or encyclopedias we often model linguistic meaning after). And the most important point of unlikeness is their dynamic nature. They are constantly adapting to the context of the listeners and speakers, never exactly the same from use to use. We have a rich inventory of them at our disposal but by reaching into it, we are also constantly remaking it.</p><p>We assume that the words we use have some meanings but it is us who has the meanings. The words and other structures just carry the triggers we use to create meanings in the process of the negotiation with the world and our interlocutors that language and cognition are.</p><p>But this sounds much more mysterious and ineffable than it actually is. These things are completely mundane and they are happening every time we open our mouths or our minds. Here’s a very simple but nevertheless illuminating illustration of the process.</p><p>Not too long ago, there were two TV shows that had some premise similarities (Psych and The Mentalist). One of them came out a year earlier and its creators were feeling like their premise was copied by the other one. And they used the following analogy:</p><blockquote>“When you go to the cereal aisle in a grocery store, and you see Fruit Loops there. If you look down on the bottom, there’s something that looks just like Fruit Loops, and it’s in a different bag, and it’s called Fruity Loop-Os.” <a href=\"http://th3tvobsessed.blogspot.co.uk/2009/08/psych-vs-mentalist.html\">http://th3tvobsessed.blogspot.co.uk/2009/08/psych-vs-mentalist.html</a></blockquote><p>I was watching both shows at the time but I did not notice that much similarity. But as soon as I read that comparison it was immediately clear to me what the speaker was trying to say. I could immediately see the projection between the two domains. But even though it seemed the cereal domain was more specific, it actually brought a lot more with it than the specificity of cereal boxes and their placement on store shelves. What it brought over was the abstract relationship between them in quality and value but also many cultural scripts and knowledge associated with cereal brands and their copycats.</p><p>But there was even more to it than that. The metaphor does not stop at its first outing (it’s kind of like mushrooms and their <a href=\"https://en.wikipedia.org/wiki/Rhizome\">rhizomes</a> in this way). Whenever, I see a powerful analogy or generative metaphor on the internet, I always look for the comments where people try to reframe it and create new meanings. Something I have been calling ‘frame negotiation’. Take almost any salient metaphoric domain projection and you will find that it is only a part in a process of negotiated sense making. This goes beyond people simply disagreeing with each other’s metaphors. It includes the marshalling of complex structuring conceptual phenomena from schemas, rich images, scenarios, scripts, to propositions, definitions, taxonomies and conventionalised collocations.</p><p>This blog post and its comments contain almost all of them: <a href=\"http://th3tvobsessed.blogspot.co.uk/2009/08/psych-vs-mentalist.html\">http://th3tvobsessed.blogspot.co.uk/2009/08/psych-vs-mentalist.html</a>. First, the post author spends three paragraphs (from third on), comparing the two shows and finding similarities and differences. This may not seem like anything interesting but it reveals that the conceptual blends compressed in the cereal analogy are completely available and can be discussed as if it was a literal statement of fact.</p><p>Next, the commenters, who have much less space, return to debating the proposition by recompressing it into more metaphors. These are the first four comments in full:</p><ol><li><em>Anonymous said… They’re not totally different. It’s more like comparing Fruit Loops to Fruit Squares which happen to taste like beef.</em></li><li><a href=\"http://www.nikki0417.deviantart.com/\"><em>Nikki0417</em></a><em> said… I think a better comparison would Corn Flakes and Frosted Flakes. Both are made with the same cereal, but one’s sweeter (Psych).</em></li><li><a href=\"https://www.blogger.com/profile/09117251732141325156\"><em>TV Obsessed</em></a><em> said… Sweeter as in more comedy oriented? They are vastly different shows that are different on so many levels.</em></li><li><em>Anonymous said… nikki could not be more right with the corn flakes and frosties analogy</em></li></ol><p>Here we see the process of sense making in action. The metaphoric projection is used as one of several structuring devices around which frames are made. Comment 1. Opens the the process by bringing in the idea of reframing through other analogs in the cereal domain. 2. Continues that process by offering an alternative. 3. Challenge the very idea of using these two domains and 4. Agrees with 2 as if this were a literal statement but also referring to the metalinguistic tool being used.</p><p>The subsequent comments return to comparing the two shows . Some by offering propositions and scenarios, others by marshalling a new analogy.</p><blockquote><a href=\"https://www.blogger.com/profile/12554378526046963007\">W. D. Stephenson</a> said… The reason the Mentalist feels like House is because house is a modern day medical version of Homes as in Holmes Sherlock. Also both Psych and The Mentalist are both Holmsian in creation. That being said I love the wit and humor of psych</blockquote><p>Again, there is no evidence of the concrete/abstract duality or even one between less and better known domains. It is all about making sense of the domains in both cognitive and affective ways. Some domains have very shallow projections (partial mappings) such as cornflakes and frosty flakes, others have very deep mappings such as Sherlock Holmes. They are not providing new information or insight in the way we traditionally think of them. Nor are they providing an explanation to the uninitiated. They are giving new structure to the existing knowledge and thus recreating what is known.</p><p>The reason I picked such a seemingly mundane example is because all of this is mundane and it’s all part of the same process. One of my disagreements with much of metaphor application is the overlooking of the ‘boring’ bits surrounding the first time a metaphor is used. But metaphors are always a part of a complex textual and discursive patterns and while they are not parasitic on the literal as was the traditional slight against them, they are also not the only thing that goes one when people make sense.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4e0209dd70b6\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/fruit-loops-and-metaphors-metaphors-are-not-about-explaining-the-abstract-through-concrete-but-4e0209dd70b6\">Fruit loops and metaphors: Metaphors are not about explaining the abstract through concrete but…</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Learning vs training in machines and organizations: Production of knowledge vs production of…",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://medium.com/feed/metaphor-hacker",
        "value": "Learning vs training in machines and organizations: Production of knowledge vs production of…"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://medium.com/metaphor-hacker/learning-vs-training-in-machines-and-organizations-production-of-knowledge-vs-production-of-f1f9e6c1d9f3?source=rss----67ad910371b8---4"
        },
        {
          "rel": "license",
          "href": "http://creativecommons.org/licenses/by/4.0/"
        }
      ],
      "link": "https://medium.com/metaphor-hacker/learning-vs-training-in-machines-and-organizations-production-of-knowledge-vs-production-of-f1f9e6c1d9f3?source=rss----67ad910371b8---4",
      "id": "https://medium.com/p/f1f9e6c1d9f3",
      "guidislink": false,
      "tags": [
        {
          "term": "learning",
          "scheme": null,
          "label": null
        },
        {
          "term": "artificial-intelligence",
          "scheme": null,
          "label": null
        },
        {
          "term": "machine-learning",
          "scheme": null,
          "label": null
        },
        {
          "term": "metaphor",
          "scheme": null,
          "label": null
        },
        {
          "term": "organizational-culture",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Dominik Lukes"
        }
      ],
      "author": "Dominik Lukes",
      "author_detail": {
        "name": "Dominik Lukes"
      },
      "published": "Tue, 06 Feb 2018 21:27:27 GMT",
      "published_parsed": [
        2018,
        2,
        6,
        21,
        27,
        27,
        1,
        37,
        0
      ],
      "updated": "2018-02-14T21:56:44.460Z",
      "updated_parsed": [
        2018,
        2,
        14,
        21,
        56,
        44,
        2,
        45,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://medium.com/feed/metaphor-hacker",
          "value": "<h3>Learning vs training in machines and organizations: Production of knowledge vs production of capability — Exploration of metaphors</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EAv0CPYrJ99vgSIkp-jnJg.jpeg\" /></figure><h3>Machine learning is to organizational learning as…</h3><p>A <a href=\"https://www.sbs.ox.ac.uk/school/events-0/david-upton-memorial-lecture\">recent lecture by Gary Pisano</a> made the parallel between machine learning and organizational learning in the sense that they are both processes that produce knowledge. My immediate reaction was that they are not alike because machine learning is really a misnomer. Or rather machine learning has some aspects of learning but is dissimilar from prototypical learning in enough ways to make the analogy dangerous. But my second thought was that actually organizational learning and prototypical learning are dissimilar in the same ways that machine learning and prototypical learning are. We could then learn more about both if we look at the various mappings and mismappings (a sort of metaphor hack).</p><p>What follows is a sketch of some of the ways we could think this through.</p><h3>Prototype knowledge frame</h3><p>I would propose that the prototype (frame) of learning is built around the acquisition of communicable and demonstrable knowledge. We learn history, physics, maths, medicine and the way we show we’ve learned these subjects is by demonstrating knowledge of them. This, of course, is not all there is to the frame of learning. We also learn to swim, learn to speak French, learn to solve problems. These are examples of the acquisition of what we could call capabilities. This is well reflected in educational theory and practice such as when dividing learning objectives into knowledge, skills and attitudes (KSA). But if we look at language and behavior, our default framing tends to learning = knowledge. A quick survey of most curricular objectives will be very heavily focused on on knowledge. Equally, most objections about poor education are illustrated by lack of knowledge of things.</p><h3>Defining knowledge and capability</h3><p>For the purposes of this exercise, I would define knowledge as something that can be communicated in a significantly shorter time than it can be acquired. It may take me days to memorise all the state capitals but I can recite them in minutes. Or it can take me years to research a topic for a book, months to write it, and days for someone to read it. Capability then could be defined as something that requires constant time to acquire given the same context and the same method of acquisition. And we have a word for the aquisition of capability, namely training.</p><p>Of course, both knowledge and capability can be defined in many other ways and a similar distinction could be made using different terms. Some terms that come to mind as alternatives to capability are phronesis, practical knowledge, or tacit knowledge, skill, etc. They are not really the same beast but they graze in the same semantic fields.</p><p>A great way to illustrate this further is the distinction used in linguistics between knowledge about and knowledge of language (this could be enriched by the famous competence/performance distinction but I’ll set that aside for now by focusing only on competence). I know quite a lot about the grammar of Swahili, yet, I do not ‘know’ Swahili or I don’t have any capability to speak it. However, I ‘know’ Czech and (having written a grammmar of Czech), I know a lot about Czech. I can communicate my knowledge about Czech to someone in an amount of time it takes to speak or write. However, my knowledge of Czech cannot be communicated. I have to take someone on a journey similar to mine (in time and effort) — or, in other words, train them.</p><h3>Machine learning is machine training</h3><p>Now, machine learning is really machine training. Current algorithms are trained on data sets with certain parameters and will assign certain probabilities (weights) to nodes on a decision tree (this may be direct with real decision trees or indirect with neural networks where the decision trees are multidimensional matrices). But no knowledge that can be communicated is produced. Unlike with humans, I can take the weights produced by the learning algorithm and transplant them to another computer but I cannot communicate what those weights mean. And I cannot easily add to the knowledge in the way I can add more facts to my knowledge about something. I can add more training data to improve the weights for an intended purpose or I can use the algorithm to train on a new data set and produce entirely different weights. But in both cases, I am not producing or communicating knowledge. I am replicating a training method. That’s why machine learning plays a relatively small role in something like climate modelling. You can train it on patterns and produce predictions in fractions of the time it takes to run a complex climate simulation (which could be literally months of computing). But this learning is a complete black box. You cannot extract communicable knowledge from a neural network. And because the feedback loop of testing on climate processes is measured in decades, it is impossible to rely on models produced solely by machine learning (I’m simplifying, of course).</p><h3>Knowledge in organizational learning</h3><p>How about organizational learning? Well, it’s possible to produce a lot of knowledge about organizations. Make lists, taxonomies, charts and graphs. And parts of this knowledge can be and routinely are communicated. When I join a new organization, I am told about all (or many) of the structures and rules. But as the anthropologist James C Scott points out, these rules are parasitic on the actual complexity of actions and relationships that are created in any society or organization to make it work. That’s why following the rules exactly is tantamount to <a href=\"https://en.wikipedia.org/wiki/Work-to-rule\">a strike</a>. The rules and structures are idealizations that express a certain vision (or model) of what is happening but I cannot be successful with just knowing about them. I have to spend time in the organization to learn them in the same way I would have to spend some time in Kenya or Tanzania to really learn Swahili.</p><p>It is well known that organizational learning cannot be simply communicated across organizations. That’s because it does not produce knowledge. It produces capabilities. That’s why it’s not easy to just ‘bring back computer manufacturing to the US’. Manufacturing a chip is a known process. With enough money, anyone can buy a fab. But even with the levels of automation in the chip manufacturing process, the levels of organizational learning (which includes the learning of all the relationships in the society of organizations and the workforce available in a region) simply cannot be created out of whole cloth in the same way that a manufacturing line can be. You need to train the organizations to acquire the same capabilities.</p><p>Now, that does not mean that there is no communicable knowledge being produced as part of organizational learning. But it is produced by people in the organization not the organization itself. And it is the knowledge of more efficient structures and processes to make sure organizations can achieve the kinds of capabilities that will lead them to success. In the same way, we can get better and better training methodologies for languages, musical instruments, or sports (described in great detail in Ericsson’s <em>Peak</em>). They can reduce the amount of time or effort to achieve certain capabilities. But in competitive environments like sports, the total amount of effort and time is constant because everyone is using the same methods, so the average performance improves but the effort is constant (ie. it takes much less time and effort today to become a top athlete or virtuoso pianist of 50 years ago but as much time to reach the peak performance of today.) And with some more complex and harder to measure capabilities like social skills, speaking a language, or becoming a good manager, the time and effort reduction is minimal even with better training methods.</p><h3>Additive and non-additive learning</h3><p>The same applies to organizational and machine learning. It is not that lessons from organizational learning cannot be learned and transported. But each organization has to go through the process to train itself to acquire the capabilities. And there’s a bottom limit on how quickly that can happen. It will be months to years for single organizations and years to decades for interconnected regional and supraregional industries. That is why huge mergers hoping to capitalize on synergies so often fail. The merger drivers assume that the organizational learnings are additive but they are mostly not. If you get two people who speak a year’s worth of French, they can certainly build on each other’s competence (assuming it is complementary) but they will not be as proficient as somebody who speaks two years worth of French. They might even be less proficient together than they would be individually because of the overhead it takes them to negotiate who knows what and who should speak when.</p><p>This is also the case for machine learning algorithms (or neural networks). You cannot add together their joint abilities in the same way you could add up two parts of a knowledge repository (like an encyclopedia). Two speech recognition systems that can each recognize speech with 50% accuracy cannot be added together to get 100% accuracy even assuming their coverage was complementary. You would still need another expert system to decide which was the accurate transcription in every case — and if you had that, you might as well use that to do the transcription. But such systems do not exist as we know from P=NP for problems where correctness is easier to check than it is to arrive at an answer.</p><p>However, that does not mean that additive complementarity is impossible. On the contrary, where an easy decision algorithn is available, complementarity is incredibly powerful. Hypothetically, if we had one speech recognition system for male and one for female voices, it would be much easier to come up with a third system for distinguishing which system to use in every particular case. The same could apply to the two limited French speakers. If their domains of competence were clearly delineated, their skills could be additive. Say if one could order in a restaurant and the other ask for directions to the restaurant.</p><p>The easiest and most straightforward decision algorithm is sequentiality. If I know two stages of a process come after one another, I can train a system on them separately and then add them together. This is what much of deep learning is about. The same goes for organizations. One path to success (suggested by research) for a secodary school is to aqcuire a primary school. That way it can improve its results simply by improving inputs. Apple has done the same with chip design. (Although there is a limit to how much this can be done — for instance in manufacturing, given that the quality of a supply chain often arises from its diversity, a car manufacturer acquiring manufacturers of all parts may make temporary gains in quality at the expense of long-term improvement.)</p><p>But this is a different kind of complementarity than that of adding together two encyclopedias or dictionaries with complementary entries. We don’t get one improved trained unit but two separate complementary units. In fact, in this sense, we could say that machine learning only works because of non-intersecting additive complementarity with human learning. Humans have to decide where and when machine learning algorithms can be deployed. They provide intentionality and complex embodied judgement. They also provide something machines do not have, which is metacognition.</p><p>Giving a person a calculator will magnify their abilities, giving a chess master a chess computer will also give them a definite edge. But we cannot upload the processing capabilities of the calculator into a person’s brain and make them into one system. All tool-using is a case of additive complementarity but only on the surface. An organization may use another to complement its capabilities but a merger of the two often does not result in the same outcome. The complexities of merging the myriad of synapse-like connections and system activations proves an impossible task even for the most intelligent of designers. The two merged organizations simply have to learn how to be one.</p><h3>Can machine learning simulate organizational learning?</h3><p>Which brings us to the question also raised by <a href=\"https://www.sbs.ox.ac.uk/school/events-0/david-upton-memorial-lecture\">Gary Pisano</a> which is the ability of machine learning to simulate organizational learning. This is easily imagined in the case of highly mechanized environments where machine learning is necessary to process the amounts of data generated. Or in highly complex environments where humans have a well-known cognitive limitation — for instance, organizing the best rotas and paths for workers in a warehouse. But even here we run up against the limits of complexity. NP-hard problems are NP-hard problems even for learning machines.</p><p>It is possible that some complex system problems are simply not machine learnable but rather the systems themselves compute the solution which emerges from their interaction. Here we may find the Hayekian problem of knowledge in society relevant. He was, of course, arguing with people who believed it was possible to centrally allocate resources within the economy with sophisticated algorithms using new data collection methods (back then computed by human ‘computers’). His point was that the market using the price signal can achieve better efficiency through self-organization. Efficient central planning, Hayek argued, was impossible in principle because we cannot collect enough data and even if we could, we could not compute them. It seems that Hayek have been correct in the sense of the computational tractability of perfect market efficiency, but is it possible that a sufficiently powerful machine learning system could outperform the computational efficiency of the market itself?</p><p>Or could this be at least achieved at the scale of very large organizations? Is it even needed? Massive conglomerates have been successfully managed through more direct methods for decades with little recourse to neural-net-based-AI which have only produced anything resembling useful results in the last 10 years.</p><p>It is certainly not possible at the current state of machine learning and with the available data. The advances in the field of learning algorithms necessary to make that leap illustrate an important difference between current machine learning and organizational learning. Organizations can organize and process their own inputs — whereas machine learning requires significant human effort to create the kinds of data structures and interpretations on which the machine learning algorithms can be applied. Organizations are analog computers operating directly on the data whereas machine learning always needs to convert the input into discrete units that can be expressed as vectors.</p><h3>Types of machine learning as metaphor</h3><p>There are three broad types of machine learning: 1. supervised, 2. unsupervised and 3. reinforcement. (These are often combined in actual deployed systems.)</p><p>1) Supervised learning simply means that the system (such as neural net) is trained on a highly processed, extremely large set of labeled data. For instance, pictures with labels of what’s in them or pairs of sentences and their translations. The network can then essentially compute the likelihoods of new data matching one of the labels. Or rather these likelihoods arise out of the complex matrix of activations across millions of parameters. Something that is completely computationally infeasible otherwise. What is important to know, that there is no list of ‘if-then’ statements anywhere within the system of the type ‘if object is round, increase likely hood object is a ball by x%’. (This is what the first wave of AI systems used and it seems to me to be what most people still imagine that machine learning produces.) We get extremely great aggregate results but no way of accounting for errors such as mistaking a toothbrush for a baseball bat. That is why these systems are susceptible to unpredictable adversarial attacks — for instance, understanding a seemingly random noise as a spoken command. Nevertheless, supervised learning has produced almost all of the useful and most directly visible outcomes of recent machine learning advances such as image recognition, machine translation or speech transcription.</p><p>2) Unsupervised learning is simply identifying regularities and irregulaties (patterns) in an unlabeled set of data. For instance, asking such an algorithm to classify words in a corpus of 10s of millions of English words, would give us a classification similar (but not identical) to what we know as nouns and verbs. A useful application of this is in network security where a machine learning system can learn typical patterns and then identify irregularities. This could also be used to identify potential credit card fraud.</p><p>3) The final type is reinforcement machine learning. This was used to teach AlphaGo to defeat the world champion. The system played many millions of games against human opponents (through past records — not in real life) and then itself. It used its failures and successes to adapt the weightings of moves and counter moves that lead to success. It is also great for training robots to navigate complex and changing environments.</p><p>A special case of reinforcement learning is evolutionary computation (although it could also be seen as separate). It can for instance combine multiple reinforcement learning agents working in parallel and testing out different solutions with assigning different fitness weights to them. This can be enromously powerful when A/B testing web design.</p><h4>Aside: What’s missing in recent advances in machine learning?</h4><p>Before we proceed to think about the analogies between machine learning and organizational learning, we should be very careful about how this machine learning revolution came about. It has three sources: 1) new computational methods to employ applied mathematics (such as differential calculus) to the design of networks of weighed nodes — in other words advances in software engineering and applied mathematics; 2) advances in computational power (such as graphics cards) and memory capacity that make these new methods feasible (such as increasing the number of layers of neural nets from 1 to 3 or even more); 3) availability of large amounts of structured or semi-structured machine readable data.</p><p>But nowhere on this list of accomplishments are advances in our understanding of how the mind, language or culture work. Quite the opposite. Fred Jelinek (a speech recognition pioneer) supposedly once quipped that the more linguists he fired from his lab, the better outputs he got. I’m not surprised. All of our understanding of how the mind or language work is either computationally infeasible (in the sense of incomputable collections of if-then statements across seemingly infinitely many dimensions) or too unstructured to be amenable to machine learning approaches (for instance, traditional semantics or semiotics or anthropology — even if we could structure this data — like in FrameNet or even WordNet, we could still not collect enough of it).</p><h4>Caveat: The seductiveness of successful metaphor</h4><p>It is very seductive to conclude that just because the new machine learning (neural networks known as deep learning) is essentially inspired by how we think the brain is architected, it is more likely to produce human-like intelligence (AGI). But that is just as wrong as saying that just because airplanes have been initially inspired by how birds fly, we will soon have the capacity to fly like the birds. Airplane flight looks like bird flight (wings) and relies on the same fundamental physics. But it is achieved completely differently from bird flight and it does not only not make us able to fly like birds. It also contains no pathway to that ability. I suspect that neural networks are the same.</p><h3>What kind of machine learning is organizational learning most like?</h3><p>Given all that, it is not difficult to see that organizational learning is not similar to all three types of machine learning. It is mostly like reinforement learning (although business schools teach it in a supervised learning mode). And we can also envision that reinforcement learning algorithms with the assistance of unsupervised learning pattern recognition would be where the most fruitful approaches to achieve AI-managed organizations. Evolutionary algorithms could even simulate the market and achieve efficiencies without the inconvenience of actual market failure. However, it is equally possible that we will soon run against the limits of the sort of data we can provide these algorithms as inputs. It is far too easy to underestimate the importance of information architecture and data analysis.</p><p>So can we think about an organization learning as a type of reinforcement learning? We certainly can but with important caveats. The useful parallel is the complexity of the learning that arises out of massive amounts of connections with constantly changing weights. Which means that the system has learned to respond in useful ways to variations in inputs and produce usable outputs. But we should remember that the learning is a bit of a black box and we cannot simply extract the knowledge as a series of if-then rules which business case studies so often try to do.</p><p>However, the actual topology of the network is infinitely more complex with too many nodes and connections to identify. But even more importantly, the nodes in an organizational networks are very complex agents rather than just passive probability repositories. This means that organizations are much more able to adapt to change. The famous AlphaGo system would have been stumped if it was asked to play go on a board of 20x20 instead of 19x19. A human player would adapt easily.</p><h4>Aside: Learning from single examples, the human way</h4><p>The difference between human individual and a machine learning algorithm is instructive. Machine learning needs inhuman amounts of data. Humans can learn from single examples by techniques such as analogy or metacognition. For instance, it may take me hundreds of tries to learn that a particular animal is a dog but only one time of seeing a platypus. I can also build on previous experience in creative ways. It took me months to learn to play anything resembling a tune on the guitar and years to play it well. Yet, it took me less than an hour to play a song on a Ukulele and a few months to play it well. I also have all sorts of knowledge about my knowledge that helps. So I know that if I want to know what letter comes after e, I just have to say a, b c, d, e and ‘f’ will pop into my head in a way it does not when I just say ‘e’. I also know a lot of things about the world in a completely non-propositional way that helps me immediately recognize that a baby holding a baseball bat is more unusual than a baby holding a toothbrush. I did not have to learn this as a fact or even as one of the many configurations of the world. I just know it. And without all of the above human learning and intelligence are inconceivable. And none of it is in any way present in machine learning.</p><p>But how about organizations? Are they more like individual humans or more like machine learning systems? They are more like machine learning systems in that they need to learn as a whole to adapt to new conditions. When a new regulation or manufacturing method is introduced, it takes more time for an organization to adapt to it than it might take any individual human within that organization. Which is why it takes time when a new manufacturing process is introduced before new levels of productivity can be reached.</p><p>However, because an organization does not consist of dumb neurons that can only adapt their individual weights but humans who are capable of complex adjustments, there are many types of changes that it can adapt to easily. For instance, I may send out an email to everyone working in an office that the back door can no longer be used. This may completely throw a machine learning system trained on a situation where the backdoor could be used. However, the individual humans will make the adjustments very easily and the whole organization will reroute its activity around that without having to relearn everything about how it works.</p><h3>Negotiating the metaphor</h3><p>So the trick for a planner, leader or organizational designer is to figure which learning metaphor is more apt for any given example of organizational change. Is this a kind of change an organization will have to learn in the same way a machine learning system is trained on new data or is it more like the kind of change a human can adapt to almost without breaking a stride? Or is it the kind kind of change that even an individual human will find difficult? Such as learning to play the trumpet having mastered the ukulele.</p><h3>Oversimplifications as generative metaphors: A how to guide</h3><p>Having spent so much time showing the different ways in which machine learning, organizational and human learning are not alike, it might seem logical to conclude that we should ditch all such comparisons. But that would be extremely unwise. Generative metaphors work best when there is more dissimilarity between two domains of comparison than similarity. But as long as we know enough to avoid the seduction by analogical success and the isomorphism fallacy, we should feel free to explore. One way to do this is to see what comes out if we look at one domain in terms of another.</p><p>So it may be actually extermely useful to recast organizations as machine learning systems based around reinforcement. One of the problems our models suffer from is what Bayesian statisticians call overfitting. This happens in extremely complex environments when we try to add additional variables to a model to make it reflect reality more. But because the reality is very complex and noisy, we can never get all the parameters. So we are just as likely to pick ones that will fit the model to random noise as ones that will make it more useful. Therefore, if we simplify the organization as just a network of interconnected nodes with changing weights (neurons and synapses) we may be able to think about organizational learning in ways that will make it more tractable. The more we try to complicate the model by giving the nodes structure, the less tractable it will be and the more noise it will generate.</p><p>So what can this generative simplification get us? Well, it may help us think about things like iterations, input data and data structures. This way we can design better environments in which organizational learning succeeds or identify key elements in designs that already proved successful.</p><p>But we may also be minded to take into account that organizational learning is a black box that is conditioned by the training data. That may make us skeptical about rich narratives of change based around the behavior of certain agents. They may be significant but the model may be better off without them.</p><p>Then we must be mindful not to forget that those thick descriptions capture realities from the perspectives of those involved and that we cannot describe the behavior of the individual in terms of the system in the same way we cannot describe the behaviour of nodes in a machine learning network as learning. Thus statements such as ‘the organization learned because its members learned’ may become slightly suspect within this model. But again this also does not mean that the individual’s learning is irrelevant but just maybe that it was not part of the organization itself learning as a black box machine learning system. So we may start thinking about what role does an individual’s knowledge play in the organization? What does it mean when an organization ‘learns’ for its mistakes, etc.</p><p>The key thing is to remember that at every analogical step we should reaffirm how closely tied our statements are to the parameters of our model and that the partial success of the model does not make it more real than richer models that may not be quite as useful due to their computational intractability.</p><h3>Conclusion</h3><p>So what is the purpose of all this? The various dimensions of machine and organizational learning are already well known. But perhaps contrasting the various models of learning can provide useful models for thinking about any one of them. As in any analogical chain, it is as important to identify the discontinuities as it is to identify continuities. Otherwise, we are in the danger of overfitting — or the isomorphism fallacy. Machine, organizational and human learning have many parallels but they are not the same. They have many superficial similarities and deep dissimilarities.</p><p>Perhaps the key lesson is the distinction between knowledge and capability. Organizations and machines do not produce anything resembling knowledge. And perhaps acknowledging that can be useful. But organizations are different from trained neural networks because their components contain knowledge which can be harnessed for training them on new circumstances. All of that could be done without the machine learning analogy but this was my journey. Perhaps it can be of some use.</p><h3>Final Caveat: Prehistory of machine learning: Teaching computers rules and words then and now</h3><p>One of Stephen Pinker’s most powerful and most deeply misleading book titles is ‘Rules and words’. It is a book showing the power of the computational model of language that can take 2 sets of finite inputs (rules and words) and generate an infinite set of patterns (sentences of a language). This could be seen as the last gasp of the old way. When I started studying linguistics exactly 10 years before the book was published, it was the mainstream that still gleamed with the shine of the new. But when the book came out, nobody was doing anything interesting with the rules and words model. The computational model of the mind and language took classical structured descriptions of how we speak and think and converted them into computationally tractable structures (logical trees — not decision trees — and rewriting rules operating on them). This was the system underlying all expert systems developed in the 70s, 80s and early 90s and it led to AI being seen as a failed paradigm. That’s not to say that it was useless, it produced much of the data infrastructure that was then later used to train machine learning algorithms. And it also produced some basic working examples of machine translation and speech recognition. But it never produced anything even remotely resembling intelligence. It struggled to reliably translate even something so seemingly simple and routine as weather forecasts between 2 related languages.</p><p>There is an interesting paradox when it comes to computational power. Rule-based machine expert systems are much more computationally tractable when dealing with small sets of structured data. You can even do it on a piece of paper. However, they do not scale almost at all when confronted with the combinatorial complexity of the real world. The number of rules required to handle even the simplest words (such as ‘dog’) in all possible contexts is too monumentally staggering that even were the Moore’s law continue, it would still likely not catch up with the needs.</p><p>Stochastic approaches and neural nets on the other hand, can produce nothing of any real use whatsoever with little data or little processing power. But they scale wonderfully. Once you can run millions of iterations on hundreds of millions of tokens in training sets, they only get more useful. They can only learn regularities that exist in data that can be presented to them (so there is a limit to how far they can go) but if we tried to express those same regularities through rules, we would never get anywhere. We couldn’t come up with that many rules and if we did, we could not compute the outcomes.</p><p>But the rule-based systems are more consononant with our intuitions about how the mind works. Many of their foundations go back to the first philosophers be they in Greece, India or China. That’s what made the early AI systems like <a href=\"https://medium.com/feed/&lt;span class=&quot;inline-comment&quot;&gt;en.wikipedia.org/wiki/ELIZA\">ELIZA</a> so seductive. The rule-based AIs were obviously like us, we just needed to spend more time adding rules, and we’d be there in no time. But this was wrong. Because when we write down these rules we are cheating. The amount of rich intricate knowledge required to make sense even of trivial sentences like ‘The cat is on the mat’ is mind boggling. What are the rules for choosing ‘the’ instead of ‘a’ in one or the other word? Why do we say a bird is ‘in’ the tree but ‘on’ a perch? In Czech we say ‘on’ the tree, so why did English pick this? And how do we encode distinctions such as ‘in’ for tight and loose fit in some languages? How about local knowledge such as knowing that the mat is valuable so the cat being on is a bad thing or that that the cat could also be in the larder so its being on the mat is a good thing. Sentences in language analysis and logical propositions are not idealised examples of language or reason. They are snapshots of <a href=\"http:&lt;/span&gt;metaphorhacker.net/2014/11/what-language-looks-like-dictionary-and-grammar-are-to-language-what-standing-on-one-foot-is-to-running/\">unnatural poses we manipulate them into</a> when trying to take a look at them through themselves.</p><p>When I was first studying these approaches, stochastic processing was the timid new kid on the block but 10 years later, it was the source of all major breakthroughs in expert systems developments. But they hit a wall in the mid 00s, and new improvements were slow in coming. In the 1990s, connectionism and neural nets which had been the big hope for the future since the 80s were eclipsed by simpler approaches such as hidden markov models so much so that MIT was even <a href=\"https://www.youtube.com/watch?v=h0e2HAPTGF4\">considering dropping them from the syllabus of their AI course as late as 2010</a>. But then breakthroughs in the engineering of multilayer neural nets (deep learning) were able to incorporate all the stochastical approaches and achieve a step change in the quality of the outputs and practical utility of machine learning.</p><p>Rule-based systems still play a role today but they only provide refinements on the edges, not the core power. Machine learning systems do not rely on rules nor do they produce rules.</p><p>This history is crucial to understand that there is a fundamental discontinuity between early AI research and machine learning. And an even bigger one between research into human speech and reason and machine learning. The best experts in machine learning are great applied mathematicians and computer engineers who only have a passing knowledge of the true complexity of human communication. This is not a bad thing for the task at hand. We’re running current models on the very edge of computational feasibility. But we need to be careful when relying on the judgements of AI experts as to how generalizable these advances are and how much we should believe that they reveal anything about how the mind and language actually work.</p><h3>Acknowledgments in lieu of references</h3><p>The above is based on my reading on the subject of metaphor, machine learning and organizations over many years. I did not take the time to insert references in the text but they would create a rich tapestry of interconnections.</p><p>The key readings on metaphor I had in mind were Donald A Schon’s work on generative metaphors and Gareth Morgan’s Images of the Organization. Dedre Gentner’s work on analogies is also very relevant. But my general thinking on metaphor has been shaped most closely by George Lakoff’s work on frames and idealised cognitive models.</p><p>My thinking on formal models and machine learning is based on accessing some machine learning courses and reading more informal work by people like <a href=\"https://rodneybrooks.com/forai-machine-learning-explained\">Rodney Brooks</a> and Andrew Gelman. I wrote my first paper on unification categorial grammars and have kept an eye on developments in NLP over the last 20 years even if it has never been the main area of my study or research.</p><p>My knowledge of organizational learning theory is the thinnest of them all. All of my reading here has been incidental. I draw more on my thinking about personal experiences and general reading about organizations influenced by ethnography and formal modelling in equal measure.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f1f9e6c1d9f3\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/learning-vs-training-in-machines-and-organizations-production-of-knowledge-vs-production-of-f1f9e6c1d9f3\">Learning vs training in machines and organizations: Production of knowledge vs production of…</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        }
      ],
      "summary": "<h3>Learning vs training in machines and organizations: Production of knowledge vs production of capability — Exploration of metaphors</h3><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*EAv0CPYrJ99vgSIkp-jnJg.jpeg\" /></figure><h3>Machine learning is to organizational learning as…</h3><p>A <a href=\"https://www.sbs.ox.ac.uk/school/events-0/david-upton-memorial-lecture\">recent lecture by Gary Pisano</a> made the parallel between machine learning and organizational learning in the sense that they are both processes that produce knowledge. My immediate reaction was that they are not alike because machine learning is really a misnomer. Or rather machine learning has some aspects of learning but is dissimilar from prototypical learning in enough ways to make the analogy dangerous. But my second thought was that actually organizational learning and prototypical learning are dissimilar in the same ways that machine learning and prototypical learning are. We could then learn more about both if we look at the various mappings and mismappings (a sort of metaphor hack).</p><p>What follows is a sketch of some of the ways we could think this through.</p><h3>Prototype knowledge frame</h3><p>I would propose that the prototype (frame) of learning is built around the acquisition of communicable and demonstrable knowledge. We learn history, physics, maths, medicine and the way we show we’ve learned these subjects is by demonstrating knowledge of them. This, of course, is not all there is to the frame of learning. We also learn to swim, learn to speak French, learn to solve problems. These are examples of the acquisition of what we could call capabilities. This is well reflected in educational theory and practice such as when dividing learning objectives into knowledge, skills and attitudes (KSA). But if we look at language and behavior, our default framing tends to learning = knowledge. A quick survey of most curricular objectives will be very heavily focused on on knowledge. Equally, most objections about poor education are illustrated by lack of knowledge of things.</p><h3>Defining knowledge and capability</h3><p>For the purposes of this exercise, I would define knowledge as something that can be communicated in a significantly shorter time than it can be acquired. It may take me days to memorise all the state capitals but I can recite them in minutes. Or it can take me years to research a topic for a book, months to write it, and days for someone to read it. Capability then could be defined as something that requires constant time to acquire given the same context and the same method of acquisition. And we have a word for the aquisition of capability, namely training.</p><p>Of course, both knowledge and capability can be defined in many other ways and a similar distinction could be made using different terms. Some terms that come to mind as alternatives to capability are phronesis, practical knowledge, or tacit knowledge, skill, etc. They are not really the same beast but they graze in the same semantic fields.</p><p>A great way to illustrate this further is the distinction used in linguistics between knowledge about and knowledge of language (this could be enriched by the famous competence/performance distinction but I’ll set that aside for now by focusing only on competence). I know quite a lot about the grammar of Swahili, yet, I do not ‘know’ Swahili or I don’t have any capability to speak it. However, I ‘know’ Czech and (having written a grammmar of Czech), I know a lot about Czech. I can communicate my knowledge about Czech to someone in an amount of time it takes to speak or write. However, my knowledge of Czech cannot be communicated. I have to take someone on a journey similar to mine (in time and effort) — or, in other words, train them.</p><h3>Machine learning is machine training</h3><p>Now, machine learning is really machine training. Current algorithms are trained on data sets with certain parameters and will assign certain probabilities (weights) to nodes on a decision tree (this may be direct with real decision trees or indirect with neural networks where the decision trees are multidimensional matrices). But no knowledge that can be communicated is produced. Unlike with humans, I can take the weights produced by the learning algorithm and transplant them to another computer but I cannot communicate what those weights mean. And I cannot easily add to the knowledge in the way I can add more facts to my knowledge about something. I can add more training data to improve the weights for an intended purpose or I can use the algorithm to train on a new data set and produce entirely different weights. But in both cases, I am not producing or communicating knowledge. I am replicating a training method. That’s why machine learning plays a relatively small role in something like climate modelling. You can train it on patterns and produce predictions in fractions of the time it takes to run a complex climate simulation (which could be literally months of computing). But this learning is a complete black box. You cannot extract communicable knowledge from a neural network. And because the feedback loop of testing on climate processes is measured in decades, it is impossible to rely on models produced solely by machine learning (I’m simplifying, of course).</p><h3>Knowledge in organizational learning</h3><p>How about organizational learning? Well, it’s possible to produce a lot of knowledge about organizations. Make lists, taxonomies, charts and graphs. And parts of this knowledge can be and routinely are communicated. When I join a new organization, I am told about all (or many) of the structures and rules. But as the anthropologist James C Scott points out, these rules are parasitic on the actual complexity of actions and relationships that are created in any society or organization to make it work. That’s why following the rules exactly is tantamount to <a href=\"https://en.wikipedia.org/wiki/Work-to-rule\">a strike</a>. The rules and structures are idealizations that express a certain vision (or model) of what is happening but I cannot be successful with just knowing about them. I have to spend time in the organization to learn them in the same way I would have to spend some time in Kenya or Tanzania to really learn Swahili.</p><p>It is well known that organizational learning cannot be simply communicated across organizations. That’s because it does not produce knowledge. It produces capabilities. That’s why it’s not easy to just ‘bring back computer manufacturing to the US’. Manufacturing a chip is a known process. With enough money, anyone can buy a fab. But even with the levels of automation in the chip manufacturing process, the levels of organizational learning (which includes the learning of all the relationships in the society of organizations and the workforce available in a region) simply cannot be created out of whole cloth in the same way that a manufacturing line can be. You need to train the organizations to acquire the same capabilities.</p><p>Now, that does not mean that there is no communicable knowledge being produced as part of organizational learning. But it is produced by people in the organization not the organization itself. And it is the knowledge of more efficient structures and processes to make sure organizations can achieve the kinds of capabilities that will lead them to success. In the same way, we can get better and better training methodologies for languages, musical instruments, or sports (described in great detail in Ericsson’s <em>Peak</em>). They can reduce the amount of time or effort to achieve certain capabilities. But in competitive environments like sports, the total amount of effort and time is constant because everyone is using the same methods, so the average performance improves but the effort is constant (ie. it takes much less time and effort today to become a top athlete or virtuoso pianist of 50 years ago but as much time to reach the peak performance of today.) And with some more complex and harder to measure capabilities like social skills, speaking a language, or becoming a good manager, the time and effort reduction is minimal even with better training methods.</p><h3>Additive and non-additive learning</h3><p>The same applies to organizational and machine learning. It is not that lessons from organizational learning cannot be learned and transported. But each organization has to go through the process to train itself to acquire the capabilities. And there’s a bottom limit on how quickly that can happen. It will be months to years for single organizations and years to decades for interconnected regional and supraregional industries. That is why huge mergers hoping to capitalize on synergies so often fail. The merger drivers assume that the organizational learnings are additive but they are mostly not. If you get two people who speak a year’s worth of French, they can certainly build on each other’s competence (assuming it is complementary) but they will not be as proficient as somebody who speaks two years worth of French. They might even be less proficient together than they would be individually because of the overhead it takes them to negotiate who knows what and who should speak when.</p><p>This is also the case for machine learning algorithms (or neural networks). You cannot add together their joint abilities in the same way you could add up two parts of a knowledge repository (like an encyclopedia). Two speech recognition systems that can each recognize speech with 50% accuracy cannot be added together to get 100% accuracy even assuming their coverage was complementary. You would still need another expert system to decide which was the accurate transcription in every case — and if you had that, you might as well use that to do the transcription. But such systems do not exist as we know from P=NP for problems where correctness is easier to check than it is to arrive at an answer.</p><p>However, that does not mean that additive complementarity is impossible. On the contrary, where an easy decision algorithn is available, complementarity is incredibly powerful. Hypothetically, if we had one speech recognition system for male and one for female voices, it would be much easier to come up with a third system for distinguishing which system to use in every particular case. The same could apply to the two limited French speakers. If their domains of competence were clearly delineated, their skills could be additive. Say if one could order in a restaurant and the other ask for directions to the restaurant.</p><p>The easiest and most straightforward decision algorithm is sequentiality. If I know two stages of a process come after one another, I can train a system on them separately and then add them together. This is what much of deep learning is about. The same goes for organizations. One path to success (suggested by research) for a secodary school is to aqcuire a primary school. That way it can improve its results simply by improving inputs. Apple has done the same with chip design. (Although there is a limit to how much this can be done — for instance in manufacturing, given that the quality of a supply chain often arises from its diversity, a car manufacturer acquiring manufacturers of all parts may make temporary gains in quality at the expense of long-term improvement.)</p><p>But this is a different kind of complementarity than that of adding together two encyclopedias or dictionaries with complementary entries. We don’t get one improved trained unit but two separate complementary units. In fact, in this sense, we could say that machine learning only works because of non-intersecting additive complementarity with human learning. Humans have to decide where and when machine learning algorithms can be deployed. They provide intentionality and complex embodied judgement. They also provide something machines do not have, which is metacognition.</p><p>Giving a person a calculator will magnify their abilities, giving a chess master a chess computer will also give them a definite edge. But we cannot upload the processing capabilities of the calculator into a person’s brain and make them into one system. All tool-using is a case of additive complementarity but only on the surface. An organization may use another to complement its capabilities but a merger of the two often does not result in the same outcome. The complexities of merging the myriad of synapse-like connections and system activations proves an impossible task even for the most intelligent of designers. The two merged organizations simply have to learn how to be one.</p><h3>Can machine learning simulate organizational learning?</h3><p>Which brings us to the question also raised by <a href=\"https://www.sbs.ox.ac.uk/school/events-0/david-upton-memorial-lecture\">Gary Pisano</a> which is the ability of machine learning to simulate organizational learning. This is easily imagined in the case of highly mechanized environments where machine learning is necessary to process the amounts of data generated. Or in highly complex environments where humans have a well-known cognitive limitation — for instance, organizing the best rotas and paths for workers in a warehouse. But even here we run up against the limits of complexity. NP-hard problems are NP-hard problems even for learning machines.</p><p>It is possible that some complex system problems are simply not machine learnable but rather the systems themselves compute the solution which emerges from their interaction. Here we may find the Hayekian problem of knowledge in society relevant. He was, of course, arguing with people who believed it was possible to centrally allocate resources within the economy with sophisticated algorithms using new data collection methods (back then computed by human ‘computers’). His point was that the market using the price signal can achieve better efficiency through self-organization. Efficient central planning, Hayek argued, was impossible in principle because we cannot collect enough data and even if we could, we could not compute them. It seems that Hayek have been correct in the sense of the computational tractability of perfect market efficiency, but is it possible that a sufficiently powerful machine learning system could outperform the computational efficiency of the market itself?</p><p>Or could this be at least achieved at the scale of very large organizations? Is it even needed? Massive conglomerates have been successfully managed through more direct methods for decades with little recourse to neural-net-based-AI which have only produced anything resembling useful results in the last 10 years.</p><p>It is certainly not possible at the current state of machine learning and with the available data. The advances in the field of learning algorithms necessary to make that leap illustrate an important difference between current machine learning and organizational learning. Organizations can organize and process their own inputs — whereas machine learning requires significant human effort to create the kinds of data structures and interpretations on which the machine learning algorithms can be applied. Organizations are analog computers operating directly on the data whereas machine learning always needs to convert the input into discrete units that can be expressed as vectors.</p><h3>Types of machine learning as metaphor</h3><p>There are three broad types of machine learning: 1. supervised, 2. unsupervised and 3. reinforcement. (These are often combined in actual deployed systems.)</p><p>1) Supervised learning simply means that the system (such as neural net) is trained on a highly processed, extremely large set of labeled data. For instance, pictures with labels of what’s in them or pairs of sentences and their translations. The network can then essentially compute the likelihoods of new data matching one of the labels. Or rather these likelihoods arise out of the complex matrix of activations across millions of parameters. Something that is completely computationally infeasible otherwise. What is important to know, that there is no list of ‘if-then’ statements anywhere within the system of the type ‘if object is round, increase likely hood object is a ball by x%’. (This is what the first wave of AI systems used and it seems to me to be what most people still imagine that machine learning produces.) We get extremely great aggregate results but no way of accounting for errors such as mistaking a toothbrush for a baseball bat. That is why these systems are susceptible to unpredictable adversarial attacks — for instance, understanding a seemingly random noise as a spoken command. Nevertheless, supervised learning has produced almost all of the useful and most directly visible outcomes of recent machine learning advances such as image recognition, machine translation or speech transcription.</p><p>2) Unsupervised learning is simply identifying regularities and irregulaties (patterns) in an unlabeled set of data. For instance, asking such an algorithm to classify words in a corpus of 10s of millions of English words, would give us a classification similar (but not identical) to what we know as nouns and verbs. A useful application of this is in network security where a machine learning system can learn typical patterns and then identify irregularities. This could also be used to identify potential credit card fraud.</p><p>3) The final type is reinforcement machine learning. This was used to teach AlphaGo to defeat the world champion. The system played many millions of games against human opponents (through past records — not in real life) and then itself. It used its failures and successes to adapt the weightings of moves and counter moves that lead to success. It is also great for training robots to navigate complex and changing environments.</p><p>A special case of reinforcement learning is evolutionary computation (although it could also be seen as separate). It can for instance combine multiple reinforcement learning agents working in parallel and testing out different solutions with assigning different fitness weights to them. This can be enromously powerful when A/B testing web design.</p><h4>Aside: What’s missing in recent advances in machine learning?</h4><p>Before we proceed to think about the analogies between machine learning and organizational learning, we should be very careful about how this machine learning revolution came about. It has three sources: 1) new computational methods to employ applied mathematics (such as differential calculus) to the design of networks of weighed nodes — in other words advances in software engineering and applied mathematics; 2) advances in computational power (such as graphics cards) and memory capacity that make these new methods feasible (such as increasing the number of layers of neural nets from 1 to 3 or even more); 3) availability of large amounts of structured or semi-structured machine readable data.</p><p>But nowhere on this list of accomplishments are advances in our understanding of how the mind, language or culture work. Quite the opposite. Fred Jelinek (a speech recognition pioneer) supposedly once quipped that the more linguists he fired from his lab, the better outputs he got. I’m not surprised. All of our understanding of how the mind or language work is either computationally infeasible (in the sense of incomputable collections of if-then statements across seemingly infinitely many dimensions) or too unstructured to be amenable to machine learning approaches (for instance, traditional semantics or semiotics or anthropology — even if we could structure this data — like in FrameNet or even WordNet, we could still not collect enough of it).</p><h4>Caveat: The seductiveness of successful metaphor</h4><p>It is very seductive to conclude that just because the new machine learning (neural networks known as deep learning) is essentially inspired by how we think the brain is architected, it is more likely to produce human-like intelligence (AGI). But that is just as wrong as saying that just because airplanes have been initially inspired by how birds fly, we will soon have the capacity to fly like the birds. Airplane flight looks like bird flight (wings) and relies on the same fundamental physics. But it is achieved completely differently from bird flight and it does not only not make us able to fly like birds. It also contains no pathway to that ability. I suspect that neural networks are the same.</p><h3>What kind of machine learning is organizational learning most like?</h3><p>Given all that, it is not difficult to see that organizational learning is not similar to all three types of machine learning. It is mostly like reinforement learning (although business schools teach it in a supervised learning mode). And we can also envision that reinforcement learning algorithms with the assistance of unsupervised learning pattern recognition would be where the most fruitful approaches to achieve AI-managed organizations. Evolutionary algorithms could even simulate the market and achieve efficiencies without the inconvenience of actual market failure. However, it is equally possible that we will soon run against the limits of the sort of data we can provide these algorithms as inputs. It is far too easy to underestimate the importance of information architecture and data analysis.</p><p>So can we think about an organization learning as a type of reinforcement learning? We certainly can but with important caveats. The useful parallel is the complexity of the learning that arises out of massive amounts of connections with constantly changing weights. Which means that the system has learned to respond in useful ways to variations in inputs and produce usable outputs. But we should remember that the learning is a bit of a black box and we cannot simply extract the knowledge as a series of if-then rules which business case studies so often try to do.</p><p>However, the actual topology of the network is infinitely more complex with too many nodes and connections to identify. But even more importantly, the nodes in an organizational networks are very complex agents rather than just passive probability repositories. This means that organizations are much more able to adapt to change. The famous AlphaGo system would have been stumped if it was asked to play go on a board of 20x20 instead of 19x19. A human player would adapt easily.</p><h4>Aside: Learning from single examples, the human way</h4><p>The difference between human individual and a machine learning algorithm is instructive. Machine learning needs inhuman amounts of data. Humans can learn from single examples by techniques such as analogy or metacognition. For instance, it may take me hundreds of tries to learn that a particular animal is a dog but only one time of seeing a platypus. I can also build on previous experience in creative ways. It took me months to learn to play anything resembling a tune on the guitar and years to play it well. Yet, it took me less than an hour to play a song on a Ukulele and a few months to play it well. I also have all sorts of knowledge about my knowledge that helps. So I know that if I want to know what letter comes after e, I just have to say a, b c, d, e and ‘f’ will pop into my head in a way it does not when I just say ‘e’. I also know a lot of things about the world in a completely non-propositional way that helps me immediately recognize that a baby holding a baseball bat is more unusual than a baby holding a toothbrush. I did not have to learn this as a fact or even as one of the many configurations of the world. I just know it. And without all of the above human learning and intelligence are inconceivable. And none of it is in any way present in machine learning.</p><p>But how about organizations? Are they more like individual humans or more like machine learning systems? They are more like machine learning systems in that they need to learn as a whole to adapt to new conditions. When a new regulation or manufacturing method is introduced, it takes more time for an organization to adapt to it than it might take any individual human within that organization. Which is why it takes time when a new manufacturing process is introduced before new levels of productivity can be reached.</p><p>However, because an organization does not consist of dumb neurons that can only adapt their individual weights but humans who are capable of complex adjustments, there are many types of changes that it can adapt to easily. For instance, I may send out an email to everyone working in an office that the back door can no longer be used. This may completely throw a machine learning system trained on a situation where the backdoor could be used. However, the individual humans will make the adjustments very easily and the whole organization will reroute its activity around that without having to relearn everything about how it works.</p><h3>Negotiating the metaphor</h3><p>So the trick for a planner, leader or organizational designer is to figure which learning metaphor is more apt for any given example of organizational change. Is this a kind of change an organization will have to learn in the same way a machine learning system is trained on new data or is it more like the kind of change a human can adapt to almost without breaking a stride? Or is it the kind kind of change that even an individual human will find difficult? Such as learning to play the trumpet having mastered the ukulele.</p><h3>Oversimplifications as generative metaphors: A how to guide</h3><p>Having spent so much time showing the different ways in which machine learning, organizational and human learning are not alike, it might seem logical to conclude that we should ditch all such comparisons. But that would be extremely unwise. Generative metaphors work best when there is more dissimilarity between two domains of comparison than similarity. But as long as we know enough to avoid the seduction by analogical success and the isomorphism fallacy, we should feel free to explore. One way to do this is to see what comes out if we look at one domain in terms of another.</p><p>So it may be actually extermely useful to recast organizations as machine learning systems based around reinforcement. One of the problems our models suffer from is what Bayesian statisticians call overfitting. This happens in extremely complex environments when we try to add additional variables to a model to make it reflect reality more. But because the reality is very complex and noisy, we can never get all the parameters. So we are just as likely to pick ones that will fit the model to random noise as ones that will make it more useful. Therefore, if we simplify the organization as just a network of interconnected nodes with changing weights (neurons and synapses) we may be able to think about organizational learning in ways that will make it more tractable. The more we try to complicate the model by giving the nodes structure, the less tractable it will be and the more noise it will generate.</p><p>So what can this generative simplification get us? Well, it may help us think about things like iterations, input data and data structures. This way we can design better environments in which organizational learning succeeds or identify key elements in designs that already proved successful.</p><p>But we may also be minded to take into account that organizational learning is a black box that is conditioned by the training data. That may make us skeptical about rich narratives of change based around the behavior of certain agents. They may be significant but the model may be better off without them.</p><p>Then we must be mindful not to forget that those thick descriptions capture realities from the perspectives of those involved and that we cannot describe the behavior of the individual in terms of the system in the same way we cannot describe the behaviour of nodes in a machine learning network as learning. Thus statements such as ‘the organization learned because its members learned’ may become slightly suspect within this model. But again this also does not mean that the individual’s learning is irrelevant but just maybe that it was not part of the organization itself learning as a black box machine learning system. So we may start thinking about what role does an individual’s knowledge play in the organization? What does it mean when an organization ‘learns’ for its mistakes, etc.</p><p>The key thing is to remember that at every analogical step we should reaffirm how closely tied our statements are to the parameters of our model and that the partial success of the model does not make it more real than richer models that may not be quite as useful due to their computational intractability.</p><h3>Conclusion</h3><p>So what is the purpose of all this? The various dimensions of machine and organizational learning are already well known. But perhaps contrasting the various models of learning can provide useful models for thinking about any one of them. As in any analogical chain, it is as important to identify the discontinuities as it is to identify continuities. Otherwise, we are in the danger of overfitting — or the isomorphism fallacy. Machine, organizational and human learning have many parallels but they are not the same. They have many superficial similarities and deep dissimilarities.</p><p>Perhaps the key lesson is the distinction between knowledge and capability. Organizations and machines do not produce anything resembling knowledge. And perhaps acknowledging that can be useful. But organizations are different from trained neural networks because their components contain knowledge which can be harnessed for training them on new circumstances. All of that could be done without the machine learning analogy but this was my journey. Perhaps it can be of some use.</p><h3>Final Caveat: Prehistory of machine learning: Teaching computers rules and words then and now</h3><p>One of Stephen Pinker’s most powerful and most deeply misleading book titles is ‘Rules and words’. It is a book showing the power of the computational model of language that can take 2 sets of finite inputs (rules and words) and generate an infinite set of patterns (sentences of a language). This could be seen as the last gasp of the old way. When I started studying linguistics exactly 10 years before the book was published, it was the mainstream that still gleamed with the shine of the new. But when the book came out, nobody was doing anything interesting with the rules and words model. The computational model of the mind and language took classical structured descriptions of how we speak and think and converted them into computationally tractable structures (logical trees — not decision trees — and rewriting rules operating on them). This was the system underlying all expert systems developed in the 70s, 80s and early 90s and it led to AI being seen as a failed paradigm. That’s not to say that it was useless, it produced much of the data infrastructure that was then later used to train machine learning algorithms. And it also produced some basic working examples of machine translation and speech recognition. But it never produced anything even remotely resembling intelligence. It struggled to reliably translate even something so seemingly simple and routine as weather forecasts between 2 related languages.</p><p>There is an interesting paradox when it comes to computational power. Rule-based machine expert systems are much more computationally tractable when dealing with small sets of structured data. You can even do it on a piece of paper. However, they do not scale almost at all when confronted with the combinatorial complexity of the real world. The number of rules required to handle even the simplest words (such as ‘dog’) in all possible contexts is too monumentally staggering that even were the Moore’s law continue, it would still likely not catch up with the needs.</p><p>Stochastic approaches and neural nets on the other hand, can produce nothing of any real use whatsoever with little data or little processing power. But they scale wonderfully. Once you can run millions of iterations on hundreds of millions of tokens in training sets, they only get more useful. They can only learn regularities that exist in data that can be presented to them (so there is a limit to how far they can go) but if we tried to express those same regularities through rules, we would never get anywhere. We couldn’t come up with that many rules and if we did, we could not compute the outcomes.</p><p>But the rule-based systems are more consononant with our intuitions about how the mind works. Many of their foundations go back to the first philosophers be they in Greece, India or China. That’s what made the early AI systems like <a href=\"https://medium.com/feed/&lt;span class=&quot;inline-comment&quot;&gt;en.wikipedia.org/wiki/ELIZA\">ELIZA</a> so seductive. The rule-based AIs were obviously like us, we just needed to spend more time adding rules, and we’d be there in no time. But this was wrong. Because when we write down these rules we are cheating. The amount of rich intricate knowledge required to make sense even of trivial sentences like ‘The cat is on the mat’ is mind boggling. What are the rules for choosing ‘the’ instead of ‘a’ in one or the other word? Why do we say a bird is ‘in’ the tree but ‘on’ a perch? In Czech we say ‘on’ the tree, so why did English pick this? And how do we encode distinctions such as ‘in’ for tight and loose fit in some languages? How about local knowledge such as knowing that the mat is valuable so the cat being on is a bad thing or that that the cat could also be in the larder so its being on the mat is a good thing. Sentences in language analysis and logical propositions are not idealised examples of language or reason. They are snapshots of <a href=\"http:&lt;/span&gt;metaphorhacker.net/2014/11/what-language-looks-like-dictionary-and-grammar-are-to-language-what-standing-on-one-foot-is-to-running/\">unnatural poses we manipulate them into</a> when trying to take a look at them through themselves.</p><p>When I was first studying these approaches, stochastic processing was the timid new kid on the block but 10 years later, it was the source of all major breakthroughs in expert systems developments. But they hit a wall in the mid 00s, and new improvements were slow in coming. In the 1990s, connectionism and neural nets which had been the big hope for the future since the 80s were eclipsed by simpler approaches such as hidden markov models so much so that MIT was even <a href=\"https://www.youtube.com/watch?v=h0e2HAPTGF4\">considering dropping them from the syllabus of their AI course as late as 2010</a>. But then breakthroughs in the engineering of multilayer neural nets (deep learning) were able to incorporate all the stochastical approaches and achieve a step change in the quality of the outputs and practical utility of machine learning.</p><p>Rule-based systems still play a role today but they only provide refinements on the edges, not the core power. Machine learning systems do not rely on rules nor do they produce rules.</p><p>This history is crucial to understand that there is a fundamental discontinuity between early AI research and machine learning. And an even bigger one between research into human speech and reason and machine learning. The best experts in machine learning are great applied mathematicians and computer engineers who only have a passing knowledge of the true complexity of human communication. This is not a bad thing for the task at hand. We’re running current models on the very edge of computational feasibility. But we need to be careful when relying on the judgements of AI experts as to how generalizable these advances are and how much we should believe that they reveal anything about how the mind and language actually work.</p><h3>Acknowledgments in lieu of references</h3><p>The above is based on my reading on the subject of metaphor, machine learning and organizations over many years. I did not take the time to insert references in the text but they would create a rich tapestry of interconnections.</p><p>The key readings on metaphor I had in mind were Donald A Schon’s work on generative metaphors and Gareth Morgan’s Images of the Organization. Dedre Gentner’s work on analogies is also very relevant. But my general thinking on metaphor has been shaped most closely by George Lakoff’s work on frames and idealised cognitive models.</p><p>My thinking on formal models and machine learning is based on accessing some machine learning courses and reading more informal work by people like <a href=\"https://rodneybrooks.com/forai-machine-learning-explained\">Rodney Brooks</a> and Andrew Gelman. I wrote my first paper on unification categorial grammars and have kept an eye on developments in NLP over the last 20 years even if it has never been the main area of my study or research.</p><p>My knowledge of organizational learning theory is the thinnest of them all. All of my reading here has been incidental. I draw more on my thinking about personal experiences and general reading about organizations influenced by ethnography and formal modelling in equal measure.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f1f9e6c1d9f3\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/learning-vs-training-in-machines-and-organizations-production-of-knowledge-vs-production-of-f1f9e6c1d9f3\">Learning vs training in machines and organizations: Production of knowledge vs production of…</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Quick notes on measurement: Measuring anything and nothing",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://medium.com/feed/metaphor-hacker",
        "value": "Quick notes on measurement: Measuring anything and nothing"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://medium.com/metaphor-hacker/quick-notes-on-measurement-measuring-anything-and-nothing-29bd0d1a2e9?source=rss----67ad910371b8---4"
        },
        {
          "rel": "license",
          "href": "http://creativecommons.org/licenses/by/4.0/"
        }
      ],
      "link": "https://medium.com/metaphor-hacker/quick-notes-on-measurement-measuring-anything-and-nothing-29bd0d1a2e9?source=rss----67ad910371b8---4",
      "id": "https://medium.com/p/29bd0d1a2e9",
      "guidislink": false,
      "tags": [
        {
          "term": "measurement",
          "scheme": null,
          "label": null
        },
        {
          "term": "science",
          "scheme": null,
          "label": null
        },
        {
          "term": "metaphor",
          "scheme": null,
          "label": null
        },
        {
          "term": "philosophy",
          "scheme": null,
          "label": null
        },
        {
          "term": "rationality",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Dominik Lukes"
        }
      ],
      "author": "Dominik Lukes",
      "author_detail": {
        "name": "Dominik Lukes"
      },
      "published": "Sat, 18 Nov 2017 21:16:33 GMT",
      "published_parsed": [
        2017,
        11,
        18,
        21,
        16,
        33,
        5,
        322,
        0
      ],
      "updated": "2018-03-19T23:11:21.287Z",
      "updated_parsed": [
        2018,
        3,
        19,
        23,
        11,
        21,
        0,
        78,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://medium.com/feed/metaphor-hacker",
          "value": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*7DXn-l5lH1F1zmxAUsF-Jg.jpeg\" /></figure><h3>Note:</h3><p>These were supposed to be quick notes. But they ain’t. Be warned.</p><h3>Background</h3><p>In a recent <a href=\"http://rationallyspeakingpodcast.org/show/rs-197-doug-hubbard-on-why-people-think-some-things-cant-be.html\">Rationally Speaking</a> podcast Julia Galef and Doug Hubbard discussed the notion of ‘How to Measure Anything’. This is not a screed against daring to assign numerical values. I think we need to start from the premise that everything that Juia and Doug said is correct. However, I want to show that even with that premise, the intuitive objections to assigning numerical values to human values are still necessary. And in fact, if we object on these grounds, we can see that in fact we can measure nothing while we can still benefit from using tools of measurement to think about complex problems. But these tools are metaphors about those problems — they are not describing some hidden true essence — they are giving us a way of thinking about them. And as with any metaphors, we need to reject them as often as take them for our own.</p><h3>Measurement tropes cooled down</h3><p>So this is how the story goes.</p><p>Doug Hubbard is responding to a common trope in the form of ‘you can’t put a value on X’ where X E {life, happiness, etc.} Because these things have intrinsic value, they are too important, and on and on and on.</p><p>But in fact, we do put value on all of these things through our actions argue people like Doug Hubbard and Julia Galef. And we can calculate roughly what that value is when we look at your actions.</p><p>And we can do similar magic with things that you think are OK to measure but cannot be measured. Things like the likelihood that a factory will grind to a halt or the impact of education on future earnings. And so on and on and on.</p><p>And with a little change in perspective, we can convert this general value into a numerical value. Thus we can calculate even something as seemingly incalculable as the value of a person’s life. Simply by measuring various proxies that can have numerical values assigned to them. E.g. how much money will people pay to get a safer car, etc.</p><p>But things are not as straightforward, we all know that no matter what fancy measurement we do of complex issues, we get things wrong all the time. ALL THE TIME!</p><h3>Reframing and rereframing measurement</h3><p>Here is where Doug Hubbard pulls off a neat conceptual trick. He says: “because measurement is a way of reducing uncertainty, that means that we can truly measure anything”. And that lights up a bulb in our heads and we say, of course, we can do that. We cannot ever produce certainty, but we can always reduce uncertainty. I’m sounding a little hyperbolic and feeling very ironic, but it is atually legitimately useful perspective shift. Where once we dispaired that certainty is forever out of our reach, we now see a glimmer of hope in that we can at least reduce it somewhat. And once we have that hope, we can start reducing our uncertainty by measuring everything in sight. So we’re a little less uncertain about whether it will rain or not tomorrow, whether the stockmarket will crash, and so on and so forth.</p><p>And people who take this philosophy to heart and adopt the related tools will benefit. Not a little. A lot.</p><p>So that is all good. And I agree that more people should take this approach to more things. And that the detractors objecting to this sort of approach to measurement on principle are wrong.</p><h3>Two massive problems</h3><p>But, there are two massive problems with this view.</p><p>First, in as much as words ever mean anything, the word measurement does NOT mean ‘reducing uncertainty’. It means almost the opposite, namely, ‘producing certainty’. That’s what it means. I see a basket of apples and I think there may be 15 apples in there. But I want to be certain before I make plans for my party. So I count them. And now I am certain that each of the guests at my apple eating party will have an apple. I think my window is 90 centimeters across, but I want to be certain before I click buy on the roller blind. So I measure the window, and now I am certain.</p><p>That’s what measurement means in daily life. And that’s what people intuitively know we cannot do with really complicated or really abstract things.</p><p>But things are even more complicated. Even these simple certainty-producing measurements are relative to the need I’m measurig them for. What if one of the apples is much smaller, or pockmarked. I still have 15 ‘apples’ but not enough for 15 guests. Or maybe one of them is a strange hybrid cross with a pear so I’m not even sure if it ‘counts’ as an apple.</p><p>The same with meauring my window. What if some manufacturer’s roller blinds are 0.00001 of a centimeter too long. And the bracket on my window is 0.00001 of a centimeter too short. Well, with roller blinds, I’m sure that does not matter. But what if I’m building an atomic clock? How precise do I need to be? There is always some usecase (perspective) where more precision is required than I am able to produce, all the way to blessed infinity.</p><p>But thankfully, we have redefined measurement as a reduction of uncertainty. So I’m better off having counted the apples and measured my window to the nearest milimiter because at least I’ve increased my chances of having a fun apple party and buying the right curtains. And 666 times out of 669, it works. Certainly more times than if I didn’t measure at all. But the devil is in the detail. Or rather the perspective.</p><p>If you define measurement as producing certainty (which is the intuitive definition of most people) and take it to its conclusion. You will get the unfortunate result, that you can actually measure nothing. Because at the very best, we don’t have the tools to measure precisely enough for producing certainty and at worst, we actually can’t because of those pesky infinities.</p><p>So this is the perspective of the people who criticise measurement. And they’re not wrong. They’re just happen to be using the common sense definition of ‘measurement’.</p><p>And that’s why redefining it can be so powerful. We can not only feel better about all the measurements we are doing, we can also start measuring a lot more things. And I’m all for that.</p><p>BUT.</p><p>There is also the second massive problem. Sometimes reducing uncertainty is pretty much pointless or even actively harmful. You can sometimes reduce your uncertainty about large populations without having any sort of useful uncertainty reduction about the individuals. Let’s say there are 2 countries you’re planning to visit on your holiday, with with exactly 1 million regularly distributed inhabitants each. And we have an absolutely precise criminological survey and we know that country A has one axe murderer and country B has two. So you are twice as likely to be murdered by an axe in country B as you are in country A. Assuming that they are exactly identical in every way, it would make perfect sense to go to country A. But what if country B has beautiful beaches and gorgeous architecture and country A is just a spread of concrete low rises in the desert? Well, you say, sure, I’m still better off knowing. At least I can make an informed decision. But what if the cost of finding out this information was as expensive as your holiday? Or what if it took a whole year off your life to do the survey because you’re the only axe murder measurement expert? And worse still what if somebody in your country read your research and it triggered something evil in them and they now became an axe murderer? When is reducing uncertainty worth it?</p><p>But even that is not all. Let’s say you go to both country A and country B and know that there is 1 in a million chance that any one person you encounter will murder you with an axe in country A but in country B, the chance is 2 in a million? How different will the encounters be? How long before you start saying to your friends, don’t go to country B, I was there and everybody looked like an axe murderer. Or, you country A is so much safer than B, they have only half the axe murderers. That’s how stereotypes work. They reduce uncertainty but the reduction is usually not as useful and almost never outweighs the harm done to the thing you’re reducing your uncertainty about.</p><p>So even with measurement as an uncertainty reduction tools, you still need to be interrogating your quos for their quids. There is a continuum between uncertainty reduction amounting to common sense knowledge and representing slanderous prejudice.</p><p>For instance, a lot fewer than 100% of people in the United States speak English as their first language or even at all. Yet, I’m perfectly happy to say that people in the US speak English and study English if I want to visit. The fact that not every person I encounter while there will speak English is built into the framing of the statement.</p><p>But imagine I know that 20 out of 100 people who colour their hair blue are bad at math but 25 out of 100 people who colour their hair green are bad at math. It would seem to make sense to only hire people who dye their hear blue for trainee accountant jobs. But what am I missing out on but not getting some green haired people as well? And what harm am I doing to people with green hair? What information do I have about any of the applicants coming to my door simply based on the colour they chose to use to dye their hair. Almost none. If I’m hiring 100 trainee accountants how much better off am I if I hire all 100 blue or 50 percent of each? I get 20 bad accountants in the first case and 22–23 bad accountants in the second case. How trainee many accountants do I have to hire and what other harms do I have to offset? And what if I’m hiring only one trainee accountant. Should I reject anyone with green streaks in their manes?</p><p>Thus reducing one uncertainty introduced another one. And that’s even assuming my measurement is worth a hill of beans.</p><p>Let’s take the much agonized over international educational outcomes comparisons. They seem to give us a nice ranking of different countries. From best to worst. But (assuming we can trust the measurement all that well) what does a difference between the top country and the 20th country amount to? Probably nothing. Should a country even pay any attention to going up 5 spots or sliding 10? No, no, NO! All the effort that went into the measurement and all the consequences of the measurement are not worth it. Plus what is that we were uncertain about before? And what is it that we’re uncertain about know? Everyone would have been better of not knowing.</p><p>Actually, the various comparisons are fascinating and we coud learn a lot from them.</p><p>BUT</p><p>This brings us to a bonus third massive problem. You can’t really do very complicated measurement without numbers (or some sort of a numerical or geometric type of symbol). And numbers are magic. They are absolutely certain. So when a number is in sight everybody sits up and takes notice. 25 out of a 100 is more than 23 out of a hundred. That has to mean something! You see now we know. And there are cases when you can make or save enormous amounts of money by knowing that A gives 0.001% less return than B. If you invest 1,000,000,000 in B, you’ve made 1,000,000. But if you only invest 1,000, you’ve made 1. Probably less than the cost of the investment into the measurement itself.</p><p>But because we have so many uncertainty reductions available to us all the time, we spend enormous amounts of 1,000s to make make a few 1s.</p><p>All because numbers are magic. People (as a whole) have yet to demonstrate that they can behave sensibly in the presence of numbers. Even most mathematicians and scientists. Even me.</p><p>The problem is that numbers are precise but only until you start counting things with them. Any process of counting things requires a simplification of the world to make it countable. We are surrounded by examples of where this simplification allowed us feats previously unimagined. The digitisation of music is an example. It only takes bits of the sound wave. But it takes enough to make the reproduction of the sound much better than reproducing actual sound waves (and even if there is some vinyl quality worth keeping, the other benefits such as lossless copying more than make up for it). But there maybe a situation where that simplification is too much. Imagine we’re invaded by aliens who repelled by a particular frequency but the presence of even the most infinitesimally fine sampling in the reproduction of the soundwave negates the effect. Vinyl alien repellers would soon get plenty popular again.</p><p>So the sort of simplification done by numbers is extremely useful until it is not. And when we leave the realm of the physical or medical and go into the realm of the social, we can see the usefulness fading pretty fast or the cost of the usefulness become pretty steep.</p><p>This is a fairly important corrective to the crowd who say it’s always better to know. I’d say it’s always useful to have the information but not always better to know it.</p><p>So did I swing around to the position that we shouldn’t measure anything ever or at least not anything with a social dimension? No. I stand by my initial conviction that we should measure more things more often. But we shoud always question the utility of the uncertainty reduction the measurement provides and be very mindful of the uncertainty that remains — because no matter how many times we’ve measured, we should always feel a little worried when we cut!</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=29bd0d1a2e9\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/quick-notes-on-measurement-measuring-anything-and-nothing-29bd0d1a2e9\">Quick notes on measurement: Measuring anything and nothing</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        }
      ],
      "summary": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*7DXn-l5lH1F1zmxAUsF-Jg.jpeg\" /></figure><h3>Note:</h3><p>These were supposed to be quick notes. But they ain’t. Be warned.</p><h3>Background</h3><p>In a recent <a href=\"http://rationallyspeakingpodcast.org/show/rs-197-doug-hubbard-on-why-people-think-some-things-cant-be.html\">Rationally Speaking</a> podcast Julia Galef and Doug Hubbard discussed the notion of ‘How to Measure Anything’. This is not a screed against daring to assign numerical values. I think we need to start from the premise that everything that Juia and Doug said is correct. However, I want to show that even with that premise, the intuitive objections to assigning numerical values to human values are still necessary. And in fact, if we object on these grounds, we can see that in fact we can measure nothing while we can still benefit from using tools of measurement to think about complex problems. But these tools are metaphors about those problems — they are not describing some hidden true essence — they are giving us a way of thinking about them. And as with any metaphors, we need to reject them as often as take them for our own.</p><h3>Measurement tropes cooled down</h3><p>So this is how the story goes.</p><p>Doug Hubbard is responding to a common trope in the form of ‘you can’t put a value on X’ where X E {life, happiness, etc.} Because these things have intrinsic value, they are too important, and on and on and on.</p><p>But in fact, we do put value on all of these things through our actions argue people like Doug Hubbard and Julia Galef. And we can calculate roughly what that value is when we look at your actions.</p><p>And we can do similar magic with things that you think are OK to measure but cannot be measured. Things like the likelihood that a factory will grind to a halt or the impact of education on future earnings. And so on and on and on.</p><p>And with a little change in perspective, we can convert this general value into a numerical value. Thus we can calculate even something as seemingly incalculable as the value of a person’s life. Simply by measuring various proxies that can have numerical values assigned to them. E.g. how much money will people pay to get a safer car, etc.</p><p>But things are not as straightforward, we all know that no matter what fancy measurement we do of complex issues, we get things wrong all the time. ALL THE TIME!</p><h3>Reframing and rereframing measurement</h3><p>Here is where Doug Hubbard pulls off a neat conceptual trick. He says: “because measurement is a way of reducing uncertainty, that means that we can truly measure anything”. And that lights up a bulb in our heads and we say, of course, we can do that. We cannot ever produce certainty, but we can always reduce uncertainty. I’m sounding a little hyperbolic and feeling very ironic, but it is atually legitimately useful perspective shift. Where once we dispaired that certainty is forever out of our reach, we now see a glimmer of hope in that we can at least reduce it somewhat. And once we have that hope, we can start reducing our uncertainty by measuring everything in sight. So we’re a little less uncertain about whether it will rain or not tomorrow, whether the stockmarket will crash, and so on and so forth.</p><p>And people who take this philosophy to heart and adopt the related tools will benefit. Not a little. A lot.</p><p>So that is all good. And I agree that more people should take this approach to more things. And that the detractors objecting to this sort of approach to measurement on principle are wrong.</p><h3>Two massive problems</h3><p>But, there are two massive problems with this view.</p><p>First, in as much as words ever mean anything, the word measurement does NOT mean ‘reducing uncertainty’. It means almost the opposite, namely, ‘producing certainty’. That’s what it means. I see a basket of apples and I think there may be 15 apples in there. But I want to be certain before I make plans for my party. So I count them. And now I am certain that each of the guests at my apple eating party will have an apple. I think my window is 90 centimeters across, but I want to be certain before I click buy on the roller blind. So I measure the window, and now I am certain.</p><p>That’s what measurement means in daily life. And that’s what people intuitively know we cannot do with really complicated or really abstract things.</p><p>But things are even more complicated. Even these simple certainty-producing measurements are relative to the need I’m measurig them for. What if one of the apples is much smaller, or pockmarked. I still have 15 ‘apples’ but not enough for 15 guests. Or maybe one of them is a strange hybrid cross with a pear so I’m not even sure if it ‘counts’ as an apple.</p><p>The same with meauring my window. What if some manufacturer’s roller blinds are 0.00001 of a centimeter too long. And the bracket on my window is 0.00001 of a centimeter too short. Well, with roller blinds, I’m sure that does not matter. But what if I’m building an atomic clock? How precise do I need to be? There is always some usecase (perspective) where more precision is required than I am able to produce, all the way to blessed infinity.</p><p>But thankfully, we have redefined measurement as a reduction of uncertainty. So I’m better off having counted the apples and measured my window to the nearest milimiter because at least I’ve increased my chances of having a fun apple party and buying the right curtains. And 666 times out of 669, it works. Certainly more times than if I didn’t measure at all. But the devil is in the detail. Or rather the perspective.</p><p>If you define measurement as producing certainty (which is the intuitive definition of most people) and take it to its conclusion. You will get the unfortunate result, that you can actually measure nothing. Because at the very best, we don’t have the tools to measure precisely enough for producing certainty and at worst, we actually can’t because of those pesky infinities.</p><p>So this is the perspective of the people who criticise measurement. And they’re not wrong. They’re just happen to be using the common sense definition of ‘measurement’.</p><p>And that’s why redefining it can be so powerful. We can not only feel better about all the measurements we are doing, we can also start measuring a lot more things. And I’m all for that.</p><p>BUT.</p><p>There is also the second massive problem. Sometimes reducing uncertainty is pretty much pointless or even actively harmful. You can sometimes reduce your uncertainty about large populations without having any sort of useful uncertainty reduction about the individuals. Let’s say there are 2 countries you’re planning to visit on your holiday, with with exactly 1 million regularly distributed inhabitants each. And we have an absolutely precise criminological survey and we know that country A has one axe murderer and country B has two. So you are twice as likely to be murdered by an axe in country B as you are in country A. Assuming that they are exactly identical in every way, it would make perfect sense to go to country A. But what if country B has beautiful beaches and gorgeous architecture and country A is just a spread of concrete low rises in the desert? Well, you say, sure, I’m still better off knowing. At least I can make an informed decision. But what if the cost of finding out this information was as expensive as your holiday? Or what if it took a whole year off your life to do the survey because you’re the only axe murder measurement expert? And worse still what if somebody in your country read your research and it triggered something evil in them and they now became an axe murderer? When is reducing uncertainty worth it?</p><p>But even that is not all. Let’s say you go to both country A and country B and know that there is 1 in a million chance that any one person you encounter will murder you with an axe in country A but in country B, the chance is 2 in a million? How different will the encounters be? How long before you start saying to your friends, don’t go to country B, I was there and everybody looked like an axe murderer. Or, you country A is so much safer than B, they have only half the axe murderers. That’s how stereotypes work. They reduce uncertainty but the reduction is usually not as useful and almost never outweighs the harm done to the thing you’re reducing your uncertainty about.</p><p>So even with measurement as an uncertainty reduction tools, you still need to be interrogating your quos for their quids. There is a continuum between uncertainty reduction amounting to common sense knowledge and representing slanderous prejudice.</p><p>For instance, a lot fewer than 100% of people in the United States speak English as their first language or even at all. Yet, I’m perfectly happy to say that people in the US speak English and study English if I want to visit. The fact that not every person I encounter while there will speak English is built into the framing of the statement.</p><p>But imagine I know that 20 out of 100 people who colour their hair blue are bad at math but 25 out of 100 people who colour their hair green are bad at math. It would seem to make sense to only hire people who dye their hear blue for trainee accountant jobs. But what am I missing out on but not getting some green haired people as well? And what harm am I doing to people with green hair? What information do I have about any of the applicants coming to my door simply based on the colour they chose to use to dye their hair. Almost none. If I’m hiring 100 trainee accountants how much better off am I if I hire all 100 blue or 50 percent of each? I get 20 bad accountants in the first case and 22–23 bad accountants in the second case. How trainee many accountants do I have to hire and what other harms do I have to offset? And what if I’m hiring only one trainee accountant. Should I reject anyone with green streaks in their manes?</p><p>Thus reducing one uncertainty introduced another one. And that’s even assuming my measurement is worth a hill of beans.</p><p>Let’s take the much agonized over international educational outcomes comparisons. They seem to give us a nice ranking of different countries. From best to worst. But (assuming we can trust the measurement all that well) what does a difference between the top country and the 20th country amount to? Probably nothing. Should a country even pay any attention to going up 5 spots or sliding 10? No, no, NO! All the effort that went into the measurement and all the consequences of the measurement are not worth it. Plus what is that we were uncertain about before? And what is it that we’re uncertain about know? Everyone would have been better of not knowing.</p><p>Actually, the various comparisons are fascinating and we coud learn a lot from them.</p><p>BUT</p><p>This brings us to a bonus third massive problem. You can’t really do very complicated measurement without numbers (or some sort of a numerical or geometric type of symbol). And numbers are magic. They are absolutely certain. So when a number is in sight everybody sits up and takes notice. 25 out of a 100 is more than 23 out of a hundred. That has to mean something! You see now we know. And there are cases when you can make or save enormous amounts of money by knowing that A gives 0.001% less return than B. If you invest 1,000,000,000 in B, you’ve made 1,000,000. But if you only invest 1,000, you’ve made 1. Probably less than the cost of the investment into the measurement itself.</p><p>But because we have so many uncertainty reductions available to us all the time, we spend enormous amounts of 1,000s to make make a few 1s.</p><p>All because numbers are magic. People (as a whole) have yet to demonstrate that they can behave sensibly in the presence of numbers. Even most mathematicians and scientists. Even me.</p><p>The problem is that numbers are precise but only until you start counting things with them. Any process of counting things requires a simplification of the world to make it countable. We are surrounded by examples of where this simplification allowed us feats previously unimagined. The digitisation of music is an example. It only takes bits of the sound wave. But it takes enough to make the reproduction of the sound much better than reproducing actual sound waves (and even if there is some vinyl quality worth keeping, the other benefits such as lossless copying more than make up for it). But there maybe a situation where that simplification is too much. Imagine we’re invaded by aliens who repelled by a particular frequency but the presence of even the most infinitesimally fine sampling in the reproduction of the soundwave negates the effect. Vinyl alien repellers would soon get plenty popular again.</p><p>So the sort of simplification done by numbers is extremely useful until it is not. And when we leave the realm of the physical or medical and go into the realm of the social, we can see the usefulness fading pretty fast or the cost of the usefulness become pretty steep.</p><p>This is a fairly important corrective to the crowd who say it’s always better to know. I’d say it’s always useful to have the information but not always better to know it.</p><p>So did I swing around to the position that we shouldn’t measure anything ever or at least not anything with a social dimension? No. I stand by my initial conviction that we should measure more things more often. But we shoud always question the utility of the uncertainty reduction the measurement provides and be very mindful of the uncertainty that remains — because no matter how many times we’ve measured, we should always feel a little worried when we cut!</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=29bd0d1a2e9\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/quick-notes-on-measurement-measuring-anything-and-nothing-29bd0d1a2e9\">Quick notes on measurement: Measuring anything and nothing</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Guns, Glory, and Greed: Most wars were not caused by religion (but they weren’t prevented by it…",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://medium.com/feed/metaphor-hacker",
        "value": "Guns, Glory, and Greed: Most wars were not caused by religion (but they weren’t prevented by it…"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://medium.com/metaphor-hacker/guns-glory-and-greed-most-wars-were-not-caused-by-religion-but-they-werent-prevented-by-it-a3002aee387f?source=rss----67ad910371b8---4"
        },
        {
          "rel": "license",
          "href": "http://creativecommons.org/licenses/by/4.0/"
        }
      ],
      "link": "https://medium.com/metaphor-hacker/guns-glory-and-greed-most-wars-were-not-caused-by-religion-but-they-werent-prevented-by-it-a3002aee387f?source=rss----67ad910371b8---4",
      "id": "https://medium.com/p/a3002aee387f",
      "guidislink": false,
      "tags": [
        {
          "term": "morality",
          "scheme": null,
          "label": null
        },
        {
          "term": "war",
          "scheme": null,
          "label": null
        },
        {
          "term": "history",
          "scheme": null,
          "label": null
        },
        {
          "term": "rationalism",
          "scheme": null,
          "label": null
        },
        {
          "term": "religion",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Dominik Lukes"
        }
      ],
      "author": "Dominik Lukes",
      "author_detail": {
        "name": "Dominik Lukes"
      },
      "published": "Sat, 10 Jun 2017 19:56:28 GMT",
      "published_parsed": [
        2017,
        6,
        10,
        19,
        56,
        28,
        5,
        161,
        0
      ],
      "updated": "2017-06-10T20:00:04.607Z",
      "updated_parsed": [
        2017,
        6,
        10,
        20,
        0,
        4,
        5,
        161,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://medium.com/feed/metaphor-hacker",
          "value": "<h3>Guns, Glory, and Greed: Most wars were not caused by religion (but they weren’t prevented by it either)</h3><h3>TL;DR</h3><ol><li>Lots of people say things like ‘religion causes war’</li><li>But looking at a list of wars shows that relatively few are ‘caused’ by religion</li><li>Religion does not stop wars and is often used to bolster the case — but it is rarely the primary causal agent</li><li>What causes wars is: 1) <strong>Guns: </strong>Availability of means to wage war (not literally guns) 2) <strong>Glory:</strong> A framework within which the war makes sense (makes people feel good about fighting it) 3) <strong>Greed: </strong>A desire for resources</li><li>But all of this is even more complicated because neither <em>war</em> not <em>religion</em> are straightforward concepts that can be easily entered into some sort of causality identification algorithm.</li></ol><h3>Historical Note</h3><p>I wrote this about 2 years ago but I felt it needed more thought. Now, that I’ve reread it with some distance, I find that I have very little to add. I have found a few new things (for instance about the religious discourse around many of the Persian wars) but nothing that would make me alter the basic outline. So, here it goes.</p><h3>Background</h3><p>This post has been a long time brewing — first triggered by a Bill Maher routine which pretty much says ‘all wars are caused by religion’ but not seeing a noticeably improved level of nuance in the God Delusion, I got to thinking.</p><p>After finishing the post, I did a quick search on Google and found that ‘religion causes *‘ automatically fills in war, conflict, prejudice. But the top three links or so debunk the notion. So I could have saved myself the time. But despite the plethora of evidence against the religious roots of all conflict, the myth persists. I even heard Karen Armstrong, the ecumenical God peddler, not being able to answer a question on Start the Week with a simple denial of this nonsense. She just waffled about how religion also causes a lot of good (which it also doesn’t in comparison to other institutions or belief systems).</p><h3>So here’s the thing…</h3><p>For a breed that just loves evidence and science, the New Atheists (and frankly most scientists) are just as good at wilfully ignoring what’s right in front of their noses as everybody else. Especially, if it does not fit their grand narrative. New Atheists claim religion causes wars and suffering and will go as far (in offhand moments) to say things like “most wars were caused by religion”. But that’s so patently untrue, it took this old atheist only five minutes of reflection, 10 minutes of research (and admittedly years of peripheral study of history) to completely disprove.</p><p>When I tried this argument on an agnostic friend, she took a lot of convincing. Surely, you’re forgetting some major wars caused exclusively by religion. Maybe religion played a backdoor role. Etc. The argument that religion is at the root of all war is so pervasive that it is almost like an axiom on the order of ‘things fall down when you drop them’. But there is no doubt, religion does not cause wars any more than any other human institution causes wars. The biggest blame we can lay at its door is that it sure does not stop wars, violence and other kinds of suffering. But that’s why we’re atheists, after all. Only, when it comes to causes of war, that’s on us, people. Religion, just like profiteering and prostitution, is just along for the ride.</p><p>This post comes in two parts: 1) A basic overview of the major wars in history and the role religion played in them. 2) A quick look at what caused many of these wars. So strap in, this is going to be fun…</p><h3>The argument: It’s not that religion can’t cause wars, it just usually doesn’t</h3><p>Let’s just look at some of the major wars of history and classify them into three categories:</p><ol><li>Not having to do anything with religion (although ideology — and fervor will have played a part)</li><li>Not really caused by religion but having a significant religious dimension or some relation to it</li><li>Being directly caused by religion or completely about religion (leaving aside the fact that ‘religion’ is not really a useful category)</li></ol><p>I’ll list them in reverse chronological order (I’m going mostly from memory, assuming that this will capture the biggest ones, Wikipedia, as always will oblige with a <a href=\"https://medium.com/feed/en.wikipedia.org/wiki/List_of_wars_by_death_toll\">complete list</a>. A quick glance at those lists shows (not unexpectedly) that my own list is pretty Eurocentric and subject to the availability heuristic, but not particularly unrepresentative.</p><h3>Wars with no religion</h3><p>It would be a massive stretch to think about these wars as having been caused by religion. Sure, religion was around but so was humanism and rationality. Neither helped much.</p><p>The only controversial one here could be the Taiping Rebellion (which was really a 19th century Chinese civil war with outside intervention). The guy who was one of the instigators was so exhausted from failing the imperial examinations that he developed visions in which Jesus came to him and told him he was his brother (he did this to lots of people including old Mane leading to Manicheanism). And the leadership had some cult properties reminiscent of the early protestant excesses. But the war itself was not about religion in any meaningful way. It was all about political control. But if you want, you can stick it in the religion pile — it would be the biggest war ever to be caused by religion.</p><p>So here’s a list of wars with no religion involved. No links, if you don’t know one of them, google it.</p><ul><li>War in Iraq 2</li><li>War in Afghanistan</li><li>African wars in the Congo (DRC)</li><li>Genocide in Rwanda</li><li>War in Iraq 1</li><li>Iraq/Iran war</li><li>All those civil wars in Latin America in the 70s and 80s</li><li>Civil war / genocide in Cambodia</li><li>China-Vietnam war</li><li>USA/France-Vietnam war</li><li>Korean war</li><li>Chinese civil war(s)</li><li>World War 2</li><li>Japanese invasion of China</li><li>Russian revolution (and related wars)</li><li>World War 1</li><li>Scramble for Africa</li><li>Franco-Prussian war</li><li>Crimean war</li><li>Taiping rebellion (Chinese civil war — although the main protagonist had crazy religious visions)</li><li>US Civil War</li><li>War of 1812 (and all the US wars of that time)</li><li>Opium wars</li><li>Napoleonic wars</li><li>Wars of the French revolution</li><li>Prussia’s wars of expansion (Silesia, etc. — Friedrich the Great)</li><li>Ottomans’ wars of expansion</li><li>Aztecs’ wars of expansion</li><li>Wars of the Roses</li><li>Ghenghis Khan’s invasions (and those of his descendants)</li><li>100-year war</li><li>Viking ‘raids’</li><li>An Lushan ‘Rebellion’</li><li>Nomadic tribes’ wars against Rome, China, Central Asia</li><li>Roman wars of expansion and extermination</li><li>Asoka’s (and others’) wars of domination on the Indian subcontinent (though Asoka apparently had a religion inspired change of mind about all this killing)</li><li>Alexander the Great’s campaigns (although he started to think of himself as a God)</li><li>Chinese warring states period(s)</li><li>Persian-Greek war (although the Persians did have some religious justifications)</li><li>Egyptian wars and wars of Mesopotamian states (covering several thousand years with the proviso that Gods went hand in hand with all this)</li></ul><h3>Wars with some religion involved</h3><p>These wars had religion somewhere in them but it wasn’t the primary cause, nor was it the main mover. It may have flared up as part of the conflict or sectarian divisions could have been marshalled as ways of motivating some fighters or the population.</p><ul><li>War in Syria</li><li>Wars in Former Yugoslavia</li><li>Russian war in Aghanistan</li><li>Iranian revolution</li><li>30-year War (this one may be controversial because it sort of seamlessly blends into the ‘wars of religion’)</li><li>Arab civil wars (of 800s and 900s)</li></ul><h3>Wars primarily about religion (although no war was ever solely about religion)</h3><p>These are the wars where religion was the undisputed trigger. Arguably, IS campaigns while fought by people motivated by religion could not have taken hold without the groundwork laid by the previous secular invasion and post-war mismanagement.</p><p>But other than that (and doubtless loads of smaller conflicts here and there), we haven’t had a major religious war in recent centuries. The European Wars of Religion are the only ones to make it in the over 1 million death toll list. I suppose you could argue about the 30-year war as being just an extension of this same conflict but that was a much more geopolitical affair rather than a purely religious one.</p><p>In fact, if you scratch the surface, most of these religious wars had a lot more complicated causes. Just think of the Crusades which are often given as an example of a war caused primarily by religion. And they were full of warlike actors filled with religious zeal and sincere Biblical motivations. But they still found plenty of time for pure political intrigue and once the Crusader states were established, religion became little more than a convenient propaganda tool. But we often forget that people really do believe things sincerely and shape their interpretation of their actions to be consonant with those beliefs. But from outside of somebody’s head, it is impossible not to notice that the social trends proceed in very secular ways of competition for power and resources. Columbus, may he burn in hell, was primarily motivated by a desire to ‘free’ Jerusalem again. But it didn’t stop him from commercially exploiting the peoples he found along the way. And it would be a very brave historian who would claim that the subsequent murderous conquest of the Americas and all the things that came from it was caused by or even primarily associated with religion. Of course, most of the warlords and killers involved in it were deeply religious and religious institutions benefited from and encouraged (as well as moderated) their actions. But so did lots of other institutions — it’s just that in their case, we’re not as struck by the sheer hypocrisy of their actions and claims.</p><ul><li>IS campaigns (but precipitated by and in the middle of larger non-religious conflicts)</li><li>European wars of religion (intermixed with non-religious concerns)</li><li>Hussite wars (proto-nationalistic wars but with a strong religious component) — also part of the broader Crusade ethos</li><li>Crusades (religion played key role but neither the ultimate cause nor the only driver)</li><li>Arab conquests of 600s (purely religious but fairly quick and relatively bloodless in many places — religion was the key motivation and impetus but they were not doctrinal so much as ideological — similar to spread of democracy)</li><li>I can’t think of any earlier purely religious wars (other than those in the Old Testament) although the Persians certainly had a lot of religious zeal as did their prececessors when it suited them</li></ul><p>So we can see that it has been hundreds of years since a major war was fought purely over religion, if it ever really was.</p><h3>What causes wars?</h3><p>Having looked at the wars on my list, there are things they have in common. Thankfully, they turned out to be alliterative. They are guns, glory and greed. With a bit of gullibility thrown in. I’m not sure wars really need causes. Or that it makes sense to talk about causes any more than it makes sense to talk about <a href=\"https://medium.com/metaphor-hacker/history-as-weather-a-fractal-theory-of-history-for-ian-morris-jared-diamond-and-cgp-grey-45b5503486c5\">causes in weather</a>. Lots of little causes are easy to see, but they don’t add up to one big one. But these three are certainly a part of the grease that makes war machines grind along. So lets take them in order.</p><h3>Guns</h3><p>Guns, as we know, don’t kill people. People kill people. This is not a bad slogan and it’s not too far off the truth. You can give people who don’t want to kill other people all the guns you want, and they won’t kill with them. Switzerland is given as the shining example. What people don’t tell you about Switzerland is that while the proportion of murdered people is about the same as the rest of Europe, the proportion of the people who do get murdered that get murdered by a gun in about the same as in the US. So having guns or weapons of any kind makes war possible.</p><p>But people still have to want to use those guns, even at the risk of having other guns used against them.</p><p>Crusades could be given as an example of how guns are involved in war (metaphorical guns, of course). The Crusaders were knights which started to cause real problems around Europe around the 11th century- think Latin American drug cartels. So having them around made it natural to think of using them to achieve other goals. Sometimes this is portrayed as if the Cursades were started explicitly to give the knights something to do (preferrably outside Europe) but that could hardly have been the case. However, had the knights not been available (as well as a problem), the Crusades could not have started. Also, it is not uncommon for people to justify military service as a way of giving young men discipline.</p><p>World War I is often given as an example. All the European powers were armed to the teeth — Russia, France, and Germany, so that it was inevitable one of them would use their armaments. They didn’t have to. But they did.</p><p>It’s no coincidence that victorious parties often want the losers to disarm. It’s no coincidence that people want to reduce the spread of nuclear weapons even if nobody wants to use them. Having ‘guns’ is the <em>sine qua non</em> of war.</p><p><strong>Note:</strong> I cannot overemphasize how metaphorical ‘guns’ are here. Actual guns are often much less important when there is no good tactical advantage. They were NOT the thing that swung the big civilizational conflicts between Europe and the Americas, Europe and Africa and Europe and Asia — at least not until the 1800s by which point the damage was done. Even when they did make a difference, their effects were not predictable. Both Japan and China had to bow down (for a while) before the might of Western weaponry but neither ended up like the Americas. Even the use of a machine gun (invented to end all wars because the horrors they would unleash would make us think twice about starting them) took decades before they offered an unambiguous advantage. Nuclear weapons on the other hand, are a clear tactical winner, but any actual use is such crappy strategy, that we can imagine their use only by accident or as a result of mental derrangement. So, in short, weapons do matter. Better weapons are better than bad weapons. But (as the US found in Vietnam and Russia in Afghanistan — and much earlier Spain in the Canaries or the French at Agincourt) they’re only the decisive factor if you can find strategic uses for the tactical advantages superior weapons give you.</p><h3>Glory</h3><p>“There are certain things worth dying for.” said the celebrated Czech philosopher Jan Patočka. This is often given as an example of his love of freedom and personal courage. (He caught a chill when visiting a proscribed event, was held by the police for a few hours and died a few days later a few months short of 70th birthday). If this is an example of anything, it is an example of how much bullshit philosophers are allowed to get away with. This is the motto of every freedom fighter with case of explosive strapped around her chest. Of course, it’s only a short way from that to ‘there are things worth killing for’ after all. But it is an example of a type of glory.</p><p>The Romans had their triumphs — big marches in Rome to celebrate victorious campaigns. Many Celtic tribe was exterminated just so that a general could get a triumph. But these ‘triumphs’ can be more symbolic than that.</p><p>Honor is another type of glory. The conflicts that kicked off World War I were justified by appeals to honor. English public opinion changed around from anti to pro war in a matter of weeks once appeals to honor started being bandied about. And once you have honor, you have duty. Duty to ‘serve your country’ — ideally by killing as many other people as you can.</p><p>And the argument could be made that the long periods of European ‘peace’ were mostly due to the case for glory being discredited by first Napoleonic Wars and then WWI and WWII. In the US, it was first the Civil War and later Vietman. This discreditation of ‘war is glory’ trope seems to last one to two generations (40–80 years) but eventually fades away and new wars are started.</p><h3>Greed</h3><p>We think of greed as a negative value — unless we’re deranged free-marketeers. But greed is wanting more than you need — or reimagining your wants as needs. For millenia, many wars were fought acquisitively. The Romans, the Mongols, the Jurchen — they all fought regular wars (sometimes described as raids by historians) just to get stuff. For Mongols it was because they had very little stuff to start with. For the Roman and British empires it was a way of generating growth — ensuring sources of raw materials, food and taxes on the one hand, and new markets for goods on the other. There was plenty of other rhetoric mixed in with all this, but getting the stuff mattered.</p><p>The Crusades and the Mongol conquests destroyed a lot of established systems but by that they also opened up new opportunities for growth. As did the American conquests from 1500s all the way to the 1890s. The early Arab conquests were purely instigated by religious zeal — doing God’s work — but they used a lot of the old raiding systems to do it. Warriors got booty according to old raiding traditions, tribal rivalries flared up in the distribution of conquests — all mixed up with emerging new structures that eventually resulted in the glorious caliphates heights of science, commerce and art — and an imperial style of governance.</p><p>WWII was purely motivated about acquisition of resources — the vaunted lebens raum had a lot of wheat and oil fields in it — on the side of the instigators. A lot of other ideologies were mixed in. And what were they? Honor and glory, of course. And glorious feats of engineering producing better and better guns.</p><h3>Myopia of history for the greater glory of war</h3><p>It is interesting to read historians describing their favourite subjects’ wars. Everyone likes a winner. Egyptologists squable with Hettitologists over who won the battle of Kadesh. Roman historians weep over the lost glory of Ceasar’s conquests. Mongolologists point to the amazing feats of Ghengis Khan. Sinologists praise the expansions of the Han, Japanologists like their Tokugawa. Napoleon also gets his share of defenders.</p><p>It’s only the historians of the WWI and WWII losing sides, who get to — or are required to — dislike the subjects they study (they can’t get away with just a polite tsk, tsk). But imagine looking at those from the remove of 200 years. No survivors or direct descendants of survivors are around any more. Some old memories may still rankle, just like some Civil War grievances still rankle in the US. But the further removed, the less visceral these are. Even those defeated by Napoleon are now putting plaques on houses where he might have once slept. Some historical villains still persist — Attilla the Hun, Xerxes, Nero but most of the former villains are either plucky heros (Hannibal) or outright models to emmulate (Ceasar). Even more are just forgotten by the general public, their deeds recorded by a few specialists. But so many of those who we remember were not nice people, they had blood on their hands.</p><p>There’s also the glorification of war by the back door. Look at all the technological progress military research gave us. Drones, encryption, DARPANet. Or even, just look at all the peace war gave us. Respectable peaceful people have said all these things (not necessarily wrongly) in genteel society. Once it enters the area of politeness, it is hard to dislodge. In the US, people thank anybody in the military for ‘their service’, in the UK, people sing in choirs to get Help for Heros and everyone wears red poppies in November to remember how their ancestors killed to save democracy.</p><p>And at the same time we all chant ‘give peace a chance’,’violence never solves anything’, ‘if only people would sit down together and talk like adults’. What makes war possible is our ability to at the same time believe that killing someone is wrong under any circumstances and that killing someone near our target by a bomb is collateral damage. Now this is not a case of hypocrisy. It is a fundamental paradox of human existence.</p><p>So if we wonder who causes wars, the answer is clear. It is not religion. It is not even guns or glory or greed. It is heroes. Like us.</p><h3>What is religion? Oh, sorry, there’s no such ‘thing’</h3><p>We take it for granted that we know what <em>religion</em> is or even that there is such a thing exists. I wrote a long time ago that I know there’s no God, but <a href=\"http://metaphorhacker.net/2011/04/religion-if-it-exists-is-negotiation-of-underdetermined-metaphoric-cognition/\">I don’t believe religion exists</a>.</p><p>Since then, I’ve come across a lot more writing by scholars of religion that confirmed my intution. The notion that there is such a thing as religion or faith that is separate from other institutions or mental states is just a relatively recent invention. We just label practices and beliefs around things to do with supernatural justification of X as ‘religion’ but don’t notice how close they are to other semi-natural beliefs and practices. We think of witchcraft as religion but alternative medicine as a health movement. We talk about Shinto as a religion but not national parks or ‘areas of special natural beauty’ as religion. Neo-pagans strike us as something wrong because of a strange sense of inauthenticity not quite like Jedi but close. But when we look closely all of these, we find that the beliefs and actual practices are very similar. Even in science a lot of practices and beliefs are much closer to organized religion than we might think. But because for us, religion is this newly separate category, a statement like this immediately makes us draw up a list of differences between ‘religion’ and ‘science’ and adopt a defensive (apologetic) posture. But the question is not are ‘religion’ and ‘science’ the same. But rather are they more different from each other than individal scientific disciplines or individual religions are different from each other? How is ‘string theory’ different from ‘scholastic Christianity’, how is ‘ornithology’ different from ‘shamanism’, how is ‘political science’ different from ‘Confucianism’? These questions give us one set of answers. But how is ‘string theory’ different from ‘ornithology’ and ‘scholastic Christianity’ from ‘Confucianism’? That gives us another set of answers.</p><p>But if we put those answers side by side, we begin to see that the artificial categories of ‘science’ and ‘religion’ are not very useful. It is possible that ‘ornithology’ has more in common with ‘witchcraft’ than ‘string theory’ which in turn has less in common with ‘political science’ than ‘Daoism’. We certainly know that holding deeply supranatural beliefs is no impediment to doing theoretical physics — witness Newton and others.</p><p>But even distinctions between religions themselves can be artificial. In practice, you can see Budhists involved in a lot of Shinto practices or Hindus venerating Muslim shrines. Even things like Christianity and Islam only seem irreconcilably different to us because of historical and political reasons. Doctrinally, Islam could easily be seen (and was seen by many) as a reform version of Christianity and/or Judaism. Just as many early Christians thought of themselves as reformed Jews.</p><p>People often point to the minute doctrinal or liturgical differences between warring factions to illustrate the ‘irrationality’ of religion. But that is only because they are completely ‘irrationally’ using a category outside its context. Because these differences are easily visible, they get elevated to forefront, but other non-doctrinal or non-liturgical diferences are their foundation. Northern Ireland is a great example. Catholics and Protestants are just the ‘religious’ labels given to different sides in a political-military conflict. So are the many factional wars in the Middle East. Sunni and Shi’a have been labels for political differences over leadership of the community from the very beginning. Not in the least over the nature of doctrine (just over who in the community is responsible for its interpretation). Of course, over time the divergences became greater but still relatively minor. The other political differences, however, are huge. And they are different in different regions where Sunni and Shi’a interact — though sometimes conflicts are exported.</p><p>But the point is that people are involved in a number of personal and institutional relationships as well as experiencing a range of mental states and discoursive events. Arbitrarily cutting off some as religion and others as non-religion obscrures more than it reveals. Religion can be a useful shortcut to certain institutions such as ‘Catholic religion’ historically associated with certain type of meta-natural doctrine and some institutional structure but outside of this usage, we are much better off acknowledging that it does not exist as an actual ‘natural’ category.</p><p>Just because we have ‘religion studies’, it doesn’t mean there is such a ‘thing’ as religion. When we compare the big ‘World Religions’ such as Christianity, Islam, Hinduism, Budhism, Confucianism with each other, they end up being called ‘religion’ because we artificial extract the sort of similarities that matter to us (mostly with the lenses of Christianity on). But look much closer inside and you’ll see more differences and some surprising similarities. On the one hand, doctrinally, Islam could be seen as one branch of reformed Christianity. On the other, it has none of the institutional structures various Christian denominations have. Hinduism looks very similar metaphysically but is nothing alike in actual doctrine or organization. And what about the various Budhisms? Even more important is actual practices around the world and across time are much more complex than our ‘one man, one God’ perspective. People have always fluidly associated themselves with elements of different ‘religions’ — Hindus going to Muslim shrines, Budhist marriages and Shinto funerals in Japan, etc. It’s not unlike the old Pagan days, except that the meta-discourse filters these aspects out through all sorts of essentializating ecumenical rituals (looking at you Karen Amrstrong….).</p><h3>Is there such a thing as war?</h3><p>Yes, why else would there have been ministries of it? But the problem is not whether there is war but what should be counted as war. There are legitimate suggestions to count WWI and WWII as simply one war with some breaks — a longer one in Europe but a much shorter one in other parts of the world (such as China). But at the same time we could also say that it was a period during which many conflicts were going on around the world which we are conveniently referring to as World War — but actually most of the world was at peace during that time (India, South and Central Americas, much of Africa and lots of individual regions in Europe and Asia). And even though Canada and the US as well as Australia were combatants in the wars, they suffered no direct infrastructural damage and their civilian populations were not directly exposed to the ravages of war.</p><p>How should we count the Mongol wars of expansion? One war or many? The Crusades? Roman wars of conquest? Persian campaigns East of the Greek penninsula? Arab conquests? Thirty-year war? If we look at the Encyclopedia of Wars, we’ll see that some entries cover complex conflicts that last decades to those that are over in weeks or days. So while there’s always some armed conflict going on, when does it slip over into war? In our definition, we sort of assume state actors and formal armies but most wars elude some part of those assumptions (such as use of mercenaries, assymetry between organizational status of combatants, etc.). We also slip and slide between metaphorical and literal wars with ease. The Cold War was a metaphor but involved lots of proxy wars (US in Vietnam, USSR in Afghanistan, etc.) War on drugs or crime is also a metaphor but slides into armed conflict — often involving state actors, armies and sever consequences for local populations. But we also have war on poverty, which generally stays in the metaphorical realm, as do the culture wars, etc.</p><p>This means that if we want to be formal and count wars and their causes we’re going to be making some subjective decisions that will very much influence the outcomes.</p><h3>References</h3><p><a href=\"http://newbooksinsoutheastasianstudies.com/2015/09/15/christopher-r-duncan-violence-and-vengeance-religious-conflict-and-its-aftermath-in-eastern-indonesia-cornell-up-2013/\">http://newbooksinsoutheastasianstudies.com/2015/09/15/christopher-r-duncan-violence-and-vengeance-religious-conflict-and-its-aftermath-in-eastern-indonesia-cornell-up-2013/</a></p><p><a href=\"https://en.wikipedia.org/wiki/List_of_wars_by_death_toll#List_of_wars_by_death_toll_with_over_1.2C000.2C000_deaths\">https://en.wikipedia.org/wiki/List_of_wars_by_death_toll#List_of_wars_by_death_toll_with_over_1.2C000.2C000_deaths</a></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a3002aee387f\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/guns-glory-and-greed-most-wars-were-not-caused-by-religion-but-they-werent-prevented-by-it-a3002aee387f\">Guns, Glory, and Greed: Most wars were not caused by religion (but they weren’t prevented by it…</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        }
      ],
      "summary": "<h3>Guns, Glory, and Greed: Most wars were not caused by religion (but they weren’t prevented by it either)</h3><h3>TL;DR</h3><ol><li>Lots of people say things like ‘religion causes war’</li><li>But looking at a list of wars shows that relatively few are ‘caused’ by religion</li><li>Religion does not stop wars and is often used to bolster the case — but it is rarely the primary causal agent</li><li>What causes wars is: 1) <strong>Guns: </strong>Availability of means to wage war (not literally guns) 2) <strong>Glory:</strong> A framework within which the war makes sense (makes people feel good about fighting it) 3) <strong>Greed: </strong>A desire for resources</li><li>But all of this is even more complicated because neither <em>war</em> not <em>religion</em> are straightforward concepts that can be easily entered into some sort of causality identification algorithm.</li></ol><h3>Historical Note</h3><p>I wrote this about 2 years ago but I felt it needed more thought. Now, that I’ve reread it with some distance, I find that I have very little to add. I have found a few new things (for instance about the religious discourse around many of the Persian wars) but nothing that would make me alter the basic outline. So, here it goes.</p><h3>Background</h3><p>This post has been a long time brewing — first triggered by a Bill Maher routine which pretty much says ‘all wars are caused by religion’ but not seeing a noticeably improved level of nuance in the God Delusion, I got to thinking.</p><p>After finishing the post, I did a quick search on Google and found that ‘religion causes *‘ automatically fills in war, conflict, prejudice. But the top three links or so debunk the notion. So I could have saved myself the time. But despite the plethora of evidence against the religious roots of all conflict, the myth persists. I even heard Karen Armstrong, the ecumenical God peddler, not being able to answer a question on Start the Week with a simple denial of this nonsense. She just waffled about how religion also causes a lot of good (which it also doesn’t in comparison to other institutions or belief systems).</p><h3>So here’s the thing…</h3><p>For a breed that just loves evidence and science, the New Atheists (and frankly most scientists) are just as good at wilfully ignoring what’s right in front of their noses as everybody else. Especially, if it does not fit their grand narrative. New Atheists claim religion causes wars and suffering and will go as far (in offhand moments) to say things like “most wars were caused by religion”. But that’s so patently untrue, it took this old atheist only five minutes of reflection, 10 minutes of research (and admittedly years of peripheral study of history) to completely disprove.</p><p>When I tried this argument on an agnostic friend, she took a lot of convincing. Surely, you’re forgetting some major wars caused exclusively by religion. Maybe religion played a backdoor role. Etc. The argument that religion is at the root of all war is so pervasive that it is almost like an axiom on the order of ‘things fall down when you drop them’. But there is no doubt, religion does not cause wars any more than any other human institution causes wars. The biggest blame we can lay at its door is that it sure does not stop wars, violence and other kinds of suffering. But that’s why we’re atheists, after all. Only, when it comes to causes of war, that’s on us, people. Religion, just like profiteering and prostitution, is just along for the ride.</p><p>This post comes in two parts: 1) A basic overview of the major wars in history and the role religion played in them. 2) A quick look at what caused many of these wars. So strap in, this is going to be fun…</p><h3>The argument: It’s not that religion can’t cause wars, it just usually doesn’t</h3><p>Let’s just look at some of the major wars of history and classify them into three categories:</p><ol><li>Not having to do anything with religion (although ideology — and fervor will have played a part)</li><li>Not really caused by religion but having a significant religious dimension or some relation to it</li><li>Being directly caused by religion or completely about religion (leaving aside the fact that ‘religion’ is not really a useful category)</li></ol><p>I’ll list them in reverse chronological order (I’m going mostly from memory, assuming that this will capture the biggest ones, Wikipedia, as always will oblige with a <a href=\"https://medium.com/feed/en.wikipedia.org/wiki/List_of_wars_by_death_toll\">complete list</a>. A quick glance at those lists shows (not unexpectedly) that my own list is pretty Eurocentric and subject to the availability heuristic, but not particularly unrepresentative.</p><h3>Wars with no religion</h3><p>It would be a massive stretch to think about these wars as having been caused by religion. Sure, religion was around but so was humanism and rationality. Neither helped much.</p><p>The only controversial one here could be the Taiping Rebellion (which was really a 19th century Chinese civil war with outside intervention). The guy who was one of the instigators was so exhausted from failing the imperial examinations that he developed visions in which Jesus came to him and told him he was his brother (he did this to lots of people including old Mane leading to Manicheanism). And the leadership had some cult properties reminiscent of the early protestant excesses. But the war itself was not about religion in any meaningful way. It was all about political control. But if you want, you can stick it in the religion pile — it would be the biggest war ever to be caused by religion.</p><p>So here’s a list of wars with no religion involved. No links, if you don’t know one of them, google it.</p><ul><li>War in Iraq 2</li><li>War in Afghanistan</li><li>African wars in the Congo (DRC)</li><li>Genocide in Rwanda</li><li>War in Iraq 1</li><li>Iraq/Iran war</li><li>All those civil wars in Latin America in the 70s and 80s</li><li>Civil war / genocide in Cambodia</li><li>China-Vietnam war</li><li>USA/France-Vietnam war</li><li>Korean war</li><li>Chinese civil war(s)</li><li>World War 2</li><li>Japanese invasion of China</li><li>Russian revolution (and related wars)</li><li>World War 1</li><li>Scramble for Africa</li><li>Franco-Prussian war</li><li>Crimean war</li><li>Taiping rebellion (Chinese civil war — although the main protagonist had crazy religious visions)</li><li>US Civil War</li><li>War of 1812 (and all the US wars of that time)</li><li>Opium wars</li><li>Napoleonic wars</li><li>Wars of the French revolution</li><li>Prussia’s wars of expansion (Silesia, etc. — Friedrich the Great)</li><li>Ottomans’ wars of expansion</li><li>Aztecs’ wars of expansion</li><li>Wars of the Roses</li><li>Ghenghis Khan’s invasions (and those of his descendants)</li><li>100-year war</li><li>Viking ‘raids’</li><li>An Lushan ‘Rebellion’</li><li>Nomadic tribes’ wars against Rome, China, Central Asia</li><li>Roman wars of expansion and extermination</li><li>Asoka’s (and others’) wars of domination on the Indian subcontinent (though Asoka apparently had a religion inspired change of mind about all this killing)</li><li>Alexander the Great’s campaigns (although he started to think of himself as a God)</li><li>Chinese warring states period(s)</li><li>Persian-Greek war (although the Persians did have some religious justifications)</li><li>Egyptian wars and wars of Mesopotamian states (covering several thousand years with the proviso that Gods went hand in hand with all this)</li></ul><h3>Wars with some religion involved</h3><p>These wars had religion somewhere in them but it wasn’t the primary cause, nor was it the main mover. It may have flared up as part of the conflict or sectarian divisions could have been marshalled as ways of motivating some fighters or the population.</p><ul><li>War in Syria</li><li>Wars in Former Yugoslavia</li><li>Russian war in Aghanistan</li><li>Iranian revolution</li><li>30-year War (this one may be controversial because it sort of seamlessly blends into the ‘wars of religion’)</li><li>Arab civil wars (of 800s and 900s)</li></ul><h3>Wars primarily about religion (although no war was ever solely about religion)</h3><p>These are the wars where religion was the undisputed trigger. Arguably, IS campaigns while fought by people motivated by religion could not have taken hold without the groundwork laid by the previous secular invasion and post-war mismanagement.</p><p>But other than that (and doubtless loads of smaller conflicts here and there), we haven’t had a major religious war in recent centuries. The European Wars of Religion are the only ones to make it in the over 1 million death toll list. I suppose you could argue about the 30-year war as being just an extension of this same conflict but that was a much more geopolitical affair rather than a purely religious one.</p><p>In fact, if you scratch the surface, most of these religious wars had a lot more complicated causes. Just think of the Crusades which are often given as an example of a war caused primarily by religion. And they were full of warlike actors filled with religious zeal and sincere Biblical motivations. But they still found plenty of time for pure political intrigue and once the Crusader states were established, religion became little more than a convenient propaganda tool. But we often forget that people really do believe things sincerely and shape their interpretation of their actions to be consonant with those beliefs. But from outside of somebody’s head, it is impossible not to notice that the social trends proceed in very secular ways of competition for power and resources. Columbus, may he burn in hell, was primarily motivated by a desire to ‘free’ Jerusalem again. But it didn’t stop him from commercially exploiting the peoples he found along the way. And it would be a very brave historian who would claim that the subsequent murderous conquest of the Americas and all the things that came from it was caused by or even primarily associated with religion. Of course, most of the warlords and killers involved in it were deeply religious and religious institutions benefited from and encouraged (as well as moderated) their actions. But so did lots of other institutions — it’s just that in their case, we’re not as struck by the sheer hypocrisy of their actions and claims.</p><ul><li>IS campaigns (but precipitated by and in the middle of larger non-religious conflicts)</li><li>European wars of religion (intermixed with non-religious concerns)</li><li>Hussite wars (proto-nationalistic wars but with a strong religious component) — also part of the broader Crusade ethos</li><li>Crusades (religion played key role but neither the ultimate cause nor the only driver)</li><li>Arab conquests of 600s (purely religious but fairly quick and relatively bloodless in many places — religion was the key motivation and impetus but they were not doctrinal so much as ideological — similar to spread of democracy)</li><li>I can’t think of any earlier purely religious wars (other than those in the Old Testament) although the Persians certainly had a lot of religious zeal as did their prececessors when it suited them</li></ul><p>So we can see that it has been hundreds of years since a major war was fought purely over religion, if it ever really was.</p><h3>What causes wars?</h3><p>Having looked at the wars on my list, there are things they have in common. Thankfully, they turned out to be alliterative. They are guns, glory and greed. With a bit of gullibility thrown in. I’m not sure wars really need causes. Or that it makes sense to talk about causes any more than it makes sense to talk about <a href=\"https://medium.com/metaphor-hacker/history-as-weather-a-fractal-theory-of-history-for-ian-morris-jared-diamond-and-cgp-grey-45b5503486c5\">causes in weather</a>. Lots of little causes are easy to see, but they don’t add up to one big one. But these three are certainly a part of the grease that makes war machines grind along. So lets take them in order.</p><h3>Guns</h3><p>Guns, as we know, don’t kill people. People kill people. This is not a bad slogan and it’s not too far off the truth. You can give people who don’t want to kill other people all the guns you want, and they won’t kill with them. Switzerland is given as the shining example. What people don’t tell you about Switzerland is that while the proportion of murdered people is about the same as the rest of Europe, the proportion of the people who do get murdered that get murdered by a gun in about the same as in the US. So having guns or weapons of any kind makes war possible.</p><p>But people still have to want to use those guns, even at the risk of having other guns used against them.</p><p>Crusades could be given as an example of how guns are involved in war (metaphorical guns, of course). The Crusaders were knights which started to cause real problems around Europe around the 11th century- think Latin American drug cartels. So having them around made it natural to think of using them to achieve other goals. Sometimes this is portrayed as if the Cursades were started explicitly to give the knights something to do (preferrably outside Europe) but that could hardly have been the case. However, had the knights not been available (as well as a problem), the Crusades could not have started. Also, it is not uncommon for people to justify military service as a way of giving young men discipline.</p><p>World War I is often given as an example. All the European powers were armed to the teeth — Russia, France, and Germany, so that it was inevitable one of them would use their armaments. They didn’t have to. But they did.</p><p>It’s no coincidence that victorious parties often want the losers to disarm. It’s no coincidence that people want to reduce the spread of nuclear weapons even if nobody wants to use them. Having ‘guns’ is the <em>sine qua non</em> of war.</p><p><strong>Note:</strong> I cannot overemphasize how metaphorical ‘guns’ are here. Actual guns are often much less important when there is no good tactical advantage. They were NOT the thing that swung the big civilizational conflicts between Europe and the Americas, Europe and Africa and Europe and Asia — at least not until the 1800s by which point the damage was done. Even when they did make a difference, their effects were not predictable. Both Japan and China had to bow down (for a while) before the might of Western weaponry but neither ended up like the Americas. Even the use of a machine gun (invented to end all wars because the horrors they would unleash would make us think twice about starting them) took decades before they offered an unambiguous advantage. Nuclear weapons on the other hand, are a clear tactical winner, but any actual use is such crappy strategy, that we can imagine their use only by accident or as a result of mental derrangement. So, in short, weapons do matter. Better weapons are better than bad weapons. But (as the US found in Vietnam and Russia in Afghanistan — and much earlier Spain in the Canaries or the French at Agincourt) they’re only the decisive factor if you can find strategic uses for the tactical advantages superior weapons give you.</p><h3>Glory</h3><p>“There are certain things worth dying for.” said the celebrated Czech philosopher Jan Patočka. This is often given as an example of his love of freedom and personal courage. (He caught a chill when visiting a proscribed event, was held by the police for a few hours and died a few days later a few months short of 70th birthday). If this is an example of anything, it is an example of how much bullshit philosophers are allowed to get away with. This is the motto of every freedom fighter with case of explosive strapped around her chest. Of course, it’s only a short way from that to ‘there are things worth killing for’ after all. But it is an example of a type of glory.</p><p>The Romans had their triumphs — big marches in Rome to celebrate victorious campaigns. Many Celtic tribe was exterminated just so that a general could get a triumph. But these ‘triumphs’ can be more symbolic than that.</p><p>Honor is another type of glory. The conflicts that kicked off World War I were justified by appeals to honor. English public opinion changed around from anti to pro war in a matter of weeks once appeals to honor started being bandied about. And once you have honor, you have duty. Duty to ‘serve your country’ — ideally by killing as many other people as you can.</p><p>And the argument could be made that the long periods of European ‘peace’ were mostly due to the case for glory being discredited by first Napoleonic Wars and then WWI and WWII. In the US, it was first the Civil War and later Vietman. This discreditation of ‘war is glory’ trope seems to last one to two generations (40–80 years) but eventually fades away and new wars are started.</p><h3>Greed</h3><p>We think of greed as a negative value — unless we’re deranged free-marketeers. But greed is wanting more than you need — or reimagining your wants as needs. For millenia, many wars were fought acquisitively. The Romans, the Mongols, the Jurchen — they all fought regular wars (sometimes described as raids by historians) just to get stuff. For Mongols it was because they had very little stuff to start with. For the Roman and British empires it was a way of generating growth — ensuring sources of raw materials, food and taxes on the one hand, and new markets for goods on the other. There was plenty of other rhetoric mixed in with all this, but getting the stuff mattered.</p><p>The Crusades and the Mongol conquests destroyed a lot of established systems but by that they also opened up new opportunities for growth. As did the American conquests from 1500s all the way to the 1890s. The early Arab conquests were purely instigated by religious zeal — doing God’s work — but they used a lot of the old raiding systems to do it. Warriors got booty according to old raiding traditions, tribal rivalries flared up in the distribution of conquests — all mixed up with emerging new structures that eventually resulted in the glorious caliphates heights of science, commerce and art — and an imperial style of governance.</p><p>WWII was purely motivated about acquisition of resources — the vaunted lebens raum had a lot of wheat and oil fields in it — on the side of the instigators. A lot of other ideologies were mixed in. And what were they? Honor and glory, of course. And glorious feats of engineering producing better and better guns.</p><h3>Myopia of history for the greater glory of war</h3><p>It is interesting to read historians describing their favourite subjects’ wars. Everyone likes a winner. Egyptologists squable with Hettitologists over who won the battle of Kadesh. Roman historians weep over the lost glory of Ceasar’s conquests. Mongolologists point to the amazing feats of Ghengis Khan. Sinologists praise the expansions of the Han, Japanologists like their Tokugawa. Napoleon also gets his share of defenders.</p><p>It’s only the historians of the WWI and WWII losing sides, who get to — or are required to — dislike the subjects they study (they can’t get away with just a polite tsk, tsk). But imagine looking at those from the remove of 200 years. No survivors or direct descendants of survivors are around any more. Some old memories may still rankle, just like some Civil War grievances still rankle in the US. But the further removed, the less visceral these are. Even those defeated by Napoleon are now putting plaques on houses where he might have once slept. Some historical villains still persist — Attilla the Hun, Xerxes, Nero but most of the former villains are either plucky heros (Hannibal) or outright models to emmulate (Ceasar). Even more are just forgotten by the general public, their deeds recorded by a few specialists. But so many of those who we remember were not nice people, they had blood on their hands.</p><p>There’s also the glorification of war by the back door. Look at all the technological progress military research gave us. Drones, encryption, DARPANet. Or even, just look at all the peace war gave us. Respectable peaceful people have said all these things (not necessarily wrongly) in genteel society. Once it enters the area of politeness, it is hard to dislodge. In the US, people thank anybody in the military for ‘their service’, in the UK, people sing in choirs to get Help for Heros and everyone wears red poppies in November to remember how their ancestors killed to save democracy.</p><p>And at the same time we all chant ‘give peace a chance’,’violence never solves anything’, ‘if only people would sit down together and talk like adults’. What makes war possible is our ability to at the same time believe that killing someone is wrong under any circumstances and that killing someone near our target by a bomb is collateral damage. Now this is not a case of hypocrisy. It is a fundamental paradox of human existence.</p><p>So if we wonder who causes wars, the answer is clear. It is not religion. It is not even guns or glory or greed. It is heroes. Like us.</p><h3>What is religion? Oh, sorry, there’s no such ‘thing’</h3><p>We take it for granted that we know what <em>religion</em> is or even that there is such a thing exists. I wrote a long time ago that I know there’s no God, but <a href=\"http://metaphorhacker.net/2011/04/religion-if-it-exists-is-negotiation-of-underdetermined-metaphoric-cognition/\">I don’t believe religion exists</a>.</p><p>Since then, I’ve come across a lot more writing by scholars of religion that confirmed my intution. The notion that there is such a thing as religion or faith that is separate from other institutions or mental states is just a relatively recent invention. We just label practices and beliefs around things to do with supernatural justification of X as ‘religion’ but don’t notice how close they are to other semi-natural beliefs and practices. We think of witchcraft as religion but alternative medicine as a health movement. We talk about Shinto as a religion but not national parks or ‘areas of special natural beauty’ as religion. Neo-pagans strike us as something wrong because of a strange sense of inauthenticity not quite like Jedi but close. But when we look closely all of these, we find that the beliefs and actual practices are very similar. Even in science a lot of practices and beliefs are much closer to organized religion than we might think. But because for us, religion is this newly separate category, a statement like this immediately makes us draw up a list of differences between ‘religion’ and ‘science’ and adopt a defensive (apologetic) posture. But the question is not are ‘religion’ and ‘science’ the same. But rather are they more different from each other than individal scientific disciplines or individual religions are different from each other? How is ‘string theory’ different from ‘scholastic Christianity’, how is ‘ornithology’ different from ‘shamanism’, how is ‘political science’ different from ‘Confucianism’? These questions give us one set of answers. But how is ‘string theory’ different from ‘ornithology’ and ‘scholastic Christianity’ from ‘Confucianism’? That gives us another set of answers.</p><p>But if we put those answers side by side, we begin to see that the artificial categories of ‘science’ and ‘religion’ are not very useful. It is possible that ‘ornithology’ has more in common with ‘witchcraft’ than ‘string theory’ which in turn has less in common with ‘political science’ than ‘Daoism’. We certainly know that holding deeply supranatural beliefs is no impediment to doing theoretical physics — witness Newton and others.</p><p>But even distinctions between religions themselves can be artificial. In practice, you can see Budhists involved in a lot of Shinto practices or Hindus venerating Muslim shrines. Even things like Christianity and Islam only seem irreconcilably different to us because of historical and political reasons. Doctrinally, Islam could easily be seen (and was seen by many) as a reform version of Christianity and/or Judaism. Just as many early Christians thought of themselves as reformed Jews.</p><p>People often point to the minute doctrinal or liturgical differences between warring factions to illustrate the ‘irrationality’ of religion. But that is only because they are completely ‘irrationally’ using a category outside its context. Because these differences are easily visible, they get elevated to forefront, but other non-doctrinal or non-liturgical diferences are their foundation. Northern Ireland is a great example. Catholics and Protestants are just the ‘religious’ labels given to different sides in a political-military conflict. So are the many factional wars in the Middle East. Sunni and Shi’a have been labels for political differences over leadership of the community from the very beginning. Not in the least over the nature of doctrine (just over who in the community is responsible for its interpretation). Of course, over time the divergences became greater but still relatively minor. The other political differences, however, are huge. And they are different in different regions where Sunni and Shi’a interact — though sometimes conflicts are exported.</p><p>But the point is that people are involved in a number of personal and institutional relationships as well as experiencing a range of mental states and discoursive events. Arbitrarily cutting off some as religion and others as non-religion obscrures more than it reveals. Religion can be a useful shortcut to certain institutions such as ‘Catholic religion’ historically associated with certain type of meta-natural doctrine and some institutional structure but outside of this usage, we are much better off acknowledging that it does not exist as an actual ‘natural’ category.</p><p>Just because we have ‘religion studies’, it doesn’t mean there is such a ‘thing’ as religion. When we compare the big ‘World Religions’ such as Christianity, Islam, Hinduism, Budhism, Confucianism with each other, they end up being called ‘religion’ because we artificial extract the sort of similarities that matter to us (mostly with the lenses of Christianity on). But look much closer inside and you’ll see more differences and some surprising similarities. On the one hand, doctrinally, Islam could be seen as one branch of reformed Christianity. On the other, it has none of the institutional structures various Christian denominations have. Hinduism looks very similar metaphysically but is nothing alike in actual doctrine or organization. And what about the various Budhisms? Even more important is actual practices around the world and across time are much more complex than our ‘one man, one God’ perspective. People have always fluidly associated themselves with elements of different ‘religions’ — Hindus going to Muslim shrines, Budhist marriages and Shinto funerals in Japan, etc. It’s not unlike the old Pagan days, except that the meta-discourse filters these aspects out through all sorts of essentializating ecumenical rituals (looking at you Karen Amrstrong….).</p><h3>Is there such a thing as war?</h3><p>Yes, why else would there have been ministries of it? But the problem is not whether there is war but what should be counted as war. There are legitimate suggestions to count WWI and WWII as simply one war with some breaks — a longer one in Europe but a much shorter one in other parts of the world (such as China). But at the same time we could also say that it was a period during which many conflicts were going on around the world which we are conveniently referring to as World War — but actually most of the world was at peace during that time (India, South and Central Americas, much of Africa and lots of individual regions in Europe and Asia). And even though Canada and the US as well as Australia were combatants in the wars, they suffered no direct infrastructural damage and their civilian populations were not directly exposed to the ravages of war.</p><p>How should we count the Mongol wars of expansion? One war or many? The Crusades? Roman wars of conquest? Persian campaigns East of the Greek penninsula? Arab conquests? Thirty-year war? If we look at the Encyclopedia of Wars, we’ll see that some entries cover complex conflicts that last decades to those that are over in weeks or days. So while there’s always some armed conflict going on, when does it slip over into war? In our definition, we sort of assume state actors and formal armies but most wars elude some part of those assumptions (such as use of mercenaries, assymetry between organizational status of combatants, etc.). We also slip and slide between metaphorical and literal wars with ease. The Cold War was a metaphor but involved lots of proxy wars (US in Vietnam, USSR in Afghanistan, etc.) War on drugs or crime is also a metaphor but slides into armed conflict — often involving state actors, armies and sever consequences for local populations. But we also have war on poverty, which generally stays in the metaphorical realm, as do the culture wars, etc.</p><p>This means that if we want to be formal and count wars and their causes we’re going to be making some subjective decisions that will very much influence the outcomes.</p><h3>References</h3><p><a href=\"http://newbooksinsoutheastasianstudies.com/2015/09/15/christopher-r-duncan-violence-and-vengeance-religious-conflict-and-its-aftermath-in-eastern-indonesia-cornell-up-2013/\">http://newbooksinsoutheastasianstudies.com/2015/09/15/christopher-r-duncan-violence-and-vengeance-religious-conflict-and-its-aftermath-in-eastern-indonesia-cornell-up-2013/</a></p><p><a href=\"https://en.wikipedia.org/wiki/List_of_wars_by_death_toll#List_of_wars_by_death_toll_with_over_1.2C000.2C000_deaths\">https://en.wikipedia.org/wiki/List_of_wars_by_death_toll#List_of_wars_by_death_toll_with_over_1.2C000.2C000_deaths</a></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a3002aee387f\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/guns-glory-and-greed-most-wars-were-not-caused-by-religion-but-they-werent-prevented-by-it-a3002aee387f\">Guns, Glory, and Greed: Most wars were not caused by religion (but they weren’t prevented by it…</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "Repaved paths and generative metaphors: Expressing human purposes with technology",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://medium.com/feed/metaphor-hacker",
        "value": "Repaved paths and generative metaphors: Expressing human purposes with technology"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://medium.com/metaphor-hacker/repaved-paths-and-generative-metaphors-expressing-human-purposes-with-technology-321b10fa5bf?source=rss----67ad910371b8---4"
        }
      ],
      "link": "https://medium.com/metaphor-hacker/repaved-paths-and-generative-metaphors-expressing-human-purposes-with-technology-321b10fa5bf?source=rss----67ad910371b8---4",
      "id": "https://medium.com/p/321b10fa5bf",
      "guidislink": false,
      "tags": [
        {
          "term": "technology",
          "scheme": null,
          "label": null
        },
        {
          "term": "education",
          "scheme": null,
          "label": null
        },
        {
          "term": "teaching",
          "scheme": null,
          "label": null
        },
        {
          "term": "metaphor",
          "scheme": null,
          "label": null
        },
        {
          "term": "design",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Dominik Lukes"
        }
      ],
      "author": "Dominik Lukes",
      "author_detail": {
        "name": "Dominik Lukes"
      },
      "published": "Thu, 23 Jun 2016 07:02:17 GMT",
      "published_parsed": [
        2016,
        6,
        23,
        7,
        2,
        17,
        3,
        175,
        0
      ],
      "updated": "2016-06-23T07:02:17.159Z",
      "updated_parsed": [
        2016,
        6,
        23,
        7,
        2,
        17,
        3,
        175,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://medium.com/feed/metaphor-hacker",
          "value": "<p><strong>Note: </strong>This is an edited version of my side of a discussion on an education technology mailing list.</p><p>I fully subscribe to the idea of thinking about pedagogy as being a type of technology (so are cultural institutions according to anthropologists and many people even think of language that way). But most people think of technology as ‘digital technology’ which is how ‘education technology’ is used. For them technology is a real or virtual machine that performs tasks independent of human intervention or in response to basic human actions. What I find most useful is switching between the two perspectives.</p><p>But I’d want to expand the statement ‘technology is something we use to achieve our purposes’ into ‘technology is something we use to <strong>express </strong>our purposes’.</p><p>Often, a poet will be led by rhythm and rhyme to uncover new possibilities of meaning. New analogies or comparisons will inspire a scientist to formulate an innovative hypothesis or an engineer to rethink a mechanical problem (Donald A Schon called these sorts of analogies generative metaphors). These ‘purposes’ did not exist before the technology of language made them real.</p><p>The same goes for (digital) technology. It allows us to express new purposes. It is always developed to achieve some end assuming a purpose but only the reality of use can reveal its true potential (or allow an expression of a useful human purpose in real life contexts). That does not mean that the original purpose was wrong or that it was poorly addressed by the design of the ‘kit’. Take Instagram — it started as a photo sharing site (and a really good and successful one) but since then it’s developed into a social network and even a place where people sell things.</p><p>Likewise, interactive whiteboards (often given as an example of an overhyped failed investment) were a very good idea and in the hands of a skilled, enthusiastic teacher, they do great work. But it turns out that most teachers end up using them to project PowerPoints or Word documents. That’s still useful but less than intended — so unlike with Instagram (or Twitter) which is used to achieve more than intended, interactive whiteboards are used to achieve less. But they’re still used.</p><p>I’m always reminded of the history of <a href=\"http://moodle.org\">Moodle</a> which started as an expression of a particular pedagogical approach but that users started using in a very different way until it responded and became just a standard VLE. Moodle tried to embody an expression of a purpose of a sort of student-centred dialogic pedagogy but teachers used it as a way of sharing documents with students, giving them quizzes, and generally telling them what to do. Now it is a completely structured environment for teachers to guide students step by step. It started as a way to give teachers a way to let students express their learning through open dialogue. Now its design gives expression to the purposes of teachers who want to tell things to students because it turns out that’s what most teachers want to or have to do.</p><p>A similar thing happened to the ‘technology’ of critical pedagogy. It started with Freire’s pedagogy of the oppressed styling education as a tool of liberation. But it turns out teachers trained in it mostly ask a lots of questions to which the students provide answers they think the teacher wants to know. Most of so-called student-centered pedagogy tends to revert to the mean of teacher telling students stuff. Thus expressing a human purpose of keeping 30 young bundles of energy in their seats with a minimum of ruckus.</p><p>An oft-quoted (possibly apocryphal) example is of Stanford not building any paths on their new campus. They simply looked at paths trodden in the grass after a few semesters and then paved them. This is a technique that would be hard to replicate in most contexts (although the Agile development philosophy tries this with programming and tech start ups try this with pivots).</p><p>But being open to repaving once new paths are trodden alongside those you outlined with your technology (be it critical pedagogy or iPads) is perhaps the most important thing we can do in our ed tech efforts.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=321b10fa5bf\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/repaved-paths-and-generative-metaphors-expressing-human-purposes-with-technology-321b10fa5bf\">Repaved paths and generative metaphors: Expressing human purposes with technology</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        }
      ],
      "summary": "<p><strong>Note: </strong>This is an edited version of my side of a discussion on an education technology mailing list.</p><p>I fully subscribe to the idea of thinking about pedagogy as being a type of technology (so are cultural institutions according to anthropologists and many people even think of language that way). But most people think of technology as ‘digital technology’ which is how ‘education technology’ is used. For them technology is a real or virtual machine that performs tasks independent of human intervention or in response to basic human actions. What I find most useful is switching between the two perspectives.</p><p>But I’d want to expand the statement ‘technology is something we use to achieve our purposes’ into ‘technology is something we use to <strong>express </strong>our purposes’.</p><p>Often, a poet will be led by rhythm and rhyme to uncover new possibilities of meaning. New analogies or comparisons will inspire a scientist to formulate an innovative hypothesis or an engineer to rethink a mechanical problem (Donald A Schon called these sorts of analogies generative metaphors). These ‘purposes’ did not exist before the technology of language made them real.</p><p>The same goes for (digital) technology. It allows us to express new purposes. It is always developed to achieve some end assuming a purpose but only the reality of use can reveal its true potential (or allow an expression of a useful human purpose in real life contexts). That does not mean that the original purpose was wrong or that it was poorly addressed by the design of the ‘kit’. Take Instagram — it started as a photo sharing site (and a really good and successful one) but since then it’s developed into a social network and even a place where people sell things.</p><p>Likewise, interactive whiteboards (often given as an example of an overhyped failed investment) were a very good idea and in the hands of a skilled, enthusiastic teacher, they do great work. But it turns out that most teachers end up using them to project PowerPoints or Word documents. That’s still useful but less than intended — so unlike with Instagram (or Twitter) which is used to achieve more than intended, interactive whiteboards are used to achieve less. But they’re still used.</p><p>I’m always reminded of the history of <a href=\"http://moodle.org\">Moodle</a> which started as an expression of a particular pedagogical approach but that users started using in a very different way until it responded and became just a standard VLE. Moodle tried to embody an expression of a purpose of a sort of student-centred dialogic pedagogy but teachers used it as a way of sharing documents with students, giving them quizzes, and generally telling them what to do. Now it is a completely structured environment for teachers to guide students step by step. It started as a way to give teachers a way to let students express their learning through open dialogue. Now its design gives expression to the purposes of teachers who want to tell things to students because it turns out that’s what most teachers want to or have to do.</p><p>A similar thing happened to the ‘technology’ of critical pedagogy. It started with Freire’s pedagogy of the oppressed styling education as a tool of liberation. But it turns out teachers trained in it mostly ask a lots of questions to which the students provide answers they think the teacher wants to know. Most of so-called student-centered pedagogy tends to revert to the mean of teacher telling students stuff. Thus expressing a human purpose of keeping 30 young bundles of energy in their seats with a minimum of ruckus.</p><p>An oft-quoted (possibly apocryphal) example is of Stanford not building any paths on their new campus. They simply looked at paths trodden in the grass after a few semesters and then paved them. This is a technique that would be hard to replicate in most contexts (although the Agile development philosophy tries this with programming and tech start ups try this with pivots).</p><p>But being open to repaving once new paths are trodden alongside those you outlined with your technology (be it critical pedagogy or iPads) is perhaps the most important thing we can do in our ed tech efforts.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=321b10fa5bf\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/repaved-paths-and-generative-metaphors-expressing-human-purposes-with-technology-321b10fa5bf\">Repaved paths and generative metaphors: Expressing human purposes with technology</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    },
    {
      "title": "History as weather: A fractal theory of history for Ian Morris, Jared Diamond and CGP Grey",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://medium.com/feed/metaphor-hacker",
        "value": "History as weather: A fractal theory of history for Ian Morris, Jared Diamond and CGP Grey"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://medium.com/metaphor-hacker/history-as-weather-a-fractal-theory-of-history-for-ian-morris-jared-diamond-and-cgp-grey-45b5503486c5?source=rss----67ad910371b8---4"
        },
        {
          "rel": "license",
          "href": "https://creativecommons.org/licenses/by-sa/4.0/"
        }
      ],
      "link": "https://medium.com/metaphor-hacker/history-as-weather-a-fractal-theory-of-history-for-ian-morris-jared-diamond-and-cgp-grey-45b5503486c5?source=rss----67ad910371b8---4",
      "id": "https://medium.com/p/45b5503486c5",
      "guidislink": false,
      "tags": [
        {
          "term": "history",
          "scheme": null,
          "label": null
        },
        {
          "term": "fractals",
          "scheme": null,
          "label": null
        },
        {
          "term": "complexity-theory",
          "scheme": null,
          "label": null
        }
      ],
      "authors": [
        {
          "name": "Dominik Lukes"
        }
      ],
      "author": "Dominik Lukes",
      "author_detail": {
        "name": "Dominik Lukes"
      },
      "published": "Sat, 13 Feb 2016 09:03:34 GMT",
      "published_parsed": [
        2016,
        2,
        13,
        9,
        3,
        34,
        5,
        44,
        0
      ],
      "updated": "2016-02-16T07:37:19.178Z",
      "updated_parsed": [
        2016,
        2,
        16,
        7,
        37,
        19,
        1,
        47,
        0
      ],
      "content": [
        {
          "type": "text/html",
          "language": null,
          "base": "https://medium.com/feed/metaphor-hacker",
          "value": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5o_VsnRC1R-ZYXKYiAbnFw.jpeg\" /></figure><h3>Outline of the argument</h3><p>History of often accused of not being sufficiently scientific. To remedy this people try to come up with all sorts of theories of history that try to look like science. They come up with some measurable variable (e.g. availability of domesticable species as Jared Diamond did or energy output as Ian Morris did or size of population, advancement of technology, size of armies, etc. as many others do). This, they hope, explains the past and predicts the future. Based on the theories, the would-be-scientist-historians build models that seem to make perfect sense but seemingly fall apart when overtaken by events. This makes history seem like it’s not at all scientific. But that’s only because we’re comparing it to the very rare instances in science where long-term perfect prediction is possible. Like the motions of planetary bodies or the behavior of a computer. But in most cases, science struggles with perfect prediction. In medicine, it’s impossible to predict the exact course a disease will take in any one individual, even though it is often possible to predict what course it will take over a thousand individuals. In biology, it is impossible to predict, which features will be selected over others through natural selection. But it is possible to predict that some will be. But the best analogy, to my mind, is the weather.</p><p>Weather is the result of a large number (fractally infinite), perfectly knowable, describable and predictable physical events. But yet we cannot perfectly predict it even in hours or days and not at all in weeks or months. History (or rather any given present in the past or now or in the future) is also the result of a large (fractally infinite) number of relatively knowable, relatively describable and relatively predictable social events. Yet, we cannot predict it very well on any scale worth predicting. The reason we don’t think of history as science but we think of meteorology as science even though both build models based on observation, known regularities and constants, is because the sort of reliable predictions a historian can make are of no use to anybody, while any kind of weather prediction is extremely useful to everybody.</p><p>Here are some more things about the weather that can be used to view history. On most days, the weather is the same as the day before. You can predict the weather based on the current climate and what you know about the world for a few hours to days in advance (even if you still make errors). It is impossible to predict the exact weather (or history) months in advance but you can predict a range of possible weather patterns (e.g. summers in Europe can be warm or cold but there won’t be snow; political relations in the EU are warm or cold but there won’t be war). You can also predict trends in changes in the climate but not the exact patterns or consequences or timings of those patterns (e.g. average global temperatures will rise but will it mean it will regularly snow in the summer in France?; disagreements between EU states will continue but will it mean a return to recurring warfare known up to the 1950s?). Most common mistakes in modelling the climate/weather and history come from confusing the local with the global (weather with climate and happenstance with trend) — e.g. during a cold summer, people may question global warming or the fall of the Roman empire was being predicted pretty much throughout its history until it happened hundreds of years later. Both weather and history can be disrupted by freak (Black Swan) events.</p><p>What this means is that history can be thought of as much more scientific than is commonly claimed without having to construct models that look like those of climatology.</p><h3>History’s attractors</h3><p>In semi-technical terms, weather is modeled using complexity theory. One of the properties of complex systems is sensitivity to initial conditions. Which means small changes in initial conditions can lead to large swings in the system’s state. This is often described as the ‘butterfly effect’ which is completely misleadingly described as ‘butterfly flapping its wings will cause a hurricane’. A much better example might be a weather front moving over a wooded area results in a tiny change of temperature that can then result in a hurricane developing if it was +1 degree Celsius or not developing if it was +0.5 degrees Celsius. Neither state ‘caused’ the hurricane — which was caused by the all the big forces that cause big things like hurricanes. But it was part of an initial configuration that structures the possibilities.</p><ol><li>We have a certain set of contexts in which certain patterns tend to occur and others do not occur at all. With weather this could be climate (local or global) so it doesn’t snow in the tropics or rain in January in the Arctic. In history, people cannot use technologies yet invented, or generally new agents do not appear out of the blue. [This is the obvious part.]</li><li>Rarely, freak events occur. You may get unseasonably warm weather in the Arctic, hurricane out of season, snow flurry in the tropics. In history, the Mongols, Conquistadors, ISIS came seemingly out of nowhere (and were not in any way predictable by models based on standard assumptions).</li><li>But outside of the freak events, you can predict the range of weather patterns in a given place during a given time period. There’s a range of weather patterns on Earth and weather stays in those patterns. And you can do the same for history. Certain types of things are likely to happen (similar to things that have happened) and others are not — e.g. Alien invasions, raptures are unlikely to happen but invasions, civil wars, famines and epidemics are happening all the time.</li><li>It is possible to predict local weather patterns with a decent level of precision based on models in the extreme short term (hours to days) but even these predictions are subject to significant errors due to assumptions made in the models. E.g. it may matter whether a front went over a wooded area or a field. It is possible to accurately predict historical events in the extreme short term — hours to days — simply based on various models of causation (election results, scheduled events, advancing armies) but it is still subject to frequent errors due to assumptions in the models</li></ol><h3>Fractal histories</h3><p>Above, I used the term ‘fractal infinity’. This is not (as far as I could find out) a technical term. It’s a metaphor based on the coastline paradox made famous by Madelbrot’s paper ‘How long is the coast of Britain’. This is the <a href=\"https://en.wikipedia.org/wiki/Coastline_paradox\">description from Wikipedia</a>:</p><blockquote><em>the length of the coastline depends on the method used to measure it. Since a landmass has features at all scales, from hundreds of kilometres in size to tiny fractions of a millimetre and below, there is no obvious size of the smallest feature that should be measured around, and hence no single well-defined perimeter to the landmass.</em></blockquote><p>What this means that potentially, there can be an infinite (or an indefinetely large) number of measuring units which is the paradox bit. But more interestingly these coastlines have patterns that are similar to each other across scales. This is how <a href=\"http://science.sciencemag.org/content/156/3775/636\">Madelbrot</a> put it:</p><blockquote><em>Geographical curves are so involved in their detail that their lengths are often infinite or, rather, undefinable. However, many are statistically “selfsimilar,” meaning that each portion can be considered a reduced-scale image of the whole.</em></blockquote><p>Why is this relevant to history? I don’t want to overegg the analogy. Mandelbrot’s contribution was a mathematical description of these types of geometric objects and it worked well in certain quantifiable contexts. But that’s not what the study of social objects such as history needs. Those same quantifications won’t work on more analog problems. However, the analogy can explain a huge problem in historical analysis — namely conflation of similar patterns at vastly different levels of magnification.</p><p>Imagine history (ie. events as revealed to a human observer over time) as a landscape. As you fly over the landscape from a certain height, the different areas seem very similar or possibly identical (from a large enough distance, Earth will look like a dot). So, from a large enough distance all history will seem like striving for resources of groups of people. So it will make sense describing history as that. When you zoom in a bit closer, you will see a great inequality in the struggle for resources so you may want to describe history in terms of ability to project power. And you may want to quantify those differences. But if you zoom in even closer, you will see patterns that look very similar. Even though the groups are of different sizes, they all have very similarly hierarchical organization — with some sort of leadership on top which has to change over time. So, you will try to come up with rules for the formation of these hierarchies. Yet, you zoom in even more and you will notice completely different customs, ways of negotiation, ways of legitimizing. So you will postulate a complete incomensurability and simply describe the difference (kind of like a taxonomist biologist). But then you zoom in even more at the level of individual motivations and you will see things like lust, hunger, aims, struggle for personal success, family — and then you can postulate that we are all the same.</p><p>But the problem is that it is actually relatively easy to describe the different levels of magnification in terms of each other because they are sort of like models of one another. An individual’s desires and dreams can be recast in terms of striving for resources; and the behaviors and customs at the level of a kingdom can be talked about in terms of individual desires. So, the ‘dimensions’ of history depend on the length of the yardstick used to measure it. You can describe the same event from the ‘big man of history’ perspective, the ‘social forces’ perspective, ‘struggle for resources’ perspective or ‘long-duree’, inevitable forces of history perspective. Just like you can describe a chemical event at the level of the structure, molecular interactions or matter transformation.</p><p>But all of these levels/perspectives have their own rules and seemingly causal patterns. They also interact with each other but in complex ways that preclude simple reductionism. That is we cannot say that interactions at one level of magnification cause interactions at another in the same way that an individual brick (or the laying of it) does not cause a wall to exist.</p><p><strong>Aside</strong>: I’ve come to like the ‘wall metaphor’ of causality. We have completely different intuitions about the causal chain of events resulting in the existence of a wall than we have about the chain of events that come to result in a historical event. But perhaps rethinking historical events as walls can be helpful — as long as we also keep the differences in mind. We have the brick makers, brick layers, but also the commissioners, approvers, weather conditions, foundations, historical custom of brick making and brick laying — all of those play a role in the sort of wall we’re going to get. And it is obvious that depending on our perspective the causal chains are going to be completely different. And the same goes for when we try to destroy the wall. We need commensurate forces — big enough force to make a dent, but we also need a lot of small events down to the molecular level. And different perspectives will identify different causal chains. All of them correct but obviously belonging to certain levels. These are obvious (or fairly obvious) when it comes to walls. But not so obvious when it comes to history. Most good historians actually seem to have the same kind of intuitions about events as most of us do about walls. But they are often seduced by the customs of the genre of history writing to jettisoning their intuitions and coming up with a reductionist perspective instead.</p><h3>What this means for the notion of causation in social science</h3><p>We tend to think of causality in history as happening at the lowest level of magnification and accumulating into the total. And on a most straightforward level, this makes perfect sense. A lot of apples put into a basket + the basket, will make a basket full of apples. But that’s an unhelpful way of looking at causality because it implies a certain level of atomism — a final level of further indivisible measuring units. But that level cannot exist — or if it does exist it has to be so low (subatomic) that the whole cannot be modeled using it. Even if we believe in some sort of terminal particle, it is a fool’s errand (looking at you Stephen Hawking) to try to actually use it to measure object-level phenomena with it.</p><p>Which is why the butterfly causing a hurricane is such a poor way of thinking about sensitivity to initial conditions. No one aspect of the physical world ‘causes’ the weather — not the extra +.5 degrees Celsius of temperature, nor the gravitational pull of the moon or the Gulf stream. They constantly interact in a system at all levels. But our ability to model weather is supremely dependent on tiny deviations in measurement.</p><p>And the same holds for history and other social systems. No one event causes another event except in the most trivial sense. And an accumulation of little events does not ‘cause’ big events. Because there are no smallest events that could be said to constitute the final level of magnification for initial conditions. But our ability to perceive small events does have huge implications for our ability to model the future patterns of events. It may have made a difference that Napoleon looked to the left instead of right during a battle thus winning it which in turn led to his toppling of the Prussian state. Which in turn may have made a difference to the shape of the First World War or the Third Reich. But did that glance left actually cause any of those things or led to those things? Not outside the imaginations of writers of time travel scince fiction. It was just a small difference in the pattern (initial conditions) that made the system come out with a different state at a certain level of magnification. But on another level, even if Napoleon had never been born, the system may have looked very much the same from a certain level of magnification (European states struggling for resources).</p><p>We can have models where big things cause other big things and little things cause other little things. And we must be careful to always know what level of magnification we’re talking about. But we must remember that we are talking about our models of what happens, not what actually happens in its totality. The bigger the models, the bigger errors in measurement of initial conditions — or rather with the big models we actually have no hope of complete measurement of the initial conditions or any sort of computational tractability even if such measurement were possible. So even very good and complete models of history have by definition no way of predicting anything with any level of accuracy into any kind of future. And we cannot use their accuracy on past events because our measurement of the data for the past is already filtered by what happened (ie history is written by the winners — or at least, the record keepers). We don’t have that kind of filter for the present or recent history. We could use what we know from the past to help us sift through the data of the present but that’s where the initial conditions lie that can completely mess up our predictions. Even if we had an lawful model of individual psychology and small event dynamics — in the same way weather modelers have accurate models of molecular and Newtonian object physics — we still could not do a better job at prediction than the weather people can. And in fact, any individual success at prediction is no guarantee of the quality of the model.</p><h3>What this means for Jared Diamond, Ian Morris, Niall Ferguson or CGP Grey</h3><p>This essay was inspired by <a href=\"https://www.reddit.com/r/CGPGrey/comments/438ib1/hi_56_guns_germs_and_steel/\">a recent podcast discussion</a> of Jared Diamond’s now classic but highly controversial ‘Guns, Germs, and Steel’. But I really started thinking about it when reading Ian Morris’ ‘Why the West Rules: For now’ and Niall Ferguson’s ‘Civilization’. They both suffered from the problem of unconscious magnification refocus. Ferguson — who’s by far the sloppiest thinker though by no means a worthless one — was the most illustrative example of this. He even could not fix on the idea of the West was for more than about half a chapter. Morris, on the other hand, was the strictest in his assumptions and measurements — in effect creating two books — the arguement and the footnotes. But even he seems to compress time periods and plays around with effect sizes and scales to make the whole thing work. The same thing Steven Pinker is doing in the infuriatingly flawed by worth reading ‘Better Angels of our Nature’. I think of these as modern historiographical eschatology and it is important to read good anthropologists to fully understand what these people are reading out.</p><p>A perfect companion to all of these were David Graeber’s ‘Debt: First 5000 Years’ and most recently for me (although the earliest in publication date) Eric Wolf’s ‘Europe and the People without History’. Also worth reading are the most recent books on Atlantic history. But not of inconsiderable interest is even dross like Pat Buchanan’s ‘Decline of the West’ because it gives an example how many people are thinking about causes.</p><p>They all think of a theory of history as a collection of hypotheses about historical causality. But we already know what the causal chains are. Or, the good historians do. But then they forget about most of them when alighting on a good grand theory of history. Jumping from weather to climatology but then using the language of the climate to talk about the weather when they come to the lessons for us today or when they zoom in on the period of history they know really well.</p><p>CGP Grey when defending Diamond suggested that people keep criticising Diamond for all the small things but they never substantively critique his grand narrative. That has also been my initial impression of many of the critiques. But the criticism of Diamond is actually about him swooping from his heights of resource-utilization level of modeling history down to the level of individual events where he is much shakier and trying to use the same models on the small events. So this leads to completely inaccurate description of what happened in the Americas — where the guns and steel mattered almost not at all and the germs relatively little. What mattered were the local politics — the conquistadors became enmeshed in local politics and their victories were results of alliances with other local factions. At the same time, the Portuguese had no chance of anything like that happening on the West coast of Africa or in India (where they won some important naval victories against the Ottomans who had just as much steel, better guns and the same germs). In some way, Diamond’s critics (who would not be caught dead at an NRA rally) are saying ‘guns don’t kill people, people kill people’. They are saying, real people killed and enslaved other real people — and we must judge what happened for what it happened. It wasn’t the guns, germs or steel that did it. It was our venerable ancestors that did it. And we’re still doing it — albeit in a less mindless genocidal way. Diamond’s stated intention is to do away with the racial superiority explanation of the difference in the current power arrangements. And he explains a part of it. But he is not careful enough to explore the boundaries of his model. And when applied at the right level, his model does what it sets out to do. But when applied at other levels of magnification, it does exactly the opposite. It provides a way of justifying real bad human behavior as inevitable.</p><p>This is perhaps most starkly exemplified in his description of the Rwandan genocide in his other book ‘Collapse’. There he recasts it in terms of simple competition for resources. He may be right. But as recent Timothy Synder’s book on the holocaust argues, that same was true for the Nazi genocide (in the idea of lebensraum). But those were not causes. This involved people like us shooting other people like us. Up close and personal. And other people telling them to do it and benefiting from it in many ways. Diamond’s models can explain some of the dynamics but they can only do it if they leave a lot of important information out — and the argument is that that’s the information that should matter to us here.</p><p>CGP Grey went on to ask, give me a <strong>better theory of history</strong>. This is in response to that challenge. <strong>Theory is like the weather.</strong> This is more a way of thinking about historiography than history itself. Kind of like the language turn in philosophy. We need to be careful with our models, just like we need to be careful with our language. As the saying goes, ‘all models are wrong, but some of them are useful’.</p><h3>Stack fallacy, hierarchical structure in science and the meaning of history</h3><p>As I was writing this, I came across this article on the <a href=\"http://techcrunch.com/2016/01/18/why-big-companies-keep-failing-the-stack-fallacy\">Stack Fallacy</a> which is making the same point in a different domain. You cannot just assume that because you’re good at building all the building blocks, you can build the whole structure. Brick makers are no good at building walls by virtue of being good brick makers and conversely brick layers are not going to be any good at brick making just because they can use bricks to build durable structures. (The post makes the point about databases and CRMs but its even starker at the physical level).</p><p>One of the commenters pointed to an old paper by <a href=\"https://web2.ph.utexas.edu/~wktse/Welcome_files/More_Is_Different_Phil_Anderson.pdf\">P. W. Anderson ‘More is different’</a> that makes a similar point about physics.</p><blockquote><em>The ability to reduce everything to simple fundamental laws does not imply the ability to start from those laws and reconstruct the universe. In fact, the more the elementary particle physicists tell us about the nature of the fundamental laws, the less relevance they seem to have to the very real problems of the rest of science, much less to those of society.</em></blockquote><blockquote><em>The constructionist hypothesis breaks down when confronted with the twin difficulties of scale and complexities. The behavior of large and complex aggregates of elementary particles, it turns out, is not to be understood in terms of a simple extrapolation of the properties of a few particles. Instead, at each level of complexity entirely new properties appear, and the understanding of the new behaviors requires research which I think is as fundamental in its nature as any other.</em></blockquote><p>A different way of restating this thesis is that the fundamental laws of the lower levels of organization apply at the higher levels (e.g. bricks have strengths and periods of decay and that will impact on how much a wall will withstand or how long it will stand up) but the higher levels have new emergent properties that cannot be easily described in terms of the lower levels. And vice versa, the lower levels cannot be described in terms of the higher levels of organization. However, the structures emergent at all levels can interact with structures emergent at other levels.</p><p>What does this mean for history? It can very well matter that a king was a violent drunk who lusted after his subjects’ daughters and wives, and therefore alienated all his followers leading to the collapse of a dynasty. However, when we look at the patterns of falls and rises of things like dynasties, we do not benefit from describing them in terms of individual psychology interacting with their immediate social norms. That does not mean that the individual psychology and the social norms don’t matter to that individual king or individual dynasty. But falls and rises of dynasties just don’t rest on these issues there are broader patterns and dynamics — kings with highly praised kingly behavior cannot turn around the fall of a dynasty without resources and with external pressures and incompetent despised rulers do not tend to ruin powerful dynasties overnight.</p><p>To bring it back to the reimagined ‘butterfly effect’ metaphor. There are valid patterns that can be recognized but <strong>patterns are not causes</strong>. The exact shapes of the patterns or their varieties are extremely sensitive to initial conditions but the initial conditions are not the causes. Everything that comprises that pattern interacts together (that would include external inputs) to produce all its phases/shapes. It may even be inaccurate to call the pattern itself sensitive to initial conditions, it is our ability to make predictions about its future states and possibly judgments that is extremely sensitive to tiny errors in our observation of what we decide are the initial conditions for the purposes of our prediction making.</p><p>With certain weather patterns — like storms — the initial conditions are relatively easy to locate, if relatively difficult to measure. With historical events, it is a little more difficult. It is not always clear where we should look at the initial conditions. It is not uncommon for historians to say things like ‘but the real roots of this crisis lay much further in the past’ — Niall Ferguson’s ‘Civilisation’ is a case study in how badly this can get out of hand. But historians also tend to freely jump between the levels. So in a recent LSE lecture, Ian Morris talked about long term (in 1000s of years) trends in war and violence rates. He described an overall long-term equilibrium in violence necessary for the survival of a species. But then he started to talk about our individual natures from an evolutionary perspective, only to talk about the motivation of the elites to keep workers happy and alive to produce goods. And then he talked about the British Empire, Cold War, American supremacy and tried to draw conclusions from that. But that’s like saying one of these:</p><p>a) ‘there was a storm because it is summer when storms happen’ b) ‘this storm really began when the icebergs melted’ c) ‘there will be a storm on Tuesday, July 3rd, next year because we are experiencing global warming’</p><p>Intuitively, none of these statements make sense, even if they may be actually true. Of these, a) is uninformative, b) is also relatively uninformative because we could equally well go back to the formation of the Earth or the Big Bang but other than the arrow of time, we would have little in the way of modelling the causation, and c) is also a statement that is not useful even if it turns out that there actually is a storm on that day. But since our model is consistent with there being a storm on any day in July next year, it is not a useful prediction.</p><p>Yet, historians make statements like these all the time — they are not this starkly nonsensical but if you peel away the narrative layers and just get down to simple causation, you get things like these. At the extreme, you get things like attributing the decline of the Roman empire to lead piping or the rise of the British empire to boiling tea. But even more complex interpretations of past events suffer from similar difficulties.</p><p>For instance, post invasion, dismissing all the Bahtist army officers is often cited as the roots of the current violence in the region. But we could plausibly imagine violence of different type but similar scale ‘resulting’ from not dismissing them. Similarly, when things go well in South Africa, everybody’s pointing to the peace and reconciliation for not creating any new resentments and drawing a line under the past. And when they go badly, people wonder if reconciliation was a problem because it did not give people justice. Ultimately, we cannot say much more than both are things that could happen. We simply have no way of tracking the causal chains at the level in which we could even run the models.</p><p>Social science critics argue that unlike the meteorologists who can build their larger weather models on solid physics, chemistry and geography, historians don’t actually have solid enough psychological models underlying their bigger sociological models. But that is to make the same error. Meteorological models can just about predict the weather tomorrow and the day after while sometimes making huge errors. Historical models are just as good. Equally, historical models can predict larger trends about as accurately as climatology. But we often treat them as if we thought it was possible to predict the weather a year from now.</p><p>So what is the point of history then? Its accurate predictions are not very useful and its useful predictions are not very accurate. Any statement like ‘people who forget their history are doomed to repeat it’ are nonsense. Knowing what exactly happened in the past is no better than knowing what the weather was yesterday. Other than knowing that anything that happened in the past can happen again, we’re no better off.</p><p>But the weather analogy can come to the rescue. On most days, for most people, it does not actually matter what the weather is going to be like tomorrow. Yet, people obsessively check their forecasts. It is interesting. And also it is something to talk about. And that is not nothing. Historical knowledge — you could argue — is even more valuable. It is something to talk about but unlike the weather (unless you think God sends it as reward or punishment), it can be used to help us make sense of today — not in a causal manner but in a narrative one. And the smart historians know that.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=45b5503486c5\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/history-as-weather-a-fractal-theory-of-history-for-ian-morris-jared-diamond-and-cgp-grey-45b5503486c5\">History as weather: A fractal theory of history for Ian Morris, Jared Diamond and CGP Grey</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        }
      ],
      "summary": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5o_VsnRC1R-ZYXKYiAbnFw.jpeg\" /></figure><h3>Outline of the argument</h3><p>History of often accused of not being sufficiently scientific. To remedy this people try to come up with all sorts of theories of history that try to look like science. They come up with some measurable variable (e.g. availability of domesticable species as Jared Diamond did or energy output as Ian Morris did or size of population, advancement of technology, size of armies, etc. as many others do). This, they hope, explains the past and predicts the future. Based on the theories, the would-be-scientist-historians build models that seem to make perfect sense but seemingly fall apart when overtaken by events. This makes history seem like it’s not at all scientific. But that’s only because we’re comparing it to the very rare instances in science where long-term perfect prediction is possible. Like the motions of planetary bodies or the behavior of a computer. But in most cases, science struggles with perfect prediction. In medicine, it’s impossible to predict the exact course a disease will take in any one individual, even though it is often possible to predict what course it will take over a thousand individuals. In biology, it is impossible to predict, which features will be selected over others through natural selection. But it is possible to predict that some will be. But the best analogy, to my mind, is the weather.</p><p>Weather is the result of a large number (fractally infinite), perfectly knowable, describable and predictable physical events. But yet we cannot perfectly predict it even in hours or days and not at all in weeks or months. History (or rather any given present in the past or now or in the future) is also the result of a large (fractally infinite) number of relatively knowable, relatively describable and relatively predictable social events. Yet, we cannot predict it very well on any scale worth predicting. The reason we don’t think of history as science but we think of meteorology as science even though both build models based on observation, known regularities and constants, is because the sort of reliable predictions a historian can make are of no use to anybody, while any kind of weather prediction is extremely useful to everybody.</p><p>Here are some more things about the weather that can be used to view history. On most days, the weather is the same as the day before. You can predict the weather based on the current climate and what you know about the world for a few hours to days in advance (even if you still make errors). It is impossible to predict the exact weather (or history) months in advance but you can predict a range of possible weather patterns (e.g. summers in Europe can be warm or cold but there won’t be snow; political relations in the EU are warm or cold but there won’t be war). You can also predict trends in changes in the climate but not the exact patterns or consequences or timings of those patterns (e.g. average global temperatures will rise but will it mean it will regularly snow in the summer in France?; disagreements between EU states will continue but will it mean a return to recurring warfare known up to the 1950s?). Most common mistakes in modelling the climate/weather and history come from confusing the local with the global (weather with climate and happenstance with trend) — e.g. during a cold summer, people may question global warming or the fall of the Roman empire was being predicted pretty much throughout its history until it happened hundreds of years later. Both weather and history can be disrupted by freak (Black Swan) events.</p><p>What this means is that history can be thought of as much more scientific than is commonly claimed without having to construct models that look like those of climatology.</p><h3>History’s attractors</h3><p>In semi-technical terms, weather is modeled using complexity theory. One of the properties of complex systems is sensitivity to initial conditions. Which means small changes in initial conditions can lead to large swings in the system’s state. This is often described as the ‘butterfly effect’ which is completely misleadingly described as ‘butterfly flapping its wings will cause a hurricane’. A much better example might be a weather front moving over a wooded area results in a tiny change of temperature that can then result in a hurricane developing if it was +1 degree Celsius or not developing if it was +0.5 degrees Celsius. Neither state ‘caused’ the hurricane — which was caused by the all the big forces that cause big things like hurricanes. But it was part of an initial configuration that structures the possibilities.</p><ol><li>We have a certain set of contexts in which certain patterns tend to occur and others do not occur at all. With weather this could be climate (local or global) so it doesn’t snow in the tropics or rain in January in the Arctic. In history, people cannot use technologies yet invented, or generally new agents do not appear out of the blue. [This is the obvious part.]</li><li>Rarely, freak events occur. You may get unseasonably warm weather in the Arctic, hurricane out of season, snow flurry in the tropics. In history, the Mongols, Conquistadors, ISIS came seemingly out of nowhere (and were not in any way predictable by models based on standard assumptions).</li><li>But outside of the freak events, you can predict the range of weather patterns in a given place during a given time period. There’s a range of weather patterns on Earth and weather stays in those patterns. And you can do the same for history. Certain types of things are likely to happen (similar to things that have happened) and others are not — e.g. Alien invasions, raptures are unlikely to happen but invasions, civil wars, famines and epidemics are happening all the time.</li><li>It is possible to predict local weather patterns with a decent level of precision based on models in the extreme short term (hours to days) but even these predictions are subject to significant errors due to assumptions made in the models. E.g. it may matter whether a front went over a wooded area or a field. It is possible to accurately predict historical events in the extreme short term — hours to days — simply based on various models of causation (election results, scheduled events, advancing armies) but it is still subject to frequent errors due to assumptions in the models</li></ol><h3>Fractal histories</h3><p>Above, I used the term ‘fractal infinity’. This is not (as far as I could find out) a technical term. It’s a metaphor based on the coastline paradox made famous by Madelbrot’s paper ‘How long is the coast of Britain’. This is the <a href=\"https://en.wikipedia.org/wiki/Coastline_paradox\">description from Wikipedia</a>:</p><blockquote><em>the length of the coastline depends on the method used to measure it. Since a landmass has features at all scales, from hundreds of kilometres in size to tiny fractions of a millimetre and below, there is no obvious size of the smallest feature that should be measured around, and hence no single well-defined perimeter to the landmass.</em></blockquote><p>What this means that potentially, there can be an infinite (or an indefinetely large) number of measuring units which is the paradox bit. But more interestingly these coastlines have patterns that are similar to each other across scales. This is how <a href=\"http://science.sciencemag.org/content/156/3775/636\">Madelbrot</a> put it:</p><blockquote><em>Geographical curves are so involved in their detail that their lengths are often infinite or, rather, undefinable. However, many are statistically “selfsimilar,” meaning that each portion can be considered a reduced-scale image of the whole.</em></blockquote><p>Why is this relevant to history? I don’t want to overegg the analogy. Mandelbrot’s contribution was a mathematical description of these types of geometric objects and it worked well in certain quantifiable contexts. But that’s not what the study of social objects such as history needs. Those same quantifications won’t work on more analog problems. However, the analogy can explain a huge problem in historical analysis — namely conflation of similar patterns at vastly different levels of magnification.</p><p>Imagine history (ie. events as revealed to a human observer over time) as a landscape. As you fly over the landscape from a certain height, the different areas seem very similar or possibly identical (from a large enough distance, Earth will look like a dot). So, from a large enough distance all history will seem like striving for resources of groups of people. So it will make sense describing history as that. When you zoom in a bit closer, you will see a great inequality in the struggle for resources so you may want to describe history in terms of ability to project power. And you may want to quantify those differences. But if you zoom in even closer, you will see patterns that look very similar. Even though the groups are of different sizes, they all have very similarly hierarchical organization — with some sort of leadership on top which has to change over time. So, you will try to come up with rules for the formation of these hierarchies. Yet, you zoom in even more and you will notice completely different customs, ways of negotiation, ways of legitimizing. So you will postulate a complete incomensurability and simply describe the difference (kind of like a taxonomist biologist). But then you zoom in even more at the level of individual motivations and you will see things like lust, hunger, aims, struggle for personal success, family — and then you can postulate that we are all the same.</p><p>But the problem is that it is actually relatively easy to describe the different levels of magnification in terms of each other because they are sort of like models of one another. An individual’s desires and dreams can be recast in terms of striving for resources; and the behaviors and customs at the level of a kingdom can be talked about in terms of individual desires. So, the ‘dimensions’ of history depend on the length of the yardstick used to measure it. You can describe the same event from the ‘big man of history’ perspective, the ‘social forces’ perspective, ‘struggle for resources’ perspective or ‘long-duree’, inevitable forces of history perspective. Just like you can describe a chemical event at the level of the structure, molecular interactions or matter transformation.</p><p>But all of these levels/perspectives have their own rules and seemingly causal patterns. They also interact with each other but in complex ways that preclude simple reductionism. That is we cannot say that interactions at one level of magnification cause interactions at another in the same way that an individual brick (or the laying of it) does not cause a wall to exist.</p><p><strong>Aside</strong>: I’ve come to like the ‘wall metaphor’ of causality. We have completely different intuitions about the causal chain of events resulting in the existence of a wall than we have about the chain of events that come to result in a historical event. But perhaps rethinking historical events as walls can be helpful — as long as we also keep the differences in mind. We have the brick makers, brick layers, but also the commissioners, approvers, weather conditions, foundations, historical custom of brick making and brick laying — all of those play a role in the sort of wall we’re going to get. And it is obvious that depending on our perspective the causal chains are going to be completely different. And the same goes for when we try to destroy the wall. We need commensurate forces — big enough force to make a dent, but we also need a lot of small events down to the molecular level. And different perspectives will identify different causal chains. All of them correct but obviously belonging to certain levels. These are obvious (or fairly obvious) when it comes to walls. But not so obvious when it comes to history. Most good historians actually seem to have the same kind of intuitions about events as most of us do about walls. But they are often seduced by the customs of the genre of history writing to jettisoning their intuitions and coming up with a reductionist perspective instead.</p><h3>What this means for the notion of causation in social science</h3><p>We tend to think of causality in history as happening at the lowest level of magnification and accumulating into the total. And on a most straightforward level, this makes perfect sense. A lot of apples put into a basket + the basket, will make a basket full of apples. But that’s an unhelpful way of looking at causality because it implies a certain level of atomism — a final level of further indivisible measuring units. But that level cannot exist — or if it does exist it has to be so low (subatomic) that the whole cannot be modeled using it. Even if we believe in some sort of terminal particle, it is a fool’s errand (looking at you Stephen Hawking) to try to actually use it to measure object-level phenomena with it.</p><p>Which is why the butterfly causing a hurricane is such a poor way of thinking about sensitivity to initial conditions. No one aspect of the physical world ‘causes’ the weather — not the extra +.5 degrees Celsius of temperature, nor the gravitational pull of the moon or the Gulf stream. They constantly interact in a system at all levels. But our ability to model weather is supremely dependent on tiny deviations in measurement.</p><p>And the same holds for history and other social systems. No one event causes another event except in the most trivial sense. And an accumulation of little events does not ‘cause’ big events. Because there are no smallest events that could be said to constitute the final level of magnification for initial conditions. But our ability to perceive small events does have huge implications for our ability to model the future patterns of events. It may have made a difference that Napoleon looked to the left instead of right during a battle thus winning it which in turn led to his toppling of the Prussian state. Which in turn may have made a difference to the shape of the First World War or the Third Reich. But did that glance left actually cause any of those things or led to those things? Not outside the imaginations of writers of time travel scince fiction. It was just a small difference in the pattern (initial conditions) that made the system come out with a different state at a certain level of magnification. But on another level, even if Napoleon had never been born, the system may have looked very much the same from a certain level of magnification (European states struggling for resources).</p><p>We can have models where big things cause other big things and little things cause other little things. And we must be careful to always know what level of magnification we’re talking about. But we must remember that we are talking about our models of what happens, not what actually happens in its totality. The bigger the models, the bigger errors in measurement of initial conditions — or rather with the big models we actually have no hope of complete measurement of the initial conditions or any sort of computational tractability even if such measurement were possible. So even very good and complete models of history have by definition no way of predicting anything with any level of accuracy into any kind of future. And we cannot use their accuracy on past events because our measurement of the data for the past is already filtered by what happened (ie history is written by the winners — or at least, the record keepers). We don’t have that kind of filter for the present or recent history. We could use what we know from the past to help us sift through the data of the present but that’s where the initial conditions lie that can completely mess up our predictions. Even if we had an lawful model of individual psychology and small event dynamics — in the same way weather modelers have accurate models of molecular and Newtonian object physics — we still could not do a better job at prediction than the weather people can. And in fact, any individual success at prediction is no guarantee of the quality of the model.</p><h3>What this means for Jared Diamond, Ian Morris, Niall Ferguson or CGP Grey</h3><p>This essay was inspired by <a href=\"https://www.reddit.com/r/CGPGrey/comments/438ib1/hi_56_guns_germs_and_steel/\">a recent podcast discussion</a> of Jared Diamond’s now classic but highly controversial ‘Guns, Germs, and Steel’. But I really started thinking about it when reading Ian Morris’ ‘Why the West Rules: For now’ and Niall Ferguson’s ‘Civilization’. They both suffered from the problem of unconscious magnification refocus. Ferguson — who’s by far the sloppiest thinker though by no means a worthless one — was the most illustrative example of this. He even could not fix on the idea of the West was for more than about half a chapter. Morris, on the other hand, was the strictest in his assumptions and measurements — in effect creating two books — the arguement and the footnotes. But even he seems to compress time periods and plays around with effect sizes and scales to make the whole thing work. The same thing Steven Pinker is doing in the infuriatingly flawed by worth reading ‘Better Angels of our Nature’. I think of these as modern historiographical eschatology and it is important to read good anthropologists to fully understand what these people are reading out.</p><p>A perfect companion to all of these were David Graeber’s ‘Debt: First 5000 Years’ and most recently for me (although the earliest in publication date) Eric Wolf’s ‘Europe and the People without History’. Also worth reading are the most recent books on Atlantic history. But not of inconsiderable interest is even dross like Pat Buchanan’s ‘Decline of the West’ because it gives an example how many people are thinking about causes.</p><p>They all think of a theory of history as a collection of hypotheses about historical causality. But we already know what the causal chains are. Or, the good historians do. But then they forget about most of them when alighting on a good grand theory of history. Jumping from weather to climatology but then using the language of the climate to talk about the weather when they come to the lessons for us today or when they zoom in on the period of history they know really well.</p><p>CGP Grey when defending Diamond suggested that people keep criticising Diamond for all the small things but they never substantively critique his grand narrative. That has also been my initial impression of many of the critiques. But the criticism of Diamond is actually about him swooping from his heights of resource-utilization level of modeling history down to the level of individual events where he is much shakier and trying to use the same models on the small events. So this leads to completely inaccurate description of what happened in the Americas — where the guns and steel mattered almost not at all and the germs relatively little. What mattered were the local politics — the conquistadors became enmeshed in local politics and their victories were results of alliances with other local factions. At the same time, the Portuguese had no chance of anything like that happening on the West coast of Africa or in India (where they won some important naval victories against the Ottomans who had just as much steel, better guns and the same germs). In some way, Diamond’s critics (who would not be caught dead at an NRA rally) are saying ‘guns don’t kill people, people kill people’. They are saying, real people killed and enslaved other real people — and we must judge what happened for what it happened. It wasn’t the guns, germs or steel that did it. It was our venerable ancestors that did it. And we’re still doing it — albeit in a less mindless genocidal way. Diamond’s stated intention is to do away with the racial superiority explanation of the difference in the current power arrangements. And he explains a part of it. But he is not careful enough to explore the boundaries of his model. And when applied at the right level, his model does what it sets out to do. But when applied at other levels of magnification, it does exactly the opposite. It provides a way of justifying real bad human behavior as inevitable.</p><p>This is perhaps most starkly exemplified in his description of the Rwandan genocide in his other book ‘Collapse’. There he recasts it in terms of simple competition for resources. He may be right. But as recent Timothy Synder’s book on the holocaust argues, that same was true for the Nazi genocide (in the idea of lebensraum). But those were not causes. This involved people like us shooting other people like us. Up close and personal. And other people telling them to do it and benefiting from it in many ways. Diamond’s models can explain some of the dynamics but they can only do it if they leave a lot of important information out — and the argument is that that’s the information that should matter to us here.</p><p>CGP Grey went on to ask, give me a <strong>better theory of history</strong>. This is in response to that challenge. <strong>Theory is like the weather.</strong> This is more a way of thinking about historiography than history itself. Kind of like the language turn in philosophy. We need to be careful with our models, just like we need to be careful with our language. As the saying goes, ‘all models are wrong, but some of them are useful’.</p><h3>Stack fallacy, hierarchical structure in science and the meaning of history</h3><p>As I was writing this, I came across this article on the <a href=\"http://techcrunch.com/2016/01/18/why-big-companies-keep-failing-the-stack-fallacy\">Stack Fallacy</a> which is making the same point in a different domain. You cannot just assume that because you’re good at building all the building blocks, you can build the whole structure. Brick makers are no good at building walls by virtue of being good brick makers and conversely brick layers are not going to be any good at brick making just because they can use bricks to build durable structures. (The post makes the point about databases and CRMs but its even starker at the physical level).</p><p>One of the commenters pointed to an old paper by <a href=\"https://web2.ph.utexas.edu/~wktse/Welcome_files/More_Is_Different_Phil_Anderson.pdf\">P. W. Anderson ‘More is different’</a> that makes a similar point about physics.</p><blockquote><em>The ability to reduce everything to simple fundamental laws does not imply the ability to start from those laws and reconstruct the universe. In fact, the more the elementary particle physicists tell us about the nature of the fundamental laws, the less relevance they seem to have to the very real problems of the rest of science, much less to those of society.</em></blockquote><blockquote><em>The constructionist hypothesis breaks down when confronted with the twin difficulties of scale and complexities. The behavior of large and complex aggregates of elementary particles, it turns out, is not to be understood in terms of a simple extrapolation of the properties of a few particles. Instead, at each level of complexity entirely new properties appear, and the understanding of the new behaviors requires research which I think is as fundamental in its nature as any other.</em></blockquote><p>A different way of restating this thesis is that the fundamental laws of the lower levels of organization apply at the higher levels (e.g. bricks have strengths and periods of decay and that will impact on how much a wall will withstand or how long it will stand up) but the higher levels have new emergent properties that cannot be easily described in terms of the lower levels. And vice versa, the lower levels cannot be described in terms of the higher levels of organization. However, the structures emergent at all levels can interact with structures emergent at other levels.</p><p>What does this mean for history? It can very well matter that a king was a violent drunk who lusted after his subjects’ daughters and wives, and therefore alienated all his followers leading to the collapse of a dynasty. However, when we look at the patterns of falls and rises of things like dynasties, we do not benefit from describing them in terms of individual psychology interacting with their immediate social norms. That does not mean that the individual psychology and the social norms don’t matter to that individual king or individual dynasty. But falls and rises of dynasties just don’t rest on these issues there are broader patterns and dynamics — kings with highly praised kingly behavior cannot turn around the fall of a dynasty without resources and with external pressures and incompetent despised rulers do not tend to ruin powerful dynasties overnight.</p><p>To bring it back to the reimagined ‘butterfly effect’ metaphor. There are valid patterns that can be recognized but <strong>patterns are not causes</strong>. The exact shapes of the patterns or their varieties are extremely sensitive to initial conditions but the initial conditions are not the causes. Everything that comprises that pattern interacts together (that would include external inputs) to produce all its phases/shapes. It may even be inaccurate to call the pattern itself sensitive to initial conditions, it is our ability to make predictions about its future states and possibly judgments that is extremely sensitive to tiny errors in our observation of what we decide are the initial conditions for the purposes of our prediction making.</p><p>With certain weather patterns — like storms — the initial conditions are relatively easy to locate, if relatively difficult to measure. With historical events, it is a little more difficult. It is not always clear where we should look at the initial conditions. It is not uncommon for historians to say things like ‘but the real roots of this crisis lay much further in the past’ — Niall Ferguson’s ‘Civilisation’ is a case study in how badly this can get out of hand. But historians also tend to freely jump between the levels. So in a recent LSE lecture, Ian Morris talked about long term (in 1000s of years) trends in war and violence rates. He described an overall long-term equilibrium in violence necessary for the survival of a species. But then he started to talk about our individual natures from an evolutionary perspective, only to talk about the motivation of the elites to keep workers happy and alive to produce goods. And then he talked about the British Empire, Cold War, American supremacy and tried to draw conclusions from that. But that’s like saying one of these:</p><p>a) ‘there was a storm because it is summer when storms happen’ b) ‘this storm really began when the icebergs melted’ c) ‘there will be a storm on Tuesday, July 3rd, next year because we are experiencing global warming’</p><p>Intuitively, none of these statements make sense, even if they may be actually true. Of these, a) is uninformative, b) is also relatively uninformative because we could equally well go back to the formation of the Earth or the Big Bang but other than the arrow of time, we would have little in the way of modelling the causation, and c) is also a statement that is not useful even if it turns out that there actually is a storm on that day. But since our model is consistent with there being a storm on any day in July next year, it is not a useful prediction.</p><p>Yet, historians make statements like these all the time — they are not this starkly nonsensical but if you peel away the narrative layers and just get down to simple causation, you get things like these. At the extreme, you get things like attributing the decline of the Roman empire to lead piping or the rise of the British empire to boiling tea. But even more complex interpretations of past events suffer from similar difficulties.</p><p>For instance, post invasion, dismissing all the Bahtist army officers is often cited as the roots of the current violence in the region. But we could plausibly imagine violence of different type but similar scale ‘resulting’ from not dismissing them. Similarly, when things go well in South Africa, everybody’s pointing to the peace and reconciliation for not creating any new resentments and drawing a line under the past. And when they go badly, people wonder if reconciliation was a problem because it did not give people justice. Ultimately, we cannot say much more than both are things that could happen. We simply have no way of tracking the causal chains at the level in which we could even run the models.</p><p>Social science critics argue that unlike the meteorologists who can build their larger weather models on solid physics, chemistry and geography, historians don’t actually have solid enough psychological models underlying their bigger sociological models. But that is to make the same error. Meteorological models can just about predict the weather tomorrow and the day after while sometimes making huge errors. Historical models are just as good. Equally, historical models can predict larger trends about as accurately as climatology. But we often treat them as if we thought it was possible to predict the weather a year from now.</p><p>So what is the point of history then? Its accurate predictions are not very useful and its useful predictions are not very accurate. Any statement like ‘people who forget their history are doomed to repeat it’ are nonsense. Knowing what exactly happened in the past is no better than knowing what the weather was yesterday. Other than knowing that anything that happened in the past can happen again, we’re no better off.</p><p>But the weather analogy can come to the rescue. On most days, for most people, it does not actually matter what the weather is going to be like tomorrow. Yet, people obsessively check their forecasts. It is interesting. And also it is something to talk about. And that is not nothing. Historical knowledge — you could argue — is even more valuable. It is something to talk about but unlike the weather (unless you think God sends it as reward or punishment), it can be used to help us make sense of today — not in a causal manner but in a narrative one. And the smart historians know that.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=45b5503486c5\" width=\"1\" /><hr /><p><a href=\"https://medium.com/metaphor-hacker/history-as-weather-a-fractal-theory-of-history-for-ian-morris-jared-diamond-and-cgp-grey-45b5503486c5\">History as weather: A fractal theory of history for Ian Morris, Jared Diamond and CGP Grey</a> was originally published in <a href=\"https://medium.com/metaphor-hacker\">Metaphor Hacker</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
    }
  ],
  "feed": {
    "title": "Metaphor Hacker - Medium",
    "title_detail": {
      "type": "text/plain",
      "language": null,
      "base": "https://medium.com/feed/metaphor-hacker",
      "value": "Metaphor Hacker - Medium"
    },
    "subtitle": "Little analogy hacks and perspective shifts can go a long way (in temporary exile from MetaphorHacker.net) - Medium",
    "subtitle_detail": {
      "type": "text/html",
      "language": null,
      "base": "https://medium.com/feed/metaphor-hacker",
      "value": "Little analogy hacks and perspective shifts can go a long way (in temporary exile from MetaphorHacker.net) - Medium"
    },
    "links": [
      {
        "rel": "alternate",
        "type": "text/html",
        "href": "https://medium.com/metaphor-hacker?source=rss----67ad910371b8---4"
      },
      {
        "href": "https://medium.com/feed/metaphor-hacker",
        "rel": "self",
        "type": "application/rss+xml"
      },
      {
        "href": "http://medium.superfeedr.com",
        "rel": "hub",
        "type": "text/html"
      }
    ],
    "link": "https://medium.com/metaphor-hacker?source=rss----67ad910371b8---4",
    "image": {
      "href": "https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png",
      "title": "Metaphor Hacker - Medium",
      "title_detail": {
        "type": "text/plain",
        "language": null,
        "base": "https://medium.com/feed/metaphor-hacker",
        "value": "Metaphor Hacker - Medium"
      },
      "links": [
        {
          "rel": "alternate",
          "type": "text/html",
          "href": "https://medium.com/metaphor-hacker?source=rss----67ad910371b8---4"
        }
      ],
      "link": "https://medium.com/metaphor-hacker?source=rss----67ad910371b8---4"
    },
    "generator_detail": {
      "name": "Medium"
    },
    "generator": "Medium",
    "updated": "Sat, 16 Aug 2025 09:29:39 GMT",
    "updated_parsed": [
      2025,
      8,
      16,
      9,
      29,
      39,
      5,
      228,
      0
    ],
    "publisher": "yourfriends@medium.com",
    "publisher_detail": {
      "email": "yourfriends@medium.com"
    }
  },
  "headers": {
    "date": "Sat, 16 Aug 2025 09:31:13 GMT",
    "content-type": "text/xml; charset=UTF-8",
    "transfer-encoding": "chunked",
    "connection": "close",
    "cf-ray": "96ffeadcf890b373-PRG",
    "cf-cache-status": "DYNAMIC",
    "cache-control": "private, must-revalidate, max-age=900",
    "expires": "Sat, 16 Aug 2025 09:46:12 GMT",
    "link": "<https://medium.com/humans.txt>; rel=\"humans\"",
    "set-cookie": "_cfuvid=0VQtIlTcsycSgZsp5pl7WikO86fmAUkeW_PA7_z1dkI-1755336673038-0.0.1.1-604800000; path=/; domain=.medium.com; HttpOnly; Secure; SameSite=None",
    "strict-transport-security": "max-age=31536000; includeSubDomains; preload",
    "pragma": "private",
    "content-security-policy": "default-src 'self'; connect-src https://localhost https://*.instapaper.com https://*.stripe.com https://glyph.medium.com https://*.paypal.com https://*.braintree-api.com https://*.braintreegateway.com https://accounts.google.com https://getpocket.com https://medium.com https://*.medium.com https://*.medium.com https://medium.com https://*.medium.com https://*.algolia.net https://cdn-static-1.medium.com https://dnqgz544uhbo8.cloudfront.net https://cdn-videos-1.medium.com https://cdn-audio-1.medium.com https://${LIGHTSTEP_HOST} https://*.branch.io 'self'; font-src data: https://*.amazonaws.com https://*.medium.com https://glyph.medium.com https://glyph-sandbox.medium.sh https://medium.com https://*.gstatic.com https://dnqgz544uhbo8.cloudfront.net https://cdn-static-1.medium.com 'self'; frame-src chromenull: https: webviewprogressproxy: blob: medium: 'self'; img-src blob: data: https: 'self'; media-src https://*.cdn.vine.co https://d1fcbxp97j4nb2.cloudfront.net https://d262ilb51hltx0.cloudfront.net https://*.medium.com https://gomiro.medium.com https://miro.medium.com https://pbs.twimg.com 'self' blob:; object-src 'self'; script-src 'unsafe-eval' 'unsafe-inline' about: https: 'self'; style-src 'unsafe-inline' data: https: 'self'; report-uri https://csp.medium.com",
    "medium-fulfilled-by": "edgy/8.15.0, valencia/main-20250814-203755-a218cf7d88",
    "worker-missing-cookies": "1",
    "x-content-type-options": "nosniff",
    "x-envoy-upstream-service-time": "73",
    "x-frame-options": "sameorigin",
    "x-obvious-info": "20250814-2152-root,fdd6a0fc",
    "x-obvious-tid": "1755336672884:3f3deee8ded8",
    "x-powered-by": "Medium",
    "x-ua-compatible": "IE=edge, Chrome=1",
    "x-xss-protection": "1; mode=block",
    "vary": "Accept-Encoding",
    "server": "cloudflare",
    "content-encoding": "gzip",
    "alt-svc": "h3=\":443\"; ma=86400"
  },
  "href": "https://medium.com/feed/metaphor-hacker",
  "status": 200,
  "encoding": "UTF-8",
  "version": "rss20",
  "namespaces": {
    "dc": "http://purl.org/dc/elements/1.1/",
    "content": "http://purl.org/rss/1.0/modules/content/",
    "": "http://www.w3.org/2005/Atom",
    "creativeCommons": "http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html"
  }
}